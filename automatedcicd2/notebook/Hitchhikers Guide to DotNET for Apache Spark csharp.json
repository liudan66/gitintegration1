{
	"name": "Hitchhikers Guide to DotNET for Apache Spark csharp",
	"properties": {
		"folder": {
			"name": "New folder/sample"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "small",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "fc61ae91-2e41-4fcf-8027-c86a93ca6f68"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_sparkdotnet",
				"display_name": "csharp"
			},
			"language_info": {
				"name": "csharp"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
				"name": "small",
				"type": "Spark",
				"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"// Simple assignments should just work \n",
					"var x = 1 + 25;"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"// You can either use traditional approach to printing a variable...\n",
					"Console.WriteLine(x);\n",
					"\n",
					"// ... or just type it and execute a cell\n",
					"256"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"source": [
					"// You can even play with built-in libraries/functions\n",
					"Enumerable.Range(1, 5)"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"source": [
					"// And now for some C# 8.0 features. If you haven't read it,\n",
					"// here's the link: \n",
					"// https://docs.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-8\n",
					"1..4"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"source": [
					"// We can even do pattern matching!\n",
					"public static string RockPaperScissors(string first, string second)\n",
					"    => (first, second) switch\n",
					"    {\n",
					"        (\"rock\", \"paper\") => \"rock is covered by paper. Paper wins.\", // <-- Next cell prints this out\n",
					"        (\"rock\", \"scissors\") => \"rock breaks scissors. Rock wins.\",\n",
					"        (\"paper\", \"rock\") => \"paper covers rock. Paper wins.\",\n",
					"        (\"paper\", \"scissors\") => \"paper is cut by scissors. Scissors wins.\",\n",
					"        (\"scissors\", \"rock\") => \"scissors is broken by rock. Rock wins.\",\n",
					"        (\"scissors\", \"paper\") => \"scissors cuts paper. Scissors wins.\",\n",
					"        (_, _) => \"tie\"\n",
					"    };"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"RockPaperScissors(\"rock\", \"paper\")"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"source": [
					"// Now, for the fun part! You can render HTML\n",
					"display(\n",
					"    div(\n",
					"        h1(\"Our Incredibly Declarative Example\"),\n",
					"        p(\"Can you believe we wrote this \", b(\"in C#\"), \"?\"),\n",
					"        img[src:\"https://media.giphy.com/media/xUPGcguWZHRC2HyBRS/giphy.gif\"],\n",
					"        p(\"What will \", b(\"you\"), \" create next?\")\n",
					"    )\n",
					");"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"source": [
					"// Let us use some sample data. In this cell, we create this data \n",
					"// from *scratch* but you can also load it from your storage container. \n",
					"// For instance, \n",
					"// var df = spark.Read().Json(\"wasbs://<account>@<container>.blob.core.windows.net/people.json\");\n",
					"\n",
					"using Microsoft.Spark.Sql;\n",
					"using Microsoft.Spark.Sql.Types;\n",
					"using static Microsoft.Spark.Sql.Functions;\n",
					"\n",
					"var schema = new StructType(new List<StructField>()\n",
					"    {\n",
					"        new StructField(\"id\", new IntegerType()),\n",
					"        new StructField(\"name\", new StringType())\n",
					"    });\n",
					"\n",
					"var data = new List<GenericRow>();\n",
					"data.Add(new GenericRow(new object[] { 0,  \"Michael\" }));\n",
					"data.Add(new GenericRow(new object[] { 1,  \"Elva\"    }));\n",
					"data.Add(new GenericRow(new object[] { 2,  \"Terry\"   }));\n",
					"data.Add(new GenericRow(new object[] { 3,  \"Steve\"   }));\n",
					"data.Add(new GenericRow(new object[] { 4,  \"Brigit\"  }));\n",
					"data.Add(new GenericRow(new object[] { 5,  \"Niharika\"}));\n",
					"data.Add(new GenericRow(new object[] { 6,  \"Rahul\"   }));\n",
					"data.Add(new GenericRow(new object[] { 7,  \"Tomas\"   }));\n",
					"data.Add(new GenericRow(new object[] { 8,  \"Euan\"   }));\n",
					"data.Add(new GenericRow(new object[] { 9,  \"Lev\"   }));\n",
					"data.Add(new GenericRow(new object[] { 10, \"Saveen\"   }));\n",
					"\n",
					"var df = spark.CreateDataFrame(data, schema);\n",
					"df.Show();"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"source": [
					"// What we're doing here is to define a specific formatter that is tied to \n",
					"// Microsoft.Spark.Sql.DataFrame and registering it. When we then invoke\n",
					"// display() and pass a DataFrame, the formatter is invoked, which then\n",
					"// generates the necessary HTML\n",
					"\n",
					"Microsoft.DotNet.Interactive.Formatting.Formatter<Microsoft.Spark.Sql.DataFrame>.Register((df, writer) =>\n",
					"{\n",
					"    var headers = new List<dynamic>();\n",
					"    var columnNames = df.Columns();\n",
					"    headers.Add(th(i(\"index\")));\n",
					"    headers.AddRange(columnNames.Select(c => th(c)));\n",
					"\n",
					"    var rows = new List<List<dynamic>>();\n",
					"    var currentRow = 0;\n",
					"    var dfRows = df.Take(Math.Min(20, (int)df.Count()));\n",
					"    foreach (Row dfRow in dfRows)\n",
					"    {\n",
					"        var cells = new List<dynamic>();\n",
					"        cells.Add(td(currentRow));\n",
					"\n",
					"        foreach (string columnName in columnNames)\n",
					"        {\n",
					"            cells.Add(td(dfRow.Get(columnName)));\n",
					"        }\n",
					"\n",
					"        rows.Add(cells);\n",
					"        ++currentRow;\n",
					"    }\n",
					"\n",
					"    var t = table[@border: \"0.1\"](\n",
					"        thead[@style: \"background-color: blue; color: white\"](headers),\n",
					"        tbody[@style: \"color: red\"](rows.Select(r => tr(r))));\n",
					"\n",
					"    writer.Write(t);\n",
					"}, \"text/html\");"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"source": [
					"// Now, let's try rendering the Spark's DataFrame in two ways...\n",
					"\n",
					"// ... a regular way ...\n",
					"df.Show();\n",
					"\n",
					"// ... and just typing df (so it invokes the formatter we just defined)\n",
					"display(df);"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"source": [
					"// ... and just typing df (equivalent to \"display(df);\")\n",
					"df"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"// Let us now try something more advanced like, defining C# classes on-the-fly...\n",
					"public static class A {\n",
					"    public static readonly string s = \"The person named \";\n",
					"}"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"source": [
					"// ... and just for illustration, let's define one more simple class\n",
					"public static class B {\n",
					"    private static Random _r = new Random();\n",
					"    private static List<string> _moods = new List<string>{ \"happy\",\"funny\",\"awesome\",\"cool\"};\n",
					"\n",
					"    public static string GetMood() {\n",
					"        return _moods[_r.Next(_moods.Count)];\n",
					"    }\n",
					"}"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"source": [
					"// Let us now define a Spark User-defined Function (UDF) that utilizes\n",
					"// the classes we just defined above. If you do not recognize the syntax\n",
					"// below, here's some relevant documentation:\n",
					"// https://docs.microsoft.com/en-us/dotnet/api/system.func-2?view=netframework-4.8\n",
					"// https://github.com/dotnet/spark/blob/master/examples/Microsoft.Spark.CSharp.Examples/Sql/Basic.cs\n",
					"//\n",
					"// Note: If you change the class definition above, and execute the cell,\n",
					"// you should re-execute this cell (i.e., the cell that defines the UDF)\n",
					"var udf = Udf<string, string>(str => $\"{A.s} - {str} - is {B.GetMood()}!\");"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"source": [
					"// Let's use the UDF on our Spark DataFrame\n",
					"display(\n",
					"    df\n",
					"    .Select(\n",
					"        udf((Microsoft.Spark.Sql.Column)df[\"name\"])));"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"source": [
					"// Tables are not that interesting, right? :) Let's do some visualizations now.\n",
					"// Let us start with something simple to illustrate the idea. We highly encourage\n",
					"// you to look at https://fslab.org/XPlot/ to understand how you can use XPlot's\n",
					"// full capabilities. While the examples are in F#, it is fairly straightforward\n",
					"// to rewrite in C#.\n",
					"\n",
					"using XPlot.Plotly;\n",
					"\n",
					"var lineChart = Chart.Line(new List<int> { 1, 2, 3, 4, 5, 6, 10, 44 });\n",
					"lineChart.WithTitle(\"My awesome chart\");\n",
					"lineChart.WithXTitle(\"X axis\");\n",
					"lineChart.WithYTitle(\"Y axis\");\n",
					"display(lineChart);"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"source": [
					"// Good! Now let us try to visualize the Spark DataFrame we have.\n",
					"// Now is a good time to refresh your concept of a Spark DataFrame\n",
					"// https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
					"// Remember that a Spark DataFrame is a distributed representation \n",
					"// of your dataset (yes, even if your data is a few KB). Since we\n",
					"// are using a visualization library, we need to first 'collect'\n",
					"// (notice how we are using df.Collect().ToArray() below)\n",
					"// all the data that is distributed on your cluster, and shape it\n",
					"// appropriately for XPlot.\n",
					"//\n",
					"// Note: Visualizations are good for smaller datasets (typically, \n",
					"// a few 10s of thousands of data points coming to KBs), so if you are\n",
					"// trying to visualize GBs of data, it is usually a good idea to\n",
					"// summarize your data appropriately using Spark.NET's APIs. For\n",
					"// a list of summarization APIs, see here:\n",
					"// https://docs.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.functions?view=spark-dotnet\n",
					"\n",
					"var names = new List<string>();\n",
					"var ids = new List<int>();\n",
					"\n",
					"foreach (Row row in df.Collect().ToArray())\n",
					"{\n",
					" names.Add(row.GetAs<string>(\"name\"));\n",
					" int? id = row.GetAs<int?>(\"id\");\n",
					" ids.Add( id ?? 0);\n",
					"}\n",
					"var bar = new Graph.Bar\n",
					"{\n",
					" name = \"bar chart\",\n",
					" x = names,\n",
					" y = ids\n",
					"};\n",
					"\n",
					"var chart = Chart.Plot(new[] {bar});\n",
					"display(chart);"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"source": [
					"// As a final step, let us now plot a histogram of a random dataset\n",
					"\n",
					"using XPlot.Plotly;\n",
					"\n",
					"var schema = new StructType(new List<StructField>()\n",
					"    {\n",
					"        new StructField(\"number\", new DoubleType())\n",
					"    });\n",
					"\n",
					"Random random = new Random();\n",
					"\n",
					"var data = new List<GenericRow>();\n",
					"for(int i = 0; i <=100; i++) {\n",
					"    data.Add(new GenericRow(new object[] { random.NextDouble() }));\n",
					"}\n",
					"\n",
					"var histogramDf = spark.CreateDataFrame(data, schema);\n",
					"histogramDf.Show()"
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"source": [
					"// Time to use LINQ (or Language Integrated Query) :)\n",
					"// For those that are not familiar with LINQ, you can read more about it\n",
					"// here: https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/\n",
					"\n",
					"using System.Linq;\n",
					"\n",
					"// Let us take the histogramDf we loaded through Spark and sample some data points\n",
					"// for the histogram. We will then use LINQ to shape the data for our next \n",
					"// steps (visualization!)\n",
					"var sample1 = \n",
					"        histogramDf.Sample(0.5, true).Collect().ToArray() // <---- Spark APIs\n",
					"        .Select(x => x.GetAs<double>(\"number\")); // <---- LINQ APIs\n",
					"        \n",
					"// Let us create two more sample sets we can use for plotting\n",
					"var sample2 = histogramDf.Sample(0.3, false).Collect().ToArray().Select(x => x.GetAs<double>(\"number\"));\n",
					"var sample3 = histogramDf.Sample(0.6, true).Collect().ToArray().Select(x => x.GetAs<double>(\"number\"));"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"source": [
					"// Let us plot the histograms now!\n",
					"var hist1 = new Graph.Histogram{x = sample1, opacity = 0.75};\n",
					"var hist2 = new Graph.Histogram{x = sample2, opacity = 0.75};\n",
					"var hist3 = new Graph.Histogram{x = sample3, opacity = 0.75};"
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"source": [
					"Chart.Plot(new[] {hist1})"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"source": [
					"Chart.Plot(new[] {hist2})"
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"source": [
					"Chart.Plot(new[] {hist3})"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"source": [
					"// but wait, that's three different graphs and it's impossible to read them\n",
					"// altogether! Let's try an overlay histogram, shall we?\n",
					"var layout = new XPlot.Plotly.Layout.Layout{barmode=\"overlay\", title=\"Overlaid Histogram\"};\n",
					"var histogram = Chart.Plot(new[] {hist1, hist2, hist3});\n",
					"histogram.WithLayout(layout);\n",
					"histogram"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"source": [
					"// And for the final touches\n",
					"using static XPlot.Plotly.Graph;\n",
					"\n",
					"layout.title = \"Overlaid Histogram with cool colors!\";\n",
					"hist1.marker = new Marker {color = \"#D65108)\"};\n",
					"hist2.marker = new Marker {color = \"#ffff00\"}; \n",
					"hist3.marker = new Marker {color = \"#462255\"};\n",
					"\n",
					"histogram"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"source": [
					"// Let's construct a VectorUdf by directly using Arrow.\n",
					"using Apache.Arrow;\n",
					"using static Microsoft.Spark.Sql.ArrowFunctions;\n",
					"using Column = Microsoft.Spark.Sql.Column;\n",
					"\n",
					"// Helper method to construct an ArrowArray from a string[].\n",
					"public static IArrowArray ToStringArrowArray(string[] array)\n",
					"{\n",
					"    var valueOffsets = new ArrowBuffer.Builder<int>();\n",
					"    var valueBuffer = new ArrowBuffer.Builder<byte>();\n",
					"    int offset = 0;\n",
					"\n",
					"    foreach (string str in array)\n",
					"    {\n",
					"        byte[] bytes = Encoding.UTF8.GetBytes(str);\n",
					"        valueOffsets.Append(offset);\n",
					"        valueBuffer.Append(bytes);\n",
					"        offset += bytes.Length;\n",
					"    }\n",
					"\n",
					"    valueOffsets.Append(offset);\n",
					"    return new StringArray(\n",
					"        new ArrayData(\n",
					"            Apache.Arrow.Types.StringType.Default,\n",
					"            valueOffsets.Length - 1,\n",
					"            0,\n",
					"            0,\n",
					"            new[] { ArrowBuffer.Empty, valueOffsets.Build(), valueBuffer.Build() }));\n",
					"}\n",
					"\n",
					"Func<Int32Array, StringArray, StringArray> arrowUdf =\n",
					"    (ids, names) => (StringArray)ToStringArrowArray(\n",
					"        Enumerable.Range(0, names.Length)\n",
					"            .Select(i => $\"id: {ids.GetValue(i)}, name: {names.GetString(i)}\")\n",
					"            .ToArray());\n",
					"\n",
					"Func<Column, Column, Column> vectorUdf1 = VectorUdf(arrowUdf);"
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"source": [
					"df.Select(vectorUdf1(df[\"id\"], df[\"name\"]))"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"source": [
					"// Now let's construct a VectorUdf by using Microsoft Dataframe\n",
					"using Microsoft.Data.Analysis;\n",
					"using static Microsoft.Spark.Sql.DataFrameFunctions;\n",
					"\n",
					"Func<Int32DataFrameColumn, ArrowStringDataFrameColumn, ArrowStringDataFrameColumn> msftDfFunc =\n",
					"    (ids, names) =>\n",
					"    {\n",
					"        long i = 0;\n",
					"        return names.Apply(cur => $\"id: {ids[i++]}, name: {cur}\");\n",
					"    };\n",
					"\n",
					"Func<Column, Column, Column> vectorUdf2 = VectorUdf(msftDfFunc);"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"source": [
					"df.Select(vectorUdf2(df[\"id\"], df[\"name\"]))"
				],
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"source": [
					"// Use #r to install new packages into the current session\n",
					"\n",
					"// Installs latest version\n",
					"#r \"nuget: MathNet.Numerics\"\n",
					"\n",
					"// Installs specified version\n",
					"#r \"nuget: NumSharp,0.20.5\""
				],
				"execution_count": 30
			},
			{
				"cell_type": "code",
				"source": [
					"using MathNet.Numerics.LinearAlgebra;\n",
					"using MathNet.Numerics.LinearAlgebra.Double;\n",
					"using NumSharp;\n",
					"\n",
					"var mathNetUdf = Udf<string, string>(str => {\n",
					"    Matrix<double> matrix = DenseMatrix.OfArray(new double[,] {\n",
					"        {1,1,1,1},\n",
					"        {1,2,3,4},\n",
					"        {4,3,2,1}});\n",
					"\n",
					"    return $\"{matrix[0, 0]} - {str} - {matrix[1, 1]}!\";\n",
					"});\n",
					"\n",
					"var numSharpUdf = Udf<string, string>(str => {\n",
					"    var nd = np.arange(12);\n",
					"\n",
					"    return $\"{nd[1].ToString()} - {str} - {nd[5].ToString()}!\";\n",
					"});"
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"// UDFs are run on the Microsoft.Spark.Worker process. The package assemblies\n",
					"// defined as a Udf depedency are shipped to the Worker so they are available\n",
					"// at the time of execution.\n",
					"df.Select(mathNetUdf(df[\"name\"])).Show();\n",
					"\n",
					"df.Select(numSharpUdf(df[\"name\"])).Show();\n",
					"\n",
					"// We can also chain udfs.\n",
					"df.Select(mathNetUdf(numSharpUdf(df[\"name\"])))"
				],
				"execution_count": 32
			},
			{
				"cell_type": "code",
				"source": [
					"// Utility for obtaining credentials (tokens and keys) for Synapse resources.\n",
					"// Credentials methods https://dev.azure.com/dnceng/internal/_git/dotnet-spark-extensions?path=%2Fsrc%2FMicrosoft.Spark.Extensions.Azure.Synapse.Analytics%2FNotebook%2FMSSparkUtils%2FCredentials.cs\n",
					"using Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Notebook.MSSparkUtils;\n",
					"\n",
					"// Note that the help message is the help message returned by the Scala implementation. This needs to be updated and addressed in a future version.\n",
					"Console.WriteLine($\"Help:\\n{Credentials.Help()}\");"
				],
				"execution_count": 33
			},
			{
				"cell_type": "code",
				"source": [
					"// Utility for obtaining environment metadata for Synapse.\n",
					"// Env methods https://dev.azure.com/dnceng/internal/_git/dotnet-spark-extensions?path=%2Fsrc%2FMicrosoft.Spark.Extensions.Azure.Synapse.Analytics%2FNotebook%2FMSSparkUtils%2FEnv.cs\n",
					"Console.WriteLine($\"UserName: {Env.GetUserName()}\");\n",
					"Console.WriteLine($\"UserId: {Env.GetUserId()}\");\n",
					"Console.WriteLine($\"WorkspaceName: {Env.GetWorkspaceName()}\");\n",
					"Console.WriteLine($\"PoolName: {Env.GetPoolName()}\");\n",
					"Console.WriteLine($\"ClusterId: {Env.GetClusterId()}\");\n",
					"\n",
					"// Note that the help message is the help message returned by the Scala implementation. This needs to be updated and addressed in a future version.\n",
					"Console.WriteLine($\"Help:\\n{Env.Help()}\");"
				],
				"execution_count": 34
			},
			{
				"cell_type": "code",
				"source": [
					"// Utility for filesystem operations in Synapse notebook\n",
					"// FS methods https://dev.azure.com/dnceng/internal/_git/dotnet-spark-extensions?path=%2Fsrc%2FMicrosoft.Spark.Extensions.Azure.Synapse.Analytics%2FNotebook%2FMSSparkUtils%2FFS.cs\n",
					"// FileInfo methods https://dev.azure.com/dnceng/internal/_git/dotnet-spark-extensions?path=%2Fsrc%2FMicrosoft.Spark.Extensions.Azure.Synapse.Analytics%2FNotebook%2FMSSparkUtils%2FFileInfo.cs\n",
					"\n",
					"// Note that the help message is the help message returned by the Scala implementation. This needs to be updated and addressed in a future version.\n",
					"FS.Help(\"\");"
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"source": [
					"// Utility for notebook operations (e.g, chaining Synapse notebooks together)\n",
					"// Notebook methods https://dev.azure.com/dnceng/internal/_git/dotnet-spark-extensions?path=%2Fsrc%2FMicrosoft.Spark.Extensions.Azure.Synapse.Analytics%2FNotebook%2FMSSparkUtils%2FNotebook.cs\n",
					"\n",
					"// Note that the help message is the help message returned by the Scala implementation. This needs to be updated and addressed in a future version.\n",
					"Notebook.Help(\"\");"
				],
				"execution_count": 36
			},
			{
				"cell_type": "code",
				"source": [
					"using Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Notebook.Visualization;\n",
					"// Construct an specific html fragment to synapse notebook front-end for rendering\n",
					"// based on user-input html content.\n",
					"DisplayHTML(\"<h1>Hello World</h1>\");"
				],
				"execution_count": 37
			},
			{
				"cell_type": "code",
				"source": [
					"using Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Utils;\n",
					"\n",
					"// Note that the help message is the help message returned by the Scala implementation. This needs to be updated and addressed in a future version.\n",
					"// TODO: Methodname needs to be uppercase.\n",
					"Console.WriteLine($\"Help:\\n{TokenLibrary.help()}\");"
				],
				"execution_count": 38
			},
			{
				"cell_type": "code",
				"source": [
					"// Curious about the version of Spark .NET currently installed?\n",
					"// Let's use the following method to find out!\n",
					"using Microsoft.Spark.Experimental.Sql;\n",
					"spark.GetAssemblyInfo()"
				],
				"execution_count": 40
			},
			{
				"cell_type": "code",
				"source": [
					"// Current version of the dotnet-interactive REPL.\n",
					"#!about"
				],
				"execution_count": 41
			},
			{
				"cell_type": "code",
				"source": [
					"// We can even run powershell core commands\n",
					"#!pwsh\n",
					"cat /etc/hosts"
				],
				"execution_count": 42
			},
			{
				"cell_type": "code",
				"source": [
					"// We can also run F# code\n",
					"#!fsharp\n",
					"open System\n",
					"printfn \"Hello World from F#!\""
				],
				"execution_count": 43
			},
			{
				"cell_type": "code",
				"source": [
					"// Whatever code is deemed invalid by the C# Compiler, is invalid here too \n",
					"var z = 12345"
				],
				"execution_count": 44
			},
			{
				"cell_type": "code",
				"source": [
					"// You could write code that throws exceptions and they bubble up to the notebook\n",
					"throw new Exception(\"watzzz\");"
				],
				"execution_count": 45
			}
		]
	}
}