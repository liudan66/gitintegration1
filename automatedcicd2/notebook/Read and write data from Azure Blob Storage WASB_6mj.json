{
	"name": "Read and write data from Azure Blob Storage WASB_6mj",
	"properties": {
		"folder": {
			"name": "scala"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "small",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "a8db71f6-b25d-4dd7-90ee-8bb2a6248de3"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_spark",
				"display_name": "scala"
			},
			"language_info": {
				"name": "scala"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
				"name": "small",
				"type": "Spark",
				"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Access data on Azure Storage Blob (WASB) with Synapse Spark\n",
					"\n",
					"You can access data on Azure Storage Blob (WASB) with Synapse Spark via following URL:\n",
					"\n",
					"    wasb[s]://<container_name>@<storage_account_name>.blob.core.windows.net/<path>\n",
					"\n",
					"This notebook provides examples of how to read data from WASB into a Spark context and how to write the output of Spark jobs directly into a WASB location."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Load a sample data\n",
					"\n",
					"Let's first load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) from Azure Open datasets as a sample."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"// set blob storage account connection for open dataset\n",
					"\n",
					"val hol_blob_account_name = \"azureopendatastorage\"\n",
					"val hol_blob_container_name = \"holidaydatacontainer\"\n",
					"val hol_blob_relative_path = \"Processed\"\n",
					"val hol_blob_sas_token = \"\"\n",
					"\n",
					"val hol_wasbs_path = f\"wasbs://$hol_blob_container_name@$hol_blob_account_name.blob.core.windows.net/$hol_blob_relative_path\"\n",
					"spark.conf.set(f\"fs.azure.sas.$hol_blob_container_name.$hol_blob_account_name.blob.core.windows.net\",hol_blob_sas_token)"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"source": [
					"// load the sample data as a Spark DataFrame\n",
					"val hol_df = spark.read.parquet(hol_wasbs_path) \n",
					"hol_df.show(5, truncate = false)"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Write data to Azure Storage Blob\n",
					"\n",
					"We are going to write the spark dateframe to your Azure Blob Storage (WASB) path using **shared access signature (sas)**. Go to [Azure Portal](https://portal.azure.com/), open your Azure storage blob, select **shared access signature** in the **settings** and generate your sas token. Please make sure to allow contatiner level read and write permission. Fill in the access info for your Azure storage blob in the cell below. \n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"// set your blob storage account connection\n",
					"\n",
					"val blob_account_name = \"samplenbblob\" // replace with your blob name\n",
					"val blob_container_name = \"data\" //replace with your container name\n",
					"val blob_relative_path = \"samplenb/\" //replace with your relative folder path\n",
					"val blob_sas_token = \"?sv=2019-02-02&ss=b&srt=sco&sp=rwdlac&se=2021-03-23T17:05:16Z&st=2020-03-24T09:05:16Z&spr=https,http&sig=drtIrL68s07nPW0Q9WEb5XFL6y5Eb7%2BOpmpxGyAHLaw%3D\" //replace with your sas token\n",
					"\n",
					"val wasbs_path = f\"wasbs://$blob_container_name@$blob_account_name.blob.core.windows.net/$blob_relative_path\"\n",
					"spark.conf.set(f\"fs.azure.sas.$blob_container_name.$blob_account_name.blob.core.windows.net\",blob_sas_token)\n",
					""
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Save a dataframe as Parquet, JSON or CSV\n",
					"If you have a dataframe, you can save it to Parquet or JSON with the .write.parquet(), .write.json() and .write.csv() methods respectively.\n",
					"\n",
					"Dataframes can be saved in any format, regardless of the input format.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"// set the path for the output file\n",
					"\n",
					"val parquet_path = wasbs_path + \"holiday.parquet\"\n",
					"val json_path = wasbs_path + \"holiday.json\"\n",
					"val csv_path = wasbs_path + \"holiday.csv\""
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"import org.apache.spark.sql.SaveMode\n",
					"\n",
					"hol_df.write.mode(SaveMode.Overwrite).parquet(parquet_path)\n",
					"hol_df.write.mode(SaveMode.Overwrite).json(json_path)\n",
					"hol_df.write.mode(SaveMode.Overwrite).option(\"header\", \"true\").csv(csv_path)"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Save a dataframe as text files\n",
					"If you have a dataframe that you want ot save as text file, you must first covert it to an RDD and then save that RDD as a text file.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"// Define the text file path and covert spark dataframe into RDD\n",
					"val text_path = wasbs_path + \"holiday02233.txt\"\n",
					"val hol_RDD = hol_df.rdd"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"source": [
					"If you have an RDD, you can convert it to a text file like the following:\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"// Save RDD as text file\n",
					"hol_RDD.saveAsTextFile(text_path)"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"source": [
					"# Read data from Azure Storage Blob\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Create a dataframe from parquet files\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"val df_parquet = spark.read.parquet(parquet_path)"
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Create a dataframe from JSON files\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"val df_json = spark.read.json(json_path)"
				],
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Create a dataframe from CSV files\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"val df_csv = spark.read.option(\"header\", \"true\").csv(csv_path)"
				],
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Create an RDD from text file\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"val text = sc.textFile(text_path)"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"source": [
					"text.take(5).foreach(println)"
				],
				"execution_count": 13
			}
		]
	}
}