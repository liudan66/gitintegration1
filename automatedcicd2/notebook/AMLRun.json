{
	"name": "AMLRun",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpoolaml",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"noPermissionToViewOutput": true,
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/8b3b8a60-1dd0-4824-8770-2ed6a55d8e27/resourceGroups/yifso-synapse-rg/providers/Microsoft.Synapse/workspaces/yifso1022scus/bigDataPools/sparkpoolaml",
				"name": "sparkpoolaml",
				"type": "Spark",
				"endpoint": "https://yifso1022scus.dev.azuresynapse-dogfood.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpoolaml",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56
			}
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"inputCollapsed": true,
					"jupyter": {
						"source_hidden": true
					}
				},
				"source": [
					"# -------------------------------------------------------------------------------------------------\n",
					"# TODO: Remove this code cell when the latest version of the AML SDK is deployed to the base image.\n",
					"# -------------------------------------------------------------------------------------------------\n",
					"\n",
					"def parse_target(target, add_managed_dataset_prefix=False):\n",
					"    from azureml.data.azure_storage_datastore import AbstractAzureStorageDatastore\n",
					"    from azureml.data.azure_data_lake_datastore import AbstractADLSDatastore\n",
					"    from azureml.data.datapath import DataPath\n",
					"    #from azureml.data.constants import MANAGED_DATASET\n",
					"    MANAGED_DATASET = 'managed-dataset'\n",
					"\n",
					"    datastore = None\n",
					"    relative_path = None\n",
					"\n",
					"    if isinstance(target, AbstractAzureStorageDatastore) or isinstance(target, AbstractADLSDatastore):\n",
					"        datastore = target\n",
					"        relative_path = MANAGED_DATASET if add_managed_dataset_prefix else '/'\n",
					"    elif isinstance(target, DataPath):\n",
					"        datastore = target._datastore\n",
					"        relative_path = (MANAGED_DATASET if add_managed_dataset_prefix else '/') \\\n",
					"            if target.path_on_datastore is None else target.path_on_datastore\n",
					"    elif isinstance(target, tuple) and len(target) == 2:\n",
					"        datastore = target[0]\n",
					"        relative_path = target[1]\n",
					"    if not isinstance(datastore, AbstractAzureStorageDatastore) and not isinstance(datastore, AbstractADLSDatastore):\n",
					"        raise ValueError(\"The target type is not supported, target: {}\".format(target))\n",
					"\n",
					"    return datastore, relative_path\n",
					"\n",
					"\n",
					"def _set_spark_config(datastore):\n",
					"    from pyspark.sql import SparkSession\n",
					"    from azureml.data.constants import AZURE_BLOB, AZURE_DATA_LAKE_GEN2, AZURE_DATA_LAKE\n",
					"    spark = SparkSession.builder.getOrCreate()\n",
					"\n",
					"    if datastore.datastore_type == AZURE_BLOB:\n",
					"        account_name = datastore.account_name\n",
					"        account_key = datastore.account_key\n",
					"        endpoint = datastore.endpoint\n",
					"        spark.conf.set('fs.azure.account.key.{}.blob.{}'.format(account_name, endpoint), account_key)\n",
					"    elif datastore.datastore_type == AZURE_DATA_LAKE_GEN2:\n",
					"        account_name = datastore.account_name\n",
					"        client_id = datastore.client_id\n",
					"        client_secret = datastore.client_secret\n",
					"        endpoint = datastore.endpoint\n",
					"        tenant_id = datastore.tenant_id\n",
					"        authority_url = datastore.authority_url\n",
					"        prefix = \"fs.azure.account\"\n",
					"        provider = \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"\n",
					"        storage_account = \"{}.dfs.{}\".format(account_name, endpoint)\n",
					"        spark.conf.set(\"{}.auth.type.{}\".format(prefix, storage_account), \"OAuth\")\n",
					"        spark.conf.set(\"{}.oauth.provider.type.{}\".format(prefix, storage_account), provider)\n",
					"        spark.conf.set(\"{}.oauth2.client.id.{}\".format(prefix, storage_account), client_id)\n",
					"        spark.conf.set(\"{}.oauth2.client.secret.{}\".format(prefix, storage_account), client_secret)\n",
					"        spark.conf.set(\"{}.oauth2.client.endpoint.{}\".format(prefix, storage_account),\n",
					"                       \"{}/{}/oauth2/token\".format(authority_url, tenant_id))\n",
					"    elif datastore.datastore_type == AZURE_DATA_LAKE:\n",
					"        client_id = datastore.client_id\n",
					"        client_secret = datastore.client_secret\n",
					"        tenant_id = datastore.tenant_id\n",
					"        authority_url = datastore.authority_url\n",
					"        prefix = \"fs.adl\"  # dfs.adls deprecated\n",
					"        spark.conf.set(\"{}.oauth2.access.token.provider.type\".format(prefix), \"ClientCredential\")\n",
					"        spark.conf.set(\"{}.oauth2.client.id\".format(prefix), client_id)\n",
					"        spark.conf.set(\"{}.oauth2.credential\".format(prefix), client_secret)\n",
					"        spark.conf.set(\"{}.oauth2.refresh.url\".format(prefix),\n",
					"                       \"{}/{}/oauth2/token\".format(authority_url, tenant_id))\n",
					"    else:\n",
					"        raise ValueError(\n",
					"            \"The datastore type {} is not supported.\".format(datastore.datastore_type))\n",
					"\n",
					"\n",
					"def _get_output_uri(datastore, path):\n",
					"    from azureml.data.constants import AZURE_BLOB, AZURE_DATA_LAKE_GEN2, AZURE_DATA_LAKE\n",
					"\n",
					"    if datastore.datastore_type == AZURE_BLOB:\n",
					"        output_uri = 'wasbs://{}@{}.blob.{}/{}'.format(datastore.container_name, datastore.account_name,\n",
					"                                                       datastore.endpoint, path)\n",
					"    elif datastore.datastore_type == AZURE_DATA_LAKE_GEN2:\n",
					"        output_uri = 'abfss://{}@{}.dfs.{}/{}'.format(datastore.container_name, datastore.account_name,\n",
					"                                                      datastore.endpoint, path)\n",
					"    elif datastore.datastore_type == AZURE_DATA_LAKE:\n",
					"        output_uri = 'adl://{}.azuredatalakestore.net/{}'.format(datastore.store_name, path)\n",
					"    else:\n",
					"        raise ValueError(\n",
					"            \"The datastore type {} is not supported.\".format(datastore.datastore_type))\n",
					"\n",
					"    return output_uri\n",
					"\n",
					"\n",
					"def write_spark_dataframe(spark_dataframe, datastore, relative_path_with_guid, show_progress):\n",
					"    console = get_progress_logger(show_progress)\n",
					"    _set_spark_config(datastore)\n",
					"    output_uri = _get_output_uri(datastore, relative_path_with_guid)\n",
					"\n",
					"    console(\"Writing spark dataframe to {}\".format(relative_path_with_guid))\n",
					"    spark_dataframe.write.mode(\"overwrite\").option(\"header\", \"true\").format(\"parquet\").save(output_uri)\n",
					"\n",
					"\n",
					"def get_progress_logger(show_progress):\n",
					"    import sys\n",
					"    console = sys.stdout\n",
					"\n",
					"    def log(message):\n",
					"        if show_progress:\n",
					"            console.write(\"{}\\n\".format(message))\n",
					"\n",
					"    return log\n",
					"\n",
					"def _check_type(arg, arg_name, expected_type):\n",
					"    if not isinstance(arg, expected_type):\n",
					"        raise ValueError(\"Expected {} of type {} but received {}\".format(arg_name, expected_type, type(arg)))\n",
					"\n",
					"def register_spark_dataframe(dataframe, target, name, description=None, tags=None, show_progress=True):\n",
					"        \"\"\"Create a dataset from spark dataframe.\n",
					"\n",
					"        :param dataframe: Required, in memory dataframe to be uploaded.\n",
					"        :type dataframe: pyspark.sql.DataFrame\n",
					"        :param target: Required, the datastore path where the dataframe parquet data will be uploaded to.\n",
					"            A guid folder will be generated under the target path to avoid conflict.\n",
					"        :type target: azureml.data.datapath.DataPath, azureml.core.datastore.Datastore\n",
					"            or tuple(azureml.core.datastore.Datastore, str) object\n",
					"        :param name: Required, the name of the registered dataset.\n",
					"        :type name: str\n",
					"        :param description: Optional. A text description of the dataset. Defaults to None.\n",
					"        :type description: str\n",
					"        :param tags: Optional. Dictionary of key value tags to give the dataset. Defaults to None.\n",
					"        :type tags: dict[str, str]\n",
					"        :param show_progress: Optional, indicates whether to show progress of the upload in the console.\n",
					"            Defaults to be True.\n",
					"        :type show_progress: bool\n",
					"        :return: The registered dataset.\n",
					"        :rtype: azureml.data.TabularDataset\n",
					"        \"\"\"\n",
					"        from azureml.data.datapath import DataPath\n",
					"        from pyspark.sql import DataFrame\n",
					"        from uuid import uuid4\n",
					"\n",
					"        console = get_progress_logger(show_progress)\n",
					"\n",
					"        console(\"Validating arguments.\")\n",
					"        _check_type(dataframe, \"dataframe\", DataFrame)\n",
					"        _check_type(name, \"name\", str)\n",
					"        datastore, relative_path = parse_target(target, True)\n",
					"        console(\"Arguments validated.\")\n",
					"\n",
					"        guid = uuid4()\n",
					"        relative_path_with_guid = \"{}/{}\".format(relative_path, guid)\n",
					"        write_spark_dataframe(dataframe, datastore, relative_path_with_guid, show_progress)\n",
					"\n",
					"        console(\"Creating new dataset\")\n",
					"        datapath = DataPath(datastore, \"/{}/*.parquet\".format(relative_path_with_guid))\n",
					"        saved_dataset = Dataset.Tabular.from_parquet_files(datapath)\n",
					"\n",
					"        console(\"Registering new dataset\")\n",
					"        registered_dataset = saved_dataset.register(datastore.workspace, name,\n",
					"                                                    description=description,\n",
					"                                                    tags=tags,\n",
					"                                                    create_new_version=True)\n",
					"        console(\"Successfully created and registered a new dataset.\")\n",
					"\n",
					"        return registered_dataset\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"import azureml.core\n",
					"\n",
					"from azureml.core import Experiment, Workspace, Dataset, Datastore\n",
					"from azureml.train.automl import AutoMLConfig"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"subscription_id = \"051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3\"\n",
					"resource_group = \"chaxu-test\"\n",
					"workspace_name = \"chaxuamleus\"\n",
					"experiment_name = \"yifso1022scus-nyc_taxi-legacy-notebook\"\n",
					"\n",
					"ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
					"experiment = Experiment(ws, experiment_name)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"df = spark.sql(\"SELECT * FROM default.nyc_taxi\")\n",
					"\n",
					"datastore = Datastore.get_default(ws)\n",
					"dataset = register_spark_dataframe(df, datastore, name = experiment_name + \"-dataset\")\n",
					"dataset_train, dataset_test = dataset.random_split(percentage = 0.8)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"automl_config = AutoMLConfig(spark_context = sc,\n",
					"                             task = \"classification\",\n",
					"                             training_data = dataset_train,\n",
					"                             label_column_name = \"tipped\",\n",
					"                             primary_metric = \"accuracy\",\n",
					"                             experiment_timeout_hours = 1,\n",
					"                             max_concurrent_iterations = 2,\n",
					"                             enable_onnx_compatible_models = True)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"run = experiment.submit(automl_config)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"displayHTML(\"<a href={} target='_blank'>Your experiment in Azure Machine Learning portal: {}</a>\".format(run.get_portal_url(), run.id))"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"inputCollapsed": true,
					"jupyter": {
						"source_hidden": true
					}
				},
				"source": [
					"run.wait_for_completion()\n",
					"\n",
					"# # If you want to register the best model, please uncomment the following codes\n",
					"# import onnxruntime\n",
					"# import mlflow\n",
					"# import mlflow.onnx\n",
					"\n",
					"# from mlflow.models.signature import ModelSignature\n",
					"# from mlflow.types import DataType\n",
					"# from mlflow.types.schema import ColSpec, Schema\n",
					"\n",
					"# # Get best model from automl run\n",
					"# best_run, onnx_model = run.get_output(return_onnx_model=True)\n",
					"\n",
					"# # Define utility functions to infer the schema of ONNX model\n",
					"# def _infer_schema(data):\n",
					"#     res = []\n",
					"#     for _, col in enumerate(data):\n",
					"#         t = col.type.replace(\"tensor(\", \"\").replace(\")\", \"\")\n",
					"#         if t in [\"bool\"]:\n",
					"#             dt = DataType.boolean\n",
					"#         elif t in [\"int8\", \"uint8\", \"int16\", \"uint16\", \"int32\"]:\n",
					"#             dt = DateType.integer\n",
					"#         elif t in [\"uint32\", \"int64\"]:\n",
					"#             dt = DataType.long\n",
					"#         elif t in [\"float16\", \"bfloat16\", \"float\"]:\n",
					"#             dt = DataType.float\n",
					"#         elif t in [\"double\"]:\n",
					"#             dt = DataType.double\n",
					"#         elif t in [\"string\"]:\n",
					"#             dt = DataType.string\n",
					"#         else:\n",
					"#             raise Exception(\"Unsupported type: \" + t)\n",
					"#         res.append(ColSpec(type=dt, name=col.name))\n",
					"#     return Schema(res)\n",
					"\n",
					"# def _infer_signature(onnx_model):\n",
					"#     onnx_model_bytes = onnx_model.SerializeToString()\n",
					"#     onnx_runtime = onnxruntime.InferenceSession(onnx_model_bytes)\n",
					"#     inputs = _infer_schema(onnx_runtime.get_inputs())\n",
					"#     outputs = _infer_schema(onnx_runtime.get_outputs())\n",
					"#     return ModelSignature(inputs, outputs)\n",
					"\n",
					"# # Infer signature of ONNX model\n",
					"# signature = _infer_signature(onnx_model)\n",
					"\n",
					"# artifact_path = experiment_name + \"_artifact\"\n",
					"# mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
					"# mlflow.set_experiment(experiment_name)\n",
					"\n",
					"# with mlflow.start_run() as run:\n",
					"#     # Save the model to the outputs directory for capture\n",
					"#     mlflow.onnx.log_model(onnx_model, artifact_path, signature=signature)\n",
					"\n",
					"#     # Register the model to AML model registry\n",
					"#     mlflow.register_model(\"runs:/\" + run.info.run_id + \"/\" + artifact_path, \"yifso1022scus-nyc_taxi-legacy-notebook-Best\")"
				],
				"execution_count": null
			}
		]
	}
}