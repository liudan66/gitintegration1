{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "bdbj0302ws-git"
		},
		"AzureBlobStorage1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureBlobStorage1'"
		},
		"AzureBlobStorage1106_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureBlobStorage1106'"
		},
		"AzureBlobStorage1124_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureBlobStorage1124'"
		},
		"AzureBlobStorage2_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureBlobStorage2'"
		},
		"AzureBlobStorage4_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureBlobStorage4'"
		},
		"AzureBlobStorage_TestAKV_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureBlobStorage_TestAKV'"
		},
		"AzureDataExplorer2_servicePrincipalKey": {
			"type": "secureString",
			"metadata": "Secure string for 'servicePrincipalKey' of 'AzureDataExplorer2'"
		},
		"AzureDataLakeStorage0304_servicePrincipalCredential": {
			"type": "secureString",
			"metadata": "Secure string for 'servicePrincipalCredential' of 'AzureDataLakeStorage0304'"
		},
		"AzureDataLakeStorage030402_servicePrincipalCredential": {
			"type": "secureString",
			"metadata": "Secure string for 'servicePrincipalCredential' of 'AzureDataLakeStorage030402'"
		},
		"AzureDataLakeStorage030404_servicePrincipalCredential": {
			"type": "secureString",
			"metadata": "Secure string for 'servicePrincipalCredential' of 'AzureDataLakeStorage030404'"
		},
		"AzureDataLakeStorage1_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1'"
		},
		"AzureDataLakeStorage1105_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1105'"
		},
		"AzureDataLakeStorage1107_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1107'"
		},
		"AzureDataLakeStorage1108_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1108'"
		},
		"AzureDataLakeStorage1110_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1110'"
		},
		"AzureDataLakeStorage1111_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1111'"
		},
		"AzureDataLakeStorage1112_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1112'"
		},
		"AzureDataLakeStorage1113_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1113'"
		},
		"AzureDataLakeStorage1114_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1114'"
		},
		"AzureDataLakeStorage1115_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1115'"
		},
		"AzureDataLakeStorage2_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage2'"
		},
		"AzureDataLakeStorage3_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage3'"
		},
		"AzureDataLakeStorage4_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage4'"
		},
		"CosmosDb1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'CosmosDb1'"
		},
		"CosmosDbMongoDbApi_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'CosmosDbMongoDbApi'"
		},
		"CosmosDbMongoDbApi0304_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'CosmosDbMongoDbApi0304'"
		},
		"CosmosDbMongoDbApi1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'CosmosDbMongoDbApi1'"
		},
		"CosmosDbSQLAPI030401_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'CosmosDbSQLAPI030401'"
		},
		"CosmosDbsqlapi0304_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'CosmosDbsqlapi0304'"
		},
		"Kusto0304_servicePrincipalKey": {
			"type": "secureString",
			"metadata": "Secure string for 'servicePrincipalKey' of 'Kusto0304'"
		},
		"Kusto030402_servicePrincipalKey": {
			"type": "secureString",
			"metadata": "Secure string for 'servicePrincipalKey' of 'Kusto030402'"
		},
		"Kusto030403_servicePrincipalKey": {
			"type": "secureString",
			"metadata": "Secure string for 'servicePrincipalKey' of 'Kusto030403'"
		},
		"Kusto1105_servicePrincipalKey": {
			"type": "secureString",
			"metadata": "Secure string for 'servicePrincipalKey' of 'Kusto1105'"
		},
		"MongoDbApi1105_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'MongoDbApi1105'"
		},
		"SQLApi1105_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'SQLApi1105'"
		},
		"bdbj0105ws-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'bdbj0105ws-WorkspaceDefaultSqlServer'"
		},
		"bdbj0112ws-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'bdbj0112ws-WorkspaceDefaultSqlServer'"
		},
		"bdbj0119ws-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'bdbj0119ws-WorkspaceDefaultSqlServer'"
		},
		"bdbj0202ws-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'bdbj0202ws-WorkspaceDefaultSqlServer'"
		},
		"bdbj0302ws-git-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'bdbj0302ws-git-WorkspaceDefaultSqlServer'"
		},
		"bdbj1027ws-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'bdbj1027ws-WorkspaceDefaultSqlServer'"
		},
		"bdbj1103ws-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'bdbj1103ws-WorkspaceDefaultSqlServer'"
		},
		"bdbj1113ws-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'bdbj1113ws-WorkspaceDefaultSqlServer'"
		},
		"bdbj1119ws-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'bdbj1119ws-WorkspaceDefaultSqlServer'"
		},
		"bdbj1208ws-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'bdbj1208ws-WorkspaceDefaultSqlServer'"
		},
		"bigdata0925ws-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'bigdata0925ws-WorkspaceDefaultSqlServer'"
		},
		"bigdata1128ws-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'bigdata1128ws-WorkspaceDefaultSqlServer'"
		},
		"bigdata1128ws2-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'bigdata1128ws2-WorkspaceDefaultSqlServer'"
		},
		"bigdataqa0301ws-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'bigdataqa0301ws-WorkspaceDefaultSqlServer'"
		},
		"bigdataqa0512ws2-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'bigdataqa0512ws2-WorkspaceDefaultSqlServer'"
		},
		"bigdataqa0917ws-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'bigdataqa0917ws-WorkspaceDefaultSqlServer'"
		},
		"bigdataqa0924ws-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'bigdataqa0924ws-WorkspaceDefaultSqlServer'"
		},
		"catestws2-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'catestws2-WorkspaceDefaultSqlServer'"
		},
		"chaxuamleus_servicePrincipalKey": {
			"type": "secureString",
			"metadata": "Secure string for 'servicePrincipalKey' of 'chaxuamleus'"
		},
		"cicdtestws-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'cicdtestws-WorkspaceDefaultSqlServer'"
		},
		"dancicdtest-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'dancicdtest-WorkspaceDefaultSqlServer'"
		},
		"hozhaobdbj_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'hozhaobdbj'"
		},
		"ltianscusworkspace-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'ltianscusworkspace-WorkspaceDefaultSqlServer'"
		},
		"xiaochuan_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'xiaochuan'"
		},
		"yifso1022scus-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'yifso1022scus-WorkspaceDefaultSqlServer'"
		},
		"AzureBlobStorage3_properties_typeProperties_connectionString_secretName": {
			"type": "string",
			"defaultValue": "Accountkeyblob01"
		},
		"AzureBlobStorage_0305AKYTEST_properties_typeProperties_connectionString_secretName": {
			"type": "string",
			"defaultValue": "Accountkeyblob01"
		},
		"AzureBlobStorage_ci_test_properties_typeProperties_connectionString_secretName": {
			"type": "string",
			"defaultValue": "Accountkeyblob01"
		},
		"AzureBlobStorage_prod_test_properties_typeProperties_connectionString_secretName": {
			"type": "string",
			"defaultValue": "Accountkeyblob01"
		},
		"AzureDataExplorer1_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72bfe100-a448-4f25-af39-5b36c7bb62ef"
		},
		"AzureDataExplorer1_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "8f6caa4c-31d9-4562-8084-bcbf13bb0068"
		},
		"AzureDataExplorer1_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "bdbjkustodatabaseqingtest"
		},
		"AzureDataExplorer2_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72bfe100-a448-4f25-af39-5b36c7bb62ef"
		},
		"AzureDataExplorer2_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "8f6caa4c-31d9-4562-8084-bcbf13bb0068"
		},
		"AzureDataExplorer2_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "bdbjkustodatabaseqingtest"
		},
		"AzureDataLakeStorage0304_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bdbjgen2qingtest.dfs.core.windows.net"
		},
		"AzureDataLakeStorage0304_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72bfe100-a448-4f25-af39-5b36c7bb62ef"
		},
		"AzureDataLakeStorage0304_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "8f6caa4c-31d9-4562-8084-bcbf13bb0068"
		},
		"AzureDataLakeStorage030401_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bdbjgen2qingtest.dfs.core.windows.net"
		},
		"AzureDataLakeStorage030401_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72bfe100-a448-4f25-af39-5b36c7bb62ef"
		},
		"AzureDataLakeStorage030401_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "8f6caa4c-31d9-4562-8084-bcbf13bb0068"
		},
		"AzureDataLakeStorage030402_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bdbjgen2qingtest.dfs.core.windows.net"
		},
		"AzureDataLakeStorage030402_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72bfe100-a448-4f25-af39-5b36c7bb62ef"
		},
		"AzureDataLakeStorage030402_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "8f6caa4c-31d9-4562-8084-bcbf13bb0068"
		},
		"AzureDataLakeStorage030403_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bdbjgen2qingtest.dfs.core.windows.net"
		},
		"AzureDataLakeStorage030403_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72bfe100-a448-4f25-af39-5b36c7bb62ef"
		},
		"AzureDataLakeStorage030403_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "8f6caa4c-31d9-4562-8084-bcbf13bb0068"
		},
		"AzureDataLakeStorage030404_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bdbjgen2qingtest.dfs.core.windows.net"
		},
		"AzureDataLakeStorage0305_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bdbjgen2qingtest.dfs.core.windows.net"
		},
		"AzureDataLakeStorage1_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhaotestgen2.dfs.core.windows.net"
		},
		"AzureDataLakeStorage1105_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhao0924gen2.dfs.core.windows.net"
		},
		"AzureDataLakeStorage1107_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhao0924gen2.dfs.core.windows.net"
		},
		"AzureDataLakeStorage1108_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhao0924gen2.dfs.core.windows.net"
		},
		"AzureDataLakeStorage1110_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhaotestgen2.dfs.core.windows.net"
		},
		"AzureDataLakeStorage1111_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhao0924gen2.dfs.core.windows.net"
		},
		"AzureDataLakeStorage1112_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhao0924gen2.dfs.core.windows.net"
		},
		"AzureDataLakeStorage1113_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhaogen2.dfs.core.windows.net"
		},
		"AzureDataLakeStorage1114_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://storagegen2qingtest.dfs.core.windows.net"
		},
		"AzureDataLakeStorage1115_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://accessibilitytest.dfs.core.windows.net"
		},
		"AzureDataLakeStorage2_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhaotestgen2.dfs.core.windows.net"
		},
		"AzureDataLakeStorage3_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhaotestgen2.dfs.core.windows.net"
		},
		"AzureDataLakeStorage30405_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bdbjgen2qingtest.dfs.core.windows.net"
		},
		"AzureDataLakeStorage4_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhaotestgen2.dfs.core.windows.net"
		},
		"AzureDataLakeStore1105_properties_typeProperties_dataLakeStoreUri": {
			"type": "string",
			"defaultValue": "https://zzytest.azuredatalakestore.net/webhdfs/v1"
		},
		"AzureDataLakeStore1105_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"AzureDataLakeStore1105_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": "8b3b8a60-1dd0-4824-8770-2ed6a55d8e27"
		},
		"AzureDataLakeStore1105_properties_typeProperties_resourceGroupName": {
			"type": "string",
			"defaultValue": "zzy-test-rg"
		},
		"AzureDataLakeStore1106_properties_typeProperties_dataLakeStoreUri": {
			"type": "string",
			"defaultValue": "adl://storagegen1qingtest.azuredatalakestore.net"
		},
		"AzureDataLakeStore1106_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"AzureDataLakeStore1106_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": ""
		},
		"AzureDataLakeStore1106_properties_typeProperties_resourceGroupName": {
			"type": "string",
			"defaultValue": "bigdataqa"
		},
		"AzureDataLakeStore1107_properties_typeProperties_dataLakeStoreUri": {
			"type": "string",
			"defaultValue": "https://storagegen1qingtest.azuredatalakestore.net/webhdfs/v1"
		},
		"AzureDataLakeStore1107_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"AzureDataLakeStore1107_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": "051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3"
		},
		"AzureDataLakeStore1107_properties_typeProperties_resourceGroupName": {
			"type": "string",
			"defaultValue": "bigdataqa"
		},
		"AzureDataLakeStore1108_properties_typeProperties_dataLakeStoreUri": {
			"type": "string",
			"defaultValue": "https://storagegen1qingtest.azuredatalakestore.net/webhdfs/v1"
		},
		"AzureDataLakeStore1108_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"AzureDataLakeStore1108_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": "051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3"
		},
		"AzureDataLakeStore1108_properties_typeProperties_resourceGroupName": {
			"type": "string",
			"defaultValue": "bigdataqa"
		},
		"AzureDataLakeStore1122_properties_typeProperties_dataLakeStoreUri": {
			"type": "string",
			"defaultValue": "https://lirsungen1.azuredatalakestore.net/webhdfs/v1"
		},
		"AzureDataLakeStore1122_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"AzureDataLakeStore1122_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": "051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3"
		},
		"AzureDataLakeStore1122_properties_typeProperties_resourceGroupName": {
			"type": "string",
			"defaultValue": "beijingtest"
		},
		"AzureDataLakeStore123_properties_typeProperties_dataLakeStoreUri": {
			"type": "string",
			"defaultValue": "https://storagegen1qingtest.azuredatalakestore.net/webhdfs/v1"
		},
		"AzureDataLakeStore123_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"AzureDataLakeStore123_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": "051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3"
		},
		"AzureDataLakeStore123_properties_typeProperties_resourceGroupName": {
			"type": "string",
			"defaultValue": "bigdataqa"
		},
		"AzureDataLakeStore456_properties_typeProperties_dataLakeStoreUri": {
			"type": "string",
			"defaultValue": "https://storagegen1qingtest.azuredatalakestore.net/webhdfs/v1"
		},
		"AzureDataLakeStore456_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"AzureDataLakeStore456_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": "051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3"
		},
		"AzureDataLakeStore456_properties_typeProperties_resourceGroupName": {
			"type": "string",
			"defaultValue": "bigdataqa"
		},
		"AzureKeyVault1_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://Dandatafactorykeyvault.vault.azure.net/"
		},
		"AzureKeyVault_bdbjqingtest_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://bdbjkeyvaultqingtest.vault.azure.net/"
		},
		"CosmosDbMongoDbApi_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "mongodbapidatabaseqingtest"
		},
		"CosmosDbMongoDbApi0304_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "mongodbapidatabaseqingtest"
		},
		"CosmosDbMongoDbApi1_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "yuwwang-mongo-db1"
		},
		"Gen20304_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bdbjgen2qingtest.dfs.core.windows.net"
		},
		"GitHublink_properties_typeProperties_username": {
			"type": "string",
			"defaultValue": "liudan"
		},
		"Kusto0304_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72bfe100-a448-4f25-af39-5b36c7bb62ef"
		},
		"Kusto0304_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "8f6caa4c-31d9-4562-8084-bcbf13bb0068"
		},
		"Kusto0304_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "bdbjkustodatabaseqingtest"
		},
		"Kusto030401_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72bfe100-a448-4f25-af39-5b36c7bb62ef"
		},
		"Kusto030401_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "8f6caa4c-31d9-4562-8084-bcbf13bb0068"
		},
		"Kusto030401_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "bdbjkustodatabaseqingtest"
		},
		"Kusto030402_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72bfe100-a448-4f25-af39-5b36c7bb62ef"
		},
		"Kusto030402_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "8f6caa4c-31d9-4562-8084-bcbf13bb0068"
		},
		"Kusto030402_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "bdbjkustodatabaseqingtest"
		},
		"Kusto030403_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72bfe100-a448-4f25-af39-5b36c7bb62ef"
		},
		"Kusto030403_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "8f6caa4c-31d9-4562-8084-bcbf13bb0068"
		},
		"Kusto030403_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "bdbjkustodatabaseqingtest"
		},
		"Kusto1105_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"Kusto1105_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "ac3a0e33-1db1-48ba-9b19-24ce0330ff5c"
		},
		"Kusto1105_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "cdndemo"
		},
		"MongoDbApi1105_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "yuwwang-mongo-db1"
		},
		"bdbj0105ws-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bdbjtestgen2.dfs.core.windows.net"
		},
		"bdbj0112ws-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bdbjtestgen2.dfs.core.windows.net"
		},
		"bdbj0119ws-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bdbjtestgen2.dfs.core.windows.net"
		},
		"bdbj0202ws-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bdbjtestgen2.dfs.core.windows.net"
		},
		"bdbj0302ws-git-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://honghaigen2.dfs.core.windows.net"
		},
		"bdbj1027ws-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhaobdbj.dfs.core.windows.net"
		},
		"bdbj1103ws-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bdbjtestgen2.dfs.core.windows.net"
		},
		"bdbj1113ws-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhaobdbj.dfs.core.windows.net"
		},
		"bdbj1119ws-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhaobdbj.dfs.core.windows.net"
		},
		"bdbj1208ws-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bdbjtestgen2.dfs.core.windows.net"
		},
		"bigdata0925ws-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhao0924gen2.dfs.core.windows.net"
		},
		"bigdata1128ws-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhao0924gen2.dfs.core.windows.net"
		},
		"bigdata1128ws2-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhao1128gen2.dfs.core.windows.net"
		},
		"bigdataqa0301ws-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhaogen2.dfs.core.windows.net"
		},
		"bigdataqa0512ws2-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhaogen2.dfs.core.windows.net"
		},
		"bigdataqa0917ws-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhao0917gen2.dfs.core.windows.net"
		},
		"bigdataqa0924ws-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhao0924gen2.dfs.core.windows.net"
		},
		"catestws2-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhaobdbj.dfs.core.windows.net"
		},
		"chaxuamleus_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": "051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3"
		},
		"chaxuamleus_properties_typeProperties_resourceGroupName": {
			"type": "string",
			"defaultValue": "chaxu-test"
		},
		"chaxuamleus_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "a82dd51d-9e81-4835-ae78-dae1b3ffae64"
		},
		"chaxuamleus_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"cicdtestws-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhao0924gen2.dfs.core.windows.net"
		},
		"dancicdtest-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://srbaccicd.dfs.core.windows.net"
		},
		"dansynapsebugbash-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://dansynapsestroage.dfs.core.windows.net"
		},
		"hozhaobdbj_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhaobdbj.dfs.core.windows.net/"
		},
		"ltianscusworkspace-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://ltianscusgen2.dfs.core.windows.net"
		},
		"nyc_tlc_fhv_sasUri": {
			"type": "secureString",
			"metadata": "Secure string for 'sasUri' of 'nyc_tlc_fhv'"
		},
		"us-employment-hours-earnings-state_sasUri": {
			"type": "secureString",
			"metadata": "Secure string for 'sasUri' of 'us-employment-hours-earnings-state'"
		},
		"workspacedeploydan5-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://dansynapsestroage.dfs.core.windows.net"
		},
		"yifso1022scus-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://yifsoadlsgen2westus2.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/small')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CopyPipeline_14p')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy_14p",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [
							{
								"name": "Source",
								"value": "hozhao/parquet/fact_sale.parquet"
							},
							{
								"name": "Destination",
								"value": "hozhao//"
							}
						],
						"typeProperties": {
							"source": {
								"type": "ParquetSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"validateDataConsistency": false
						},
						"inputs": [
							{
								"referenceName": "SourceDataset_14p",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DestinationDataset_14p",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2020-11-21T06:27:48Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/SourceDataset_14p')]",
				"[concat(variables('workspaceId'), '/datasets/DestinationDataset_14p')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CopyPipeline_o4a')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy_o4a",
						"description": "update test",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [
							{
								"name": "Source",
								"value": "hozhao/parquet/fact_sale.parquet"
							},
							{
								"name": "Destination",
								"value": "test//CDT1112.parquet"
							}
						],
						"typeProperties": {
							"source": {
								"type": "ParquetSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true
								}
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"validateDataConsistency": false
						},
						"inputs": [
							{
								"referenceName": "SourceDataset_o4a",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DestinationDataset_o4a",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2020-11-17T10:08:41Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/SourceDataset_o4a')]",
				"[concat(variables('workspaceId'), '/datasets/DestinationDataset_o4a')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CopyPipeline_wzv')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy_wzv",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [
							{
								"name": "Source",
								"value": "hozhao/parquet/fact_sale.parquet"
							},
							{
								"name": "Destination",
								"value": "hozhao//CDT_1117.json"
							}
						],
						"typeProperties": {
							"source": {
								"type": "ParquetSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true
								}
							},
							"sink": {
								"type": "JsonSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "JsonWriteSettings"
								}
							},
							"enableStaging": false,
							"validateDataConsistency": false,
							"translator": {
								"type": "TabularTranslator",
								"mappings": [
									{
										"source": {
											"name": "SaleKey",
											"type": "Int32"
										},
										"sink": {
											"path": "$['SaleKey']"
										}
									},
									{
										"source": {
											"name": "CityKey",
											"type": "Int32"
										},
										"sink": {
											"path": "$['CityKey']"
										}
									},
									{
										"source": {
											"name": "CustomerKey",
											"type": "Int32"
										},
										"sink": {
											"path": "$['CustomerKey']"
										}
									},
									{
										"source": {
											"name": "BillToCustomerKey",
											"type": "Int32"
										},
										"sink": {
											"path": "$['BillToCustomerKey']"
										}
									},
									{
										"source": {
											"name": "StockItemKey",
											"type": "Int32"
										},
										"sink": {
											"path": "$['StockItemKey']"
										}
									},
									{
										"source": {
											"name": "InvoiceDateKey",
											"type": "DateTime"
										},
										"sink": {
											"path": "$['InvoiceDateKey']"
										}
									},
									{
										"source": {
											"name": "DeliveryDateKey",
											"type": "DateTime"
										},
										"sink": {
											"path": "$['DeliveryDateKey']"
										}
									},
									{
										"source": {
											"name": "SalespersonKey",
											"type": "Int32"
										},
										"sink": {
											"path": "$['SalespersonKey']"
										}
									},
									{
										"source": {
											"name": "WWIInvoicID",
											"type": "Int32"
										},
										"sink": {
											"path": "$['WWIInvoicID']"
										}
									},
									{
										"source": {
											"name": "Description",
											"type": "String"
										},
										"sink": {
											"path": "$['Description']"
										}
									},
									{
										"source": {
											"name": "Package",
											"type": "String"
										},
										"sink": {
											"path": "$['Package']"
										}
									},
									{
										"source": {
											"name": "Quantity",
											"type": "Int32"
										},
										"sink": {
											"path": "$['Quantity']"
										}
									},
									{
										"source": {
											"name": "UnitPrice",
											"type": "Double"
										},
										"sink": {
											"path": "$['UnitPrice']"
										}
									},
									{
										"source": {
											"name": "TaxRate",
											"type": "Double"
										},
										"sink": {
											"path": "$['TaxRate']"
										}
									},
									{
										"source": {
											"name": "TotalExcludingTax",
											"type": "Double"
										},
										"sink": {
											"path": "$['TotalExcludingTax']"
										}
									},
									{
										"source": {
											"name": "TaxAmount",
											"type": "Double"
										},
										"sink": {
											"path": "$['TaxAmount']"
										}
									},
									{
										"source": {
											"name": "Profit",
											"type": "Double"
										},
										"sink": {
											"path": "$['Profit']"
										}
									},
									{
										"source": {
											"name": "TotalIncludingTax",
											"type": "Double"
										},
										"sink": {
											"path": "$['TotalIncludingTax']"
										}
									},
									{
										"source": {
											"name": "TotalDryItems",
											"type": "Int32"
										},
										"sink": {
											"path": "$['TotalDryItems']"
										}
									},
									{
										"source": {
											"name": "TotalChillerItems",
											"type": "Int32"
										},
										"sink": {
											"path": "$['TotalChillerItems']"
										}
									},
									{
										"source": {
											"name": "LineageKey",
											"type": "Int32"
										},
										"sink": {
											"path": "$['LineageKey']"
										}
									}
								]
							}
						},
						"inputs": [
							{
								"referenceName": "SourceDataset_wzv",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DestinationDataset_wzv",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "zhao"
				},
				"annotations": [],
				"lastPublishTime": "2020-11-21T07:03:22Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/SourceDataset_wzv')]",
				"[concat(variables('workspaceId'), '/datasets/DestinationDataset_wzv')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Notebook1",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Notebook 2",
								"type": "NotebookReference"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "zhaotets"
				},
				"annotations": [],
				"lastPublishTime": "2020-11-21T05:57:54Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/Notebook 2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 2')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Notebook1",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Notebook03",
								"type": "NotebookReference"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2020-11-23T12:28:22Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/Notebook03')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 4')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Spark job definition1",
						"type": "SparkJob",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"sparkJob": {
								"referenceName": "Spark job definition 30",
								"type": "SparkJobDefinitionReference"
							},
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2020-11-23T12:07:50Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sparkJobDefinitions/Spark job definition 30')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 71')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Dataflow30",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Dataflow30",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2020-11-23T12:48:54Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/Dataflow30')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/notebookpython')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "WASB",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Read and write data from Azure Blob Storage WASB",
								"type": "NotebookReference"
							}
						}
					},
					{
						"name": "Gen2",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "WASB",
								"dependencyConditions": [
									"Completed"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Read and write data from Azure Data Lake Storage Gen2",
								"type": "NotebookReference"
							}
						}
					},
					{
						"name": "Using Azure Open Datasets in Synapse",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Gen2",
								"dependencyConditions": [
									"Completed"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Using Azure Open Datasets in Synapse",
								"type": "NotebookReference"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2021-03-08T05:43:29Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/Read and write data from Azure Blob Storage WASB')]",
				"[concat(variables('workspaceId'), '/notebooks/Read and write data from Azure Data Lake Storage Gen2')]",
				"[concat(variables('workspaceId'), '/notebooks/Using Azure Open Datasets in Synapse')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/scala')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Creating a managed Spark Table_acd",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Creating a managed Spark Table_acd",
								"type": "NotebookReference"
							}
						}
					},
					{
						"name": "unmanaged",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Creating a managed Spark Table_acd",
								"dependencyConditions": [
									"Completed"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Creating an unmanaged Spark Table_9ln",
								"type": "NotebookReference"
							}
						}
					},
					{
						"name": "Getting Started with Delta Lake_faw",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "unmanaged",
								"dependencyConditions": [
									"Completed"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Getting Started with Delta Lake_faw",
								"type": "NotebookReference"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2021-03-08T06:12:35Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/Creating a managed Spark Table_acd')]",
				"[concat(variables('workspaceId'), '/notebooks/Creating an unmanaged Spark Table_9ln')]",
				"[concat(variables('workspaceId'), '/notebooks/Getting Started with Delta Lake_faw')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AUTHORParquet1117')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1105",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "part-00036-tid-210938564719836543-aea5b543-5e83-4a7d-8d31-69f72c50b05d-15156-1.c000.snappy.parquet",
						"folderPath": "parquet",
						"fileSystem": "hozhao"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1105')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DestinationDataset_14p')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1108",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileSystem": "hozhao"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1108')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DestinationDataset_n80')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1105",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileSystem": "hozhao"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1105')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DestinationDataset_o4a')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1108",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "CDT1112.parquet",
						"fileSystem": "test"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1108')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DestinationDataset_wzv')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1105",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "CDT_1117.json",
						"fileSystem": "hozhao"
					}
				},
				"schema": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1105')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Excelfile12')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureBlobStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Excel",
				"typeProperties": {
					"sheetName": "file name",
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "data",
						"folderPath": "data4.1",
						"container": "data4"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureBlobStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/InputParquet1109')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1105",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "part-00036-tid-210938564719836543-aea5b543-5e83-4a7d-8d31-69f72c50b05d-15156-1.c000.snappy.parquet",
						"folderPath": "parquet",
						"fileSystem": "hozhao"
					},
					"compressionCodec": "snappy"
				},
				"schema": [
					{
						"name": "vendorID",
						"type": "UTF8"
					},
					{
						"name": "tpepPickupDateTime",
						"type": "INT96"
					},
					{
						"name": "tpepDropoffDateTime",
						"type": "INT96"
					},
					{
						"name": "passengerCount",
						"type": "INT32"
					},
					{
						"name": "tripDistance",
						"type": "DOUBLE"
					},
					{
						"name": "puLocationId",
						"type": "UTF8"
					},
					{
						"name": "doLocationId",
						"type": "UTF8"
					},
					{
						"name": "startLon",
						"type": "DOUBLE"
					},
					{
						"name": "startLat",
						"type": "DOUBLE"
					},
					{
						"name": "endLon",
						"type": "DOUBLE"
					},
					{
						"name": "endLat",
						"type": "DOUBLE"
					},
					{
						"name": "rateCodeId",
						"type": "INT32"
					},
					{
						"name": "storeAndFwdFlag",
						"type": "UTF8"
					},
					{
						"name": "paymentType",
						"type": "UTF8"
					},
					{
						"name": "fareAmount",
						"type": "DOUBLE"
					},
					{
						"name": "extra",
						"type": "DOUBLE"
					},
					{
						"name": "mtaTax",
						"type": "DOUBLE"
					},
					{
						"name": "improvementSurcharge",
						"type": "UTF8"
					},
					{
						"name": "tipAmount",
						"type": "DOUBLE"
					},
					{
						"name": "tollsAmount",
						"type": "DOUBLE"
					},
					{
						"name": "totalAmount",
						"type": "DOUBLE"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1105')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/InputParquet1112')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1111",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "GB_Test.parquet",
						"fileSystem": "hozhao"
					},
					"compressionCodec": "snappy"
				},
				"schema": [
					{
						"name": "Prop_0",
						"type": "UTF8"
					},
					{
						"name": "Prop_1",
						"type": "UTF8"
					},
					{
						"name": "Prop_2",
						"type": "UTF8"
					},
					{
						"name": "Prop_3",
						"type": "UTF8"
					},
					{
						"name": "Prop_4",
						"type": "UTF8"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1111')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OutParquet1112')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1112",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileSystem": "hozhao"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1112')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OutputParquet')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1105",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "parquet",
						"fileSystem": "hozhao"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1105')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Parquet1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "bigdataqa0512ws2-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/bigdataqa0512ws2-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Parquet2')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "cicdtestws-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "parquet/output1",
						"fileSystem": "hozhao"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/cicdtestws-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SourceDataset_14p')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1108",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "fact_sale.parquet",
						"folderPath": "parquet",
						"fileSystem": "hozhao"
					},
					"compressionCodec": "snappy"
				},
				"schema": [
					{
						"name": "SaleKey",
						"type": "INT32"
					},
					{
						"name": "CityKey",
						"type": "INT32"
					},
					{
						"name": "CustomerKey",
						"type": "INT32"
					},
					{
						"name": "BillToCustomerKey",
						"type": "INT32"
					},
					{
						"name": "StockItemKey",
						"type": "INT32"
					},
					{
						"name": "InvoiceDateKey",
						"type": "INT96"
					},
					{
						"name": "DeliveryDateKey",
						"type": "INT96"
					},
					{
						"name": "SalespersonKey",
						"type": "INT32"
					},
					{
						"name": "WWIInvoicID",
						"type": "INT32"
					},
					{
						"name": "Description",
						"type": "UTF8"
					},
					{
						"name": "Package",
						"type": "UTF8"
					},
					{
						"name": "Quantity",
						"type": "INT32"
					},
					{
						"name": "UnitPrice",
						"type": "DOUBLE"
					},
					{
						"name": "TaxRate",
						"type": "DOUBLE"
					},
					{
						"name": "TotalExcludingTax",
						"type": "DOUBLE"
					},
					{
						"name": "TaxAmount",
						"type": "DOUBLE"
					},
					{
						"name": "Profit",
						"type": "DOUBLE"
					},
					{
						"name": "TotalIncludingTax",
						"type": "DOUBLE"
					},
					{
						"name": "TotalDryItems",
						"type": "INT32"
					},
					{
						"name": "TotalChillerItems",
						"type": "INT32"
					},
					{
						"name": "LineageKey",
						"type": "INT32"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1108')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SourceDataset_o4a')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1108",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "fact_sale.parquet",
						"folderPath": "parquet",
						"fileSystem": "hozhao"
					},
					"compressionCodec": "snappy"
				},
				"schema": [
					{
						"name": "SaleKey",
						"type": "INT32"
					},
					{
						"name": "CityKey",
						"type": "INT32"
					},
					{
						"name": "CustomerKey",
						"type": "INT32"
					},
					{
						"name": "BillToCustomerKey",
						"type": "INT32"
					},
					{
						"name": "StockItemKey",
						"type": "INT32"
					},
					{
						"name": "InvoiceDateKey",
						"type": "INT96"
					},
					{
						"name": "DeliveryDateKey",
						"type": "INT96"
					},
					{
						"name": "SalespersonKey",
						"type": "INT32"
					},
					{
						"name": "WWIInvoicID",
						"type": "INT32"
					},
					{
						"name": "Description",
						"type": "UTF8"
					},
					{
						"name": "Package",
						"type": "UTF8"
					},
					{
						"name": "Quantity",
						"type": "INT32"
					},
					{
						"name": "UnitPrice",
						"type": "DOUBLE"
					},
					{
						"name": "TaxRate",
						"type": "DOUBLE"
					},
					{
						"name": "TotalExcludingTax",
						"type": "DOUBLE"
					},
					{
						"name": "TaxAmount",
						"type": "DOUBLE"
					},
					{
						"name": "Profit",
						"type": "DOUBLE"
					},
					{
						"name": "TotalIncludingTax",
						"type": "DOUBLE"
					},
					{
						"name": "TotalDryItems",
						"type": "INT32"
					},
					{
						"name": "TotalChillerItems",
						"type": "INT32"
					},
					{
						"name": "LineageKey",
						"type": "INT32"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1108')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SourceDataset_wzv')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1105",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "fact_sale.parquet",
						"folderPath": "parquet",
						"fileSystem": "hozhao"
					},
					"compressionCodec": "snappy"
				},
				"schema": [
					{
						"name": "SaleKey",
						"type": "INT32"
					},
					{
						"name": "CityKey",
						"type": "INT32"
					},
					{
						"name": "CustomerKey",
						"type": "INT32"
					},
					{
						"name": "BillToCustomerKey",
						"type": "INT32"
					},
					{
						"name": "StockItemKey",
						"type": "INT32"
					},
					{
						"name": "InvoiceDateKey",
						"type": "INT96"
					},
					{
						"name": "DeliveryDateKey",
						"type": "INT96"
					},
					{
						"name": "SalespersonKey",
						"type": "INT32"
					},
					{
						"name": "WWIInvoicID",
						"type": "INT32"
					},
					{
						"name": "Description",
						"type": "UTF8"
					},
					{
						"name": "Package",
						"type": "UTF8"
					},
					{
						"name": "Quantity",
						"type": "INT32"
					},
					{
						"name": "UnitPrice",
						"type": "DOUBLE"
					},
					{
						"name": "TaxRate",
						"type": "DOUBLE"
					},
					{
						"name": "TotalExcludingTax",
						"type": "DOUBLE"
					},
					{
						"name": "TaxAmount",
						"type": "DOUBLE"
					},
					{
						"name": "Profit",
						"type": "DOUBLE"
					},
					{
						"name": "TotalIncludingTax",
						"type": "DOUBLE"
					},
					{
						"name": "TotalDryItems",
						"type": "INT32"
					},
					{
						"name": "TotalChillerItems",
						"type": "INT32"
					},
					{
						"name": "LineageKey",
						"type": "INT32"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1105')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/authorParquet1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1111",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "fact_sale.parquet",
						"folderPath": "parquet",
						"fileSystem": "hozhao"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1111')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/authorParquet2')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1112",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "GB_Test.parquet",
						"fileSystem": "hozhao"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1112')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/authorParquet2_copy1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1112",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "A"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "GB_Test.parquet",
						"fileSystem": "hozhao"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1112')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/contributorParquet1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1111",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "fact_sale.parquet",
						"folderPath": "parquet",
						"fileSystem": "hozhao"
					},
					"compressionCodec": "snappy"
				},
				"schema": [
					{
						"name": "SaleKey",
						"type": "INT32"
					},
					{
						"name": "CityKey",
						"type": "INT32"
					},
					{
						"name": "CustomerKey",
						"type": "INT32"
					},
					{
						"name": "BillToCustomerKey",
						"type": "INT32"
					},
					{
						"name": "StockItemKey",
						"type": "INT32"
					},
					{
						"name": "InvoiceDateKey",
						"type": "INT96"
					},
					{
						"name": "DeliveryDateKey",
						"type": "INT96"
					},
					{
						"name": "SalespersonKey",
						"type": "INT32"
					},
					{
						"name": "WWIInvoicID",
						"type": "INT32"
					},
					{
						"name": "Description",
						"type": "UTF8"
					},
					{
						"name": "Package",
						"type": "UTF8"
					},
					{
						"name": "Quantity",
						"type": "INT32"
					},
					{
						"name": "UnitPrice",
						"type": "DOUBLE"
					},
					{
						"name": "TaxRate",
						"type": "DOUBLE"
					},
					{
						"name": "TotalExcludingTax",
						"type": "DOUBLE"
					},
					{
						"name": "TaxAmount",
						"type": "DOUBLE"
					},
					{
						"name": "Profit",
						"type": "DOUBLE"
					},
					{
						"name": "TotalIncludingTax",
						"type": "DOUBLE"
					},
					{
						"name": "TotalDryItems",
						"type": "INT32"
					},
					{
						"name": "TotalChillerItems",
						"type": "INT32"
					},
					{
						"name": "LineageKey",
						"type": "INT32"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1111')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('AzureBlobStorage1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage1106')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('AzureBlobStorage1106_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage1124')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('AzureBlobStorage1124_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('AzureBlobStorage2_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage3')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVault_bdbjqingtest",
							"type": "LinkedServiceReference"
						},
						"secretName": "[parameters('AzureBlobStorage3_properties_typeProperties_connectionString_secretName')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault_bdbjqingtest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage4')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('AzureBlobStorage4_connectionString')]",
					"accountKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVault_bdbjqingtest",
							"type": "LinkedServiceReference"
						},
						"secretName": "accountkeyblobkey"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault_bdbjqingtest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage_0305AKYTEST')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVault_bdbjqingtest",
							"type": "LinkedServiceReference"
						},
						"secretName": "[parameters('AzureBlobStorage_0305AKYTEST_properties_typeProperties_connectionString_secretName')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault_bdbjqingtest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage_TestAKV')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('AzureBlobStorage_TestAKV_connectionString')]",
					"accountKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVault_bdbjqingtest",
							"type": "LinkedServiceReference"
						},
						"secretName": "accountkeyblobkey"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault_bdbjqingtest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage_ci_test')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVault_bdbjqingtest",
							"type": "LinkedServiceReference"
						},
						"secretName": "[parameters('AzureBlobStorage_ci_test_properties_typeProperties_connectionString_secretName')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault_bdbjqingtest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage_prod_test')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVault_bdbjqingtest",
							"type": "LinkedServiceReference"
						},
						"secretName": "[parameters('AzureBlobStorage_prod_test_properties_typeProperties_connectionString_secretName')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault_bdbjqingtest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataExplorer1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataExplorer",
				"typeProperties": {
					"endpoint": "https://bdbjkustoqingtest.eastus.kusto.windows.net",
					"tenant": "[parameters('AzureDataExplorer1_properties_typeProperties_tenant')]",
					"servicePrincipalId": "[parameters('AzureDataExplorer1_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVault_bdbjqingtest",
							"type": "LinkedServiceReference"
						},
						"secretName": "bdbjkeyvaultsecretqingtest"
					},
					"database": "[parameters('AzureDataExplorer1_properties_typeProperties_database')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault_bdbjqingtest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataExplorer2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataExplorer",
				"typeProperties": {
					"endpoint": "https://bdbjkustoqingtest.eastus.kusto.windows.net",
					"tenant": "[parameters('AzureDataExplorer2_properties_typeProperties_tenant')]",
					"servicePrincipalId": "[parameters('AzureDataExplorer2_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataExplorer2_servicePrincipalKey')]"
					},
					"database": "[parameters('AzureDataExplorer2_properties_typeProperties_database')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage0304')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage0304_properties_typeProperties_url')]",
					"tenant": "[parameters('AzureDataLakeStorage0304_properties_typeProperties_tenant')]",
					"servicePrincipalId": "[parameters('AzureDataLakeStorage0304_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalCredentialType": "ServicePrincipalKey",
					"servicePrincipalCredential": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage0304_servicePrincipalCredential')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage030401')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage030401_properties_typeProperties_url')]",
					"tenant": "[parameters('AzureDataLakeStorage030401_properties_typeProperties_tenant')]",
					"servicePrincipalId": "[parameters('AzureDataLakeStorage030401_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalCredentialType": "ServicePrincipalKey",
					"servicePrincipalCredential": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVault_bdbjqingtest",
							"type": "LinkedServiceReference"
						},
						"secretName": "bdbjkeyvaultsecretqingtest"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault_bdbjqingtest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage030402')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage030402_properties_typeProperties_url')]",
					"tenant": "[parameters('AzureDataLakeStorage030402_properties_typeProperties_tenant')]",
					"servicePrincipalId": "[parameters('AzureDataLakeStorage030402_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalCredentialType": "ServicePrincipalKey",
					"servicePrincipalCredential": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage030402_servicePrincipalCredential')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage030403')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage030403_properties_typeProperties_url')]",
					"tenant": "[parameters('AzureDataLakeStorage030403_properties_typeProperties_tenant')]",
					"servicePrincipalId": "[parameters('AzureDataLakeStorage030403_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalCredentialType": "ServicePrincipalKey",
					"servicePrincipalCredential": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVault_bdbjqingtest",
							"type": "LinkedServiceReference"
						},
						"secretName": "bdbjkeyvaultsecretqingtest"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault_bdbjqingtest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage030404')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage030404_properties_typeProperties_url')]",
					"servicePrincipalCredential": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage030404_servicePrincipalCredential')]"
					},
					"credential": {
						"referenceName": "Credential_bdbjqingtest",
						"type": "CredentialReference"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/credentials/Credential_bdbjqingtest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage0305')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage0305_properties_typeProperties_url')]",
					"accountKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVault_bdbjqingtest",
							"type": "LinkedServiceReference"
						},
						"secretName": "bdbjkeyvaultsecretqingtest",
						"secretVersion": ""
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault_bdbjqingtest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1105')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1105_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1105_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1107')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1107_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1107_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1108')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1108_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1108_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1110')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1110_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1110_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1111')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1111_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1111_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1112')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1112_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1112_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1113')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1113_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1113_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1114')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1114_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1114_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1115')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1115_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1115_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage2_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage2_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage3')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage3_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage3_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage30405')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage30405_properties_typeProperties_url')]",
					"accountKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVault_bdbjqingtest",
							"type": "LinkedServiceReference"
						},
						"secretName": "accountkey"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault_bdbjqingtest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage4')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage4_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage4_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStore1105')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataLakeStore",
				"typeProperties": {
					"dataLakeStoreUri": "[parameters('AzureDataLakeStore1105_properties_typeProperties_dataLakeStoreUri')]",
					"tenant": "[parameters('AzureDataLakeStore1105_properties_typeProperties_tenant')]",
					"subscriptionId": "[parameters('AzureDataLakeStore1105_properties_typeProperties_subscriptionId')]",
					"resourceGroupName": "[parameters('AzureDataLakeStore1105_properties_typeProperties_resourceGroupName')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStore1106')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataLakeStore",
				"typeProperties": {
					"dataLakeStoreUri": "[parameters('AzureDataLakeStore1106_properties_typeProperties_dataLakeStoreUri')]",
					"tenant": "[parameters('AzureDataLakeStore1106_properties_typeProperties_tenant')]",
					"subscriptionId": "[parameters('AzureDataLakeStore1106_properties_typeProperties_subscriptionId')]",
					"resourceGroupName": "[parameters('AzureDataLakeStore1106_properties_typeProperties_resourceGroupName')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStore1107')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataLakeStore",
				"typeProperties": {
					"dataLakeStoreUri": "[parameters('AzureDataLakeStore1107_properties_typeProperties_dataLakeStoreUri')]",
					"tenant": "[parameters('AzureDataLakeStore1107_properties_typeProperties_tenant')]",
					"subscriptionId": "[parameters('AzureDataLakeStore1107_properties_typeProperties_subscriptionId')]",
					"resourceGroupName": "[parameters('AzureDataLakeStore1107_properties_typeProperties_resourceGroupName')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStore1108')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataLakeStore",
				"typeProperties": {
					"dataLakeStoreUri": "[parameters('AzureDataLakeStore1108_properties_typeProperties_dataLakeStoreUri')]",
					"tenant": "[parameters('AzureDataLakeStore1108_properties_typeProperties_tenant')]",
					"subscriptionId": "[parameters('AzureDataLakeStore1108_properties_typeProperties_subscriptionId')]",
					"resourceGroupName": "[parameters('AzureDataLakeStore1108_properties_typeProperties_resourceGroupName')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStore1122')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataLakeStore",
				"typeProperties": {
					"dataLakeStoreUri": "[parameters('AzureDataLakeStore1122_properties_typeProperties_dataLakeStoreUri')]",
					"tenant": "[parameters('AzureDataLakeStore1122_properties_typeProperties_tenant')]",
					"subscriptionId": "[parameters('AzureDataLakeStore1122_properties_typeProperties_subscriptionId')]",
					"resourceGroupName": "[parameters('AzureDataLakeStore1122_properties_typeProperties_resourceGroupName')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStore123')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataLakeStore",
				"typeProperties": {
					"dataLakeStoreUri": "[parameters('AzureDataLakeStore123_properties_typeProperties_dataLakeStoreUri')]",
					"tenant": "[parameters('AzureDataLakeStore123_properties_typeProperties_tenant')]",
					"subscriptionId": "[parameters('AzureDataLakeStore123_properties_typeProperties_subscriptionId')]",
					"resourceGroupName": "[parameters('AzureDataLakeStore123_properties_typeProperties_resourceGroupName')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStore456')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataLakeStore",
				"typeProperties": {
					"dataLakeStoreUri": "[parameters('AzureDataLakeStore456_properties_typeProperties_dataLakeStoreUri')]",
					"tenant": "[parameters('AzureDataLakeStore456_properties_typeProperties_tenant')]",
					"subscriptionId": "[parameters('AzureDataLakeStore456_properties_typeProperties_subscriptionId')]",
					"resourceGroupName": "[parameters('AzureDataLakeStore456_properties_typeProperties_resourceGroupName')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureKeyVault1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('AzureKeyVault1_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureKeyVault_bdbjqingtest')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('AzureKeyVault_bdbjqingtest_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CosmosDb1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "CosmosDb",
				"typeProperties": {
					"connectionString": "[parameters('CosmosDb1_connectionString')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CosmosDbMongoDbApi')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "CosmosDbMongoDbApi",
				"typeProperties": {
					"connectionString": "[parameters('CosmosDbMongoDbApi_connectionString')]",
					"database": "[parameters('CosmosDbMongoDbApi_properties_typeProperties_database')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CosmosDbMongoDbApi0304')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "CosmosDbMongoDbApi",
				"typeProperties": {
					"connectionString": "[parameters('CosmosDbMongoDbApi0304_connectionString')]",
					"database": "[parameters('CosmosDbMongoDbApi0304_properties_typeProperties_database')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CosmosDbMongoDbApi1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "CosmosDbMongoDbApi",
				"typeProperties": {
					"connectionString": "[parameters('CosmosDbMongoDbApi1_connectionString')]",
					"database": "[parameters('CosmosDbMongoDbApi1_properties_typeProperties_database')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CosmosDbSQLAPI030401')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "CosmosDb",
				"typeProperties": {
					"connectionString": "[parameters('CosmosDbSQLAPI030401_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CosmosDbsqlapi0304')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "CosmosDb",
				"typeProperties": {
					"connectionString": "[parameters('CosmosDbsqlapi0304_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Gen20304')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('Gen20304_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/GitHublink')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "GitHub",
				"typeProperties": {
					"username": "[parameters('GitHublink_properties_typeProperties_username')]",
					"credential": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVault1",
							"type": "LinkedServiceReference"
						},
						"secretName": "scet"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Kusto0304')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataExplorer",
				"typeProperties": {
					"endpoint": "https://bdbjkustoqingtest.eastus.kusto.windows.net",
					"tenant": "[parameters('Kusto0304_properties_typeProperties_tenant')]",
					"servicePrincipalId": "[parameters('Kusto0304_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalKey": {
						"type": "SecureString",
						"value": "[parameters('Kusto0304_servicePrincipalKey')]"
					},
					"database": "[parameters('Kusto0304_properties_typeProperties_database')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Kusto030401')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataExplorer",
				"typeProperties": {
					"endpoint": "https://bdbjkustoqingtest.eastus.kusto.windows.net",
					"tenant": "[parameters('Kusto030401_properties_typeProperties_tenant')]",
					"servicePrincipalId": "[parameters('Kusto030401_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVault_bdbjqingtest",
							"type": "LinkedServiceReference"
						},
						"secretName": "bdbjkeyvaultsecretqingtest"
					},
					"database": "[parameters('Kusto030401_properties_typeProperties_database')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault_bdbjqingtest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Kusto030402')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataExplorer",
				"typeProperties": {
					"endpoint": "https://bdbjkustoqingtest.eastus.kusto.windows.net",
					"tenant": "[parameters('Kusto030402_properties_typeProperties_tenant')]",
					"servicePrincipalId": "[parameters('Kusto030402_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalKey": {
						"type": "SecureString",
						"value": "[parameters('Kusto030402_servicePrincipalKey')]"
					},
					"database": "[parameters('Kusto030402_properties_typeProperties_database')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Kusto030403')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataExplorer",
				"typeProperties": {
					"endpoint": "https://bdbjkustoqingtest.eastus.kusto.windows.net",
					"tenant": "[parameters('Kusto030403_properties_typeProperties_tenant')]",
					"servicePrincipalId": "[parameters('Kusto030403_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalKey": {
						"type": "SecureString",
						"value": "[parameters('Kusto030403_servicePrincipalKey')]"
					},
					"database": "[parameters('Kusto030403_properties_typeProperties_database')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Kusto1105')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataExplorer",
				"typeProperties": {
					"endpoint": "https://kustodemo.westus2.kusto.windows.net",
					"tenant": "[parameters('Kusto1105_properties_typeProperties_tenant')]",
					"servicePrincipalId": "[parameters('Kusto1105_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalKey": {
						"type": "SecureString",
						"value": "[parameters('Kusto1105_servicePrincipalKey')]"
					},
					"database": "[parameters('Kusto1105_properties_typeProperties_database')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MongoDbApi1105')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "CosmosDbMongoDbApi",
				"typeProperties": {
					"connectionString": "[parameters('MongoDbApi1105_connectionString')]",
					"database": "[parameters('MongoDbApi1105_properties_typeProperties_database')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PowerBIWorkspace1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "PowerBIWorkspace",
				"typeProperties": {
					"workspaceID": "28a0e6b5-f83b-47c9-aa9f-eaab6b8a2bf9",
					"tenantID": "72f988bf-86f1-41af-91ab-2d7cd011db47"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQLApi1105')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "CosmosDb",
				"typeProperties": {
					"connectionString": "[parameters('SQLApi1105_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bdbj0105ws-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('bdbj0105ws-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bdbj0105ws-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('bdbj0105ws-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bdbj0112ws-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('bdbj0112ws-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bdbj0112ws-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('bdbj0112ws-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bdbj0119ws-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('bdbj0119ws-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bdbj0119ws-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('bdbj0119ws-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bdbj0202ws-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('bdbj0202ws-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bdbj0202ws-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('bdbj0202ws-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bdbj0302ws-git-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('bdbj0302ws-git-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bdbj0302ws-git-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('bdbj0302ws-git-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bdbj1027ws-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('bdbj1027ws-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bdbj1027ws-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('bdbj1027ws-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bdbj1103ws-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('bdbj1103ws-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bdbj1103ws-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('bdbj1103ws-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bdbj1113ws-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('bdbj1113ws-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bdbj1113ws-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('bdbj1113ws-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bdbj1119ws-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('bdbj1119ws-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bdbj1119ws-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('bdbj1119ws-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bdbj1208ws-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('bdbj1208ws-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bdbj1208ws-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('bdbj1208ws-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bigdata0925ws-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('bigdata0925ws-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bigdata0925ws-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('bigdata0925ws-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bigdata1128ws-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('bigdata1128ws-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bigdata1128ws-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('bigdata1128ws-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bigdata1128ws2-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('bigdata1128ws2-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bigdata1128ws2-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('bigdata1128ws2-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bigdataqa0301ws-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('bigdataqa0301ws-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bigdataqa0301ws-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('bigdataqa0301ws-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bigdataqa0512ws2-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('bigdataqa0512ws2-WorkspaceDefaultSqlServer_connectionString')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bigdataqa0512ws2-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('bigdataqa0512ws2-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bigdataqa0917ws-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('bigdataqa0917ws-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bigdataqa0917ws-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('bigdataqa0917ws-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bigdataqa0924ws-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('bigdataqa0924ws-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bigdataqa0924ws-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('bigdataqa0924ws-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/catestws2-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('catestws2-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/catestws2-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('catestws2-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/chaxuamleus')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureMLService",
				"typeProperties": {
					"subscriptionId": "[parameters('chaxuamleus_properties_typeProperties_subscriptionId')]",
					"resourceGroupName": "[parameters('chaxuamleus_properties_typeProperties_resourceGroupName')]",
					"mlWorkspaceName": "chaxuamleus",
					"servicePrincipalId": "[parameters('chaxuamleus_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalKey": {
						"type": "SecureString",
						"value": "[parameters('chaxuamleus_servicePrincipalKey')]"
					},
					"tenant": "[parameters('chaxuamleus_properties_typeProperties_tenant')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/cicdtestws-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('cicdtestws-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/cicdtestws-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('cicdtestws-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dancicdtest-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('dancicdtest-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dancicdtest-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('dancicdtest-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dansynapsebugbash-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('dansynapsebugbash-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/hozhaobdbj')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('hozhaobdbj_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('hozhaobdbj_accountKey')]"
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ltianscusworkspace-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('ltianscusworkspace-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ltianscusworkspace-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('ltianscusworkspace-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nyc_tlc_fhv')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"sasUri": "[parameters('nyc_tlc_fhv_sasUri')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/us-employment-hours-earnings-state')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"sasUri": "[parameters('us-employment-hours-earnings-state_sasUri')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/workspacedeploydan5-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('workspacedeploydan5-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/xiaochuan')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('xiaochuan_connectionString')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/yifso1022scus-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('yifso1022scus-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/yifso1022scus-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('yifso1022scus-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger 1')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "5455ddd",
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "CopyPipeline_o4a",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Minute",
						"interval": 15,
						"startTime": "2020-11-12T07:41:00Z",
						"timeZone": "UTC"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/CopyPipeline_o4a')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger 2')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Minute",
						"interval": 5,
						"startTime": "2020-11-12T08:24:00Z",
						"timeZone": "UTC"
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger 3')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Minute",
						"interval": 15,
						"startTime": "2020-11-21T02:43:00Z",
						"timeZone": "UTC"
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger 4')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Minute",
						"interval": 15,
						"startTime": "2020-11-21T02:46:00Z",
						"timeZone": "UTC"
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger 5')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Minute",
						"interval": 15,
						"startTime": "2020-11-21T02:56:00Z",
						"timeZone": "UTC"
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger 6')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Minute",
						"interval": 15,
						"startTime": "2020-11-21T06:25:00Z",
						"timeZone": "UTC"
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IntegrationRuntime1')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IntegrationRuntime11')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0,
							"cleanup": false
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IntegrationRuntime2')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IntegrationRuntime5')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IntegrationRuntime6')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataflow1')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "InputParquet1112",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutParquet1112",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "Sort1"
						}
					],
					"script": "source(output(\n\t\tProp_0 as string,\n\t\tProp_1 as string,\n\t\tProp_2 as string,\n\t\tProp_3 as string,\n\t\tProp_4 as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false,\n\tformat: 'parquet') ~> source1\nsource1 sort(asc(Prop_0, true)) ~> Sort1\nSort1 sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'parquet',\n\tumask: 0022,\n\tpreCommands: [],\n\tpostCommands: [],\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> sink1"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/InputParquet1112')]",
				"[concat(variables('workspaceId'), '/datasets/OutParquet1112')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataflow1_copy5')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "D/YY"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "InputParquet1112",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutParquet1112",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "Sort1"
						}
					],
					"script": "source(output(\n\t\tProp_0 as string,\n\t\tProp_1 as string,\n\t\tProp_2 as string,\n\t\tProp_3 as string,\n\t\tProp_4 as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false,\n\tformat: 'parquet') ~> source1\nsource1 sort(asc(Prop_0, true)) ~> Sort1\nSort1 sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'parquet',\n\tumask: 0022,\n\tpreCommands: [],\n\tpostCommands: [],\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> sink1"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/InputParquet1112')]",
				"[concat(variables('workspaceId'), '/datasets/OutParquet1112')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataflow2')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "D"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "contributorParquet1",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputParquet",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "Sort1"
						}
					],
					"script": "source(output(\n\t\tSaleKey as integer,\n\t\tCityKey as integer,\n\t\tCustomerKey as integer,\n\t\tBillToCustomerKey as integer,\n\t\tStockItemKey as integer,\n\t\tInvoiceDateKey as timestamp,\n\t\tDeliveryDateKey as timestamp,\n\t\tSalespersonKey as integer,\n\t\tWWIInvoicID as integer,\n\t\tDescription as string,\n\t\tPackage as string,\n\t\tQuantity as integer,\n\t\tUnitPrice as double,\n\t\tTaxRate as double,\n\t\tTotalExcludingTax as double,\n\t\tTaxAmount as double,\n\t\tProfit as double,\n\t\tTotalIncludingTax as double,\n\t\tTotalDryItems as integer,\n\t\tTotalChillerItems as integer,\n\t\tLineageKey as integer\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false,\n\tformat: 'parquet') ~> source1\nsource1 sort(asc(InvoiceDateKey, true)) ~> Sort1\nSort1 sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'parquet',\n\tumask: 0022,\n\tpreCommands: [],\n\tpostCommands: [],\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> sink1"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/contributorParquet1')]",
				"[concat(variables('workspaceId'), '/datasets/OutputParquet')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataflow3')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "InputParquet1109",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputParquet",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "Sort1"
						}
					],
					"script": "source(output(\n\t\tvendorID as string,\n\t\ttpepPickupDateTime as timestamp,\n\t\ttpepDropoffDateTime as timestamp,\n\t\tpassengerCount as integer,\n\t\ttripDistance as double,\n\t\tpuLocationId as string,\n\t\tdoLocationId as string,\n\t\tstartLon as double,\n\t\tstartLat as double,\n\t\tendLon as double,\n\t\tendLat as double,\n\t\trateCodeId as integer,\n\t\tstoreAndFwdFlag as string,\n\t\tpaymentType as string,\n\t\tfareAmount as double,\n\t\textra as double,\n\t\tmtaTax as double,\n\t\timprovementSurcharge as string,\n\t\ttipAmount as double,\n\t\ttollsAmount as double,\n\t\ttotalAmount as double\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false,\n\tformat: 'parquet') ~> source1\nsource1 sort(desc(tpepPickupDateTime, true)) ~> Sort1\nSort1 sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'parquet',\n\tumask: 0022,\n\tpreCommands: [],\n\tpostCommands: [],\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> sink1"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/InputParquet1109')]",
				"[concat(variables('workspaceId'), '/datasets/OutputParquet')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataflow30')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "dancicdtest-WorkspaceDefaultStorage",
								"type": "LinkedServiceReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "dancicdtest-WorkspaceDefaultStorage",
								"type": "LinkedServiceReference"
							},
							"schemaLinkedService": {
								"referenceName": "dancicdtest-WorkspaceDefaultStorage",
								"type": "LinkedServiceReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [],
					"script": "source(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false,\n\tformat: 'xml',\n\tfileSystem: 'files',\n\tvalidationMode: 'none',\n\tnamespaces: true) ~> source1\nsource1 sink(allowSchemaDrift: false,\n\tvalidateSchema: false,\n\tentity: '/account',\n\tformat: 'cdm',\n\tmanifestType: 'manifest',\n\tfolderPath: '',\n\tfileSystem: 'files',\n\tcolumnNamesAsHeader: false,\n\tcolumnDelimiter: ',',\n\tquoteChar: '\\\"',\n\tescapeChar: '\\\\',\n\tcorpusPath: 'dan',\n\tcorpusStore: 'adlsgen2',\n\tadlsgen2_fileSystem: 'files',\n\ttruncate: false,\n\tumask: 0022,\n\tpreCommands: [],\n\tpostCommands: [],\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> sink1"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/dancicdtest-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Credential1')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ServicePrincipal",
				"typeProperties": {
					"tenant": "72f988bf-86f1-41af-91ab-2d7cd011db47",
					"servicePrincipalId": "123",
					"servicePrincipalKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVault1",
							"type": "LinkedServiceReference"
						},
						"secretName": "123"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Credential3')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ServicePrincipal",
				"typeProperties": {
					"tenant": "72f988bf-86f1-41af-91ab-2d7cd011db47",
					"servicePrincipalId": "df",
					"servicePrincipalKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVault1",
							"type": "LinkedServiceReference"
						},
						"secretName": "arg"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Credential4')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "adef",
				"type": "ServicePrincipal",
				"typeProperties": {
					"tenant": "72f988bf-86f1-41af-91ab-2d7cd011db47",
					"servicePrincipalId": "ae",
					"servicePrincipalKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVault1",
							"type": "LinkedServiceReference"
						},
						"secretName": "aewf"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Credential_bdbjqingtest')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ServicePrincipal",
				"typeProperties": {
					"tenant": "72bfe100-a448-4f25-af39-5b36c7bb62ef",
					"servicePrincipalId": "8f6caa4c-31d9-4562-8084-bcbf13bb0068",
					"servicePrincipalKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVault_bdbjqingtest",
							"type": "LinkedServiceReference"
						},
						"secretName": "bdbjkeyvaultsecretqingtest"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault_bdbjqingtest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AddSqlAdmin')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- type your sql script here, we now have intellisense\nCREATE USER [v-weiszh@bdbj.onmicrosoft.com] FROM EXTERNAL PROVIDER;\nEXEC sp_addrolemember 'db_owner', 'v-weiszh@bdbj.onmicrosoft.com'\n\nCREATE USER [v-qingqz@bdbj.onmicrosoft.com] FROM EXTERNAL PROVIDER;\nEXEC sp_addrolemember 'db_owner', 'v-qingqz@bdbj.onmicrosoft.com'\n\nCREATE USER [v-mzh@bdbj.onmicrosoft.com] FROM EXTERNAL PROVIDER;\nEXEC sp_addrolemember 'db_owner', 'v-mzh@bdbj.onmicrosoft.com'\n\nCREATE USER [v-yajing@bdbj.onmicrosoft.com] FROM EXTERNAL PROVIDER;\nEXEC sp_addrolemember 'db_owner', 'v-yajing@bdbj.onmicrosoft.com'\n\nCREATE USER [v-shtong@bdbj.onmicrosoft.com] FROM EXTERNAL PROVIDER; \nEXEC sp_addrolemember 'db_owner', 'v-shtong@bdbj.onmicrosoft.com'\n\nCREATE USER [v-hozhao@bdbj.onmicrosoft.com] FROM EXTERNAL PROVIDER; \nEXEC sp_addrolemember 'db_owner', 'v-hozhao@bdbj.onmicrosoft.com'\n\nCREATE USER [v-yayan6@bdbj.onmicrosoft.com] FROM EXTERNAL PROVIDER; \nEXEC sp_addrolemember 'db_owner', 'v-yayan6@bdbj.onmicrosoft.com'\n\nCREATE USER [powerbiTest@bdbj.onmicrosoft.com] FROM EXTERNAL PROVIDER; \nEXEC sp_addrolemember 'db_owner', 'powerbiTest@bdbj.onmicrosoft.com'\n\n    ",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Analyze Azure Open Datasets using serverless SQL pool')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "/*\nFull tutorial available on: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/tutorial-data-analyst\nIn this tutorial, you learn how to perform exploratory data analysis by combining different Azure Open Datasets using serverless SQL pool and then visualizing the results in Azure Synapse Studio.\n\nIn particular, you analyze the New York City (NYC) Taxi dataset that includes:\n\n - Pickup and drop-off dates and times.\n - Pick up and drop-off locations.\n - Trip distances.\n - Itemized fares. \n - Rate types.\n - Payment types.\n - Driver-reported passenger counts.*/\n\n\n/*\n * * * * * * * * * * * * * * * *\n * Automatic schema inference  *\n * * * * * * * * * * * * * * * *\n\nSince data is stored in the Parquet file format, automatic schema inference is available. You can easily query the data without listing the data types of all columns in the files. You also can use the virtual column mechanism and the filepath function to filter out a certain subset of files.\n\nLet's first get familiar with the NYC Taxi data by running the following query. */\n\nSELECT TOP 100 * FROM\n    OPENROWSET(\n        BULK 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/puYear=*/puMonth=*/*.parquet',\n        FORMAT='PARQUET'\n    ) AS [nyc];\n\n\n/* Similarly, you can query the Public Holidays dataset by using the following query. */\n\nSELECT TOP 100 * FROM\n    OPENROWSET(\n        BULK 'https://azureopendatastorage.blob.core.windows.net/holidaydatacontainer/Processed/*.parquet',\n        FORMAT='PARQUET'\n    ) AS [holidays];\n\n/* Lastly, you can also query the Weather Data dataset by using the following query. */\n\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://azureopendatastorage.blob.core.windows.net/isdweatherdatacontainer/ISDWeather/year=*/month=*/*.parquet',\n        FORMAT='PARQUET'\n    ) AS [weather];\n\n/* You can learn more about the meaning of the individual columns in the descriptions\nof the NYC Taxi, Public Holidays, and Weather Data datasets on the Azure Opendatasets page. */\n\n\n/*\n * * * * * * * * * * * * * * * * * * * * * * * * * *\n * Time series, seasonality, and outlier analysis  *\n * * * * * * * * * * * * * * * * * * * * * * * * * *\nYou can easily summarize the yearly number of taxi rides by using the following query. */\n\nSELECT\n    YEAR(tpepPickupDateTime) AS current_year,\n    COUNT(*) AS rides_per_year\nFROM\n    OPENROWSET(\n        BULK 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/puYear=*/puMonth=*/*.parquet',\n        FORMAT='PARQUET'\n    ) AS [nyc]\nWHERE nyc.filepath(1) >= '2009' AND nyc.filepath(1) <= '2019'\nGROUP BY YEAR(tpepPickupDateTime)\nORDER BY 1 ASC;\n\n/* The data can be visualized in Synapse Studio by switching from the Table to the Chart view.\nYou can choose among different chart types, such as Area, Bar, Column, Line, Pie, and Scatter.\nIn this case, plot the Column chart with the Category column set to current_year.\n\nFrom this visualization, a trend of a decreasing number of rides over years can be clearly seen.\nPresumably, this decrease is due to the recent increased popularity of ride-sharing companies.\n*/\n\n/* Next, let's focus the analysis on a single year, for example, 2016.\nThe following query returns the daily number of rides during that year. */\n\nSELECT\n    CAST([tpepPickupDateTime] AS DATE) AS [current_day],\n    COUNT(*) as rides_per_day\nFROM\n    OPENROWSET(\n        BULK 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/puYear=*/puMonth=*/*.parquet',\n        FORMAT='PARQUET'\n    ) AS [nyc]\nWHERE nyc.filepath(1) = '2016'\nGROUP BY CAST([tpepPickupDateTime] AS DATE)\nORDER BY 1 ASC;\n\n/* Again, you can easily visualize data by plotting the Column chart with\nthe Category column set to current_day and the Legend (series) column set to rides_per_day. */\n\n/* From the plot chart, you can see that there's a weekly pattern, with Saturdays as the peak day.\nDuring summer months, there are fewer taxi rides because of vacations.\nThere are also some significant drops in the number of taxi rides without a clear pattern of when and why they occur. */\n\n/* Next, let's see if the drops correlate with public holidays by joining the NYC Taxi rides dataset with the Public Holidays dataset. */\n\nWITH taxi_rides AS\n(\n    SELECT\n        CAST([tpepPickupDateTime] AS DATE) AS [current_day],\n        COUNT(*) as rides_per_day\n    FROM\n        OPENROWSET(\n            BULK 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/puYear=*/puMonth=*/*.parquet',\n            FORMAT='PARQUET'\n        ) AS [nyc]\n    WHERE nyc.filepath(1) = '2016'\n    GROUP BY CAST([tpepPickupDateTime] AS DATE)\n),\npublic_holidays AS\n(\n    SELECT\n        holidayname as holiday,\n        date\n    FROM\n        OPENROWSET(\n            BULK 'https://azureopendatastorage.blob.core.windows.net/holidaydatacontainer/Processed/*.parquet',\n            FORMAT='PARQUET'\n        ) AS [holidays]\n    WHERE countryorregion = 'United States' AND YEAR(date) = 2016\n)\nSELECT\n*\nFROM taxi_rides t\nLEFT OUTER JOIN public_holidays p on t.current_day = p.date\nORDER BY current_day ASC;\n\n/* This time, we want to highlight the number of taxi rides during public holidays.\nFor that purpose, we choose none for the Category column and rides_per_day and holiday as the Legend (series) columns. */\n\n/* From the plot chart, you can see that during public holidays the number of taxi rides is lower.\nThere's still one unexplained large drop on January 23. Let's check the weather in NYC on that day by querying the Weather Data dataset. */\n\nSELECT\n    AVG(windspeed) AS avg_windspeed,\n    MIN(windspeed) AS min_windspeed,\n    MAX(windspeed) AS max_windspeed,\n    AVG(temperature) AS avg_temperature,\n    MIN(temperature) AS min_temperature,\n    MAX(temperature) AS max_temperature,\n    AVG(sealvlpressure) AS avg_sealvlpressure,\n    MIN(sealvlpressure) AS min_sealvlpressure,\n    MAX(sealvlpressure) AS max_sealvlpressure,\n    AVG(precipdepth) AS avg_precipdepth,\n    MIN(precipdepth) AS min_precipdepth,\n    MAX(precipdepth) AS max_precipdepth,\n    AVG(snowdepth) AS avg_snowdepth,\n    MIN(snowdepth) AS min_snowdepth,\n    MAX(snowdepth) AS max_snowdepth\nFROM\n    OPENROWSET(\n        BULK 'https://azureopendatastorage.blob.core.windows.net/isdweatherdatacontainer/ISDWeather/year=*/month=*/*.parquet',\n        FORMAT='PARQUET'\n    ) AS [weather]\nWHERE countryorregion = 'US' AND CAST([datetime] AS DATE) = '2016-01-23' AND stationname = 'JOHN F KENNEDY INTERNATIONAL AIRPORT';\n\n/* The results of the query indicate that the drop in the number of taxi rides occurred because:\n\n1. There was a blizzard on that day in NYC with heavy snow (~30 cm).\n2. It was cold (temperature was below zero degrees Celsius).\n3. It was windy (~10 m/s). */\n\n\n/* This tutorial has shown how a data analyst can quickly perform exploratory data analysis, easily combine different\ndatasets by using serverless SQL pool, and visualize the results by using Azure Synapse Studio. */\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "xssc",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Basic queries to a Synapse Pool')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- Use SELECT to retrieve rows and columns\nSELECT *\nFROM DimEmployee\nORDER BY LastName;\n\n-- Use table aliasing to achieve the same result.\n\nSELECT e.*\nFROM DimEmployee AS e\nORDER BY LastName;\n\n-- Return only the rows for DimEmployee that have an EndDate that is not NULL and a MaritalStatus of 'M' (married).\n\nSELECT FirstName, LastName, StartDate AS FirstDay\nFROM DimEmployee\nWHERE EndDate IS NOT NULL\nAND MaritalStatus = 'M'\nORDER BY LastName;\n\n-- Use DISTINCT to generate a list of all unique titles in the DimEmployee table.\n\nSELECT DISTINCT Title\nFROM DimEmployee\nORDER BY Title;\n\n-- Use GROUP BY with multiple groups and where\nSELECT OrderDateKey, PromotionKey, AVG(SalesAmount) AS AvgSales, SUM(SalesAmount) AS TotalSales\nFROM FactInternetSales\nwhere OrderDateKey > '20020801'\nGROUP BY OrderDateKey, PromotionKey\nORDER BY OrderDateKey;\n\n-- Use the HAVING clause\nSELECT OrderDateKey, SUM(SalesAmount) AS TotalSales\nFROM FactInternetSales\nGROUP BY OrderDateKey\nHAVING OrderDateKey > 20010000\nORDER BY OrderDateKey;\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlpool",
						"poolName": "sqlpool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Basic queries to a Synapse Pool_create table and insert data')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE TABLE DimEmployee\n(\nLastName VARCHAR(16),\nFirstName VARCHAR(16),\nStartDate VARCHAR(16),\nFirstDay VARCHAR(16),\nEndDate VARCHAR(16),\nMaritalStatus VARCHAR(16),\nTitle VARCHAR(16)\n)\n\nINSERT INTO DimEmployee (LastName, FirstName, StartDate, FirstDay, EndDate, MaritalStatus, Title) VALUES ('Cardinal','TomB.Erichsen','2021','0305', '1231','Norway','test');\nINSERT INTO DimEmployee (LastName, FirstName, StartDate, FirstDay, EndDate, MaritalStatus, Title) VALUES ('Cardinal','TomB.Erichsen','2020','0405', '1130','Norway','test01');\nINSERT INTO DimEmployee (LastName, FirstName, StartDate, FirstDay, EndDate, MaritalStatus, Title) VALUES ('Cardinal','TomB.Erichsen','2019','0505', '1031','Norway','test02');\n\n\n\n\nCREATE TABLE FactInternetSales\n(\nOrderDateKey VARCHAR(16),\nPromotionKey VARCHAR(16),\nSalesAmount INT\n)\n\nINSERT INTO FactInternetSales (OrderDateKey, PromotionKey, SalesAmount) VALUES ('2021','TomB.Erichsen','20212021');\nINSERT INTO FactInternetSales (OrderDateKey, PromotionKey, SalesAmount) VALUES ('2020','TomB.Erichsen','200000');\nINSERT INTO FactInternetSales (OrderDateKey, PromotionKey, SalesAmount) VALUES ('2019','TomB.Erichsen','222222');\nINSERT INTO FactInternetSales (OrderDateKey, PromotionKey, SalesAmount) VALUES ('2018','TomB.Erichsen','969969');\nINSERT INTO FactInternetSales (OrderDateKey, PromotionKey, SalesAmount) VALUES ('20212021','TomB.Erichsen','969969');\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlpool",
						"poolName": "sqlpool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CREATE PROC PROC_Read_SearchLogExtParquet')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE PROC [dbo].[PROC_ExtParquet]\nAS\nBEGIN\nSELECT * from dbo.extable2\nEND",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "SQLpool",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CREATE PROC PROC_Read_SearchLogExtParquet_DBZ')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE PROC [dbo].[PROC_Read_ExtParquet_DBZ] AS\nBEGIN\nSELECT 100/(passengerCount-1) from dbo.extable2\nEND",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "SQLpool",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CREATE PROC PROC_Read_SearchLog_DBZ')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE PROC [dbo].[PROC_Read_SearchLog_DBZ]\nAS BEGIN\nSELECT 100/(latency-74) from dbo.SearchLog\nEND",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "SQLpool",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CREATE PROC PROC_SELECT_1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- type your sql script here, we now have intellisense\nCREATE PROC [dbo].[PROC_SELECT_1]\nAS BEGIN\nSELECT 1\nEND",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "SQLpool",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Column Level Security for dedicated SQL pools')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "--Create Membership table with SSN column used to store social security numbers\nCREATE TABLE Membership\n  (MemberID int IDENTITY,\n   FirstName varchar(100) NULL,\n   SSN char(9) NOT NULL,\n   LastName varchar(100) NOT NULL,\n   Phone varchar(12) NULL,\n   Email varchar(100) NULL)\n\n\nCREATE USER Manager WITHOUT LOGIN;\n--Allow TestUser to access all columns except for the SSN column, which has the sensitive data\nGRANT SELECT ON Membership(MemberID, FirstName, LastName, Phone, Email) TO Manager;\n\n--Queries executed as TestUser will fail if they include the SSN column\nSELECT * FROM Membership;\n\n-- Msg 230, Level 14, State 1, Line 12\n-- The SELECT permission was denied on the column 'SSN' of the object 'Membership', database 'CLS_TestDW', schema 'dbo'.\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlpool",
						"poolName": "sqlpool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create external table from parquet fle')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \nCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \nWITH ( FORMAT_TYPE = PARQUET)\nGO\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'qingtest_zhaotestgen2_dfs_core_windows_net') \nCREATE EXTERNAL DATA SOURCE [qingtest_zhaotestgen2_dfs_core_windows_net] \nWITH \n(    LOCATION   = 'abfss://qingtest@zhaotestgen2.dfs.core.windows.net', \n      TYPE       = HADOOP \n)\nGO\nCREATE EXTERNAL TABLE extable2 \n(\n[vendorID] varchar(max),\n[tpepPickupDateTime] datetime2,\n[tpepDropoffDateTime] datetime2,\n[passengerCount] int,\n[tripDistance] float,\n[puLocationId] varchar(max),\n[doLocationId] varchar(max),\n[startLon] float,\n[startLat] float,\n[endLon] float,\n[endLat] float,\n[rateCodeId] int,\n[storeAndFwdFlag] varchar(max),\n[paymentType] varchar(max),\n[fareAmount] float,\n[extra] float,\n[mtaTax] float,\n[improvementSurcharge] varchar(max),\n[tipAmount] float,\n[tollsAmount] float,\n[totalAmount] float \n)\nWITH\n(\n LOCATION = 'part-00064-tid-5849318790271418554-257afbeb-bf9a-4847-a978-d1bf53c44882-77464.c000.snappy.parquet', \n DATA_SOURCE = qingtest_zhaotestgen2_dfs_core_windows_net, \n FILE_FORMAT = SynapseParquetFormat, \n REJECT_TYPE = VALUE, \n REJECT_VALUE = 0\n)\nGO\nSELECT TOP 100 * FROM extable2\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "SQLpool",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CreateLogin-SQLDedicated')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "/*\nTitle: Grant access to a user at the server-level for SQL dedicated/provisioned Pools\n\nDescription: This sample shows how to creat a user and grant permissions using T-SQL.\n\nTags: Security, Login, Roles, SQL Dedicated, SQL Pools\n\n*/\n--1.a Create LOGIN for an Azure AD account.To create a login, you must be connected to the master database.\n\nCREATE LOGIN [v-weiszh@microsoft.com] FROM EXTERNAL PROVIDER;\ngo\n--1.b Create a Login for a user. To create a login, you must be connected to the master database.\nCREATE LOGIN v-weiszh@microsoft.com WITH PASSWORD = '*******';\n\n\n--2.a Create USER for the Azure AD account\nuse master -- Use your DB name\nCREATE USER weishu FROM LOGIN [v-weiszh@microsoft.com]\n\n--2.b Create USER for a SQL Auth account\nCREATE USER weishu FROM LOGIN v-weiszh@microsoft.com\n\n--Example to add the user to the db_owner role\n\nEXEC sp_addrolemember 'db_owner', 'weishu';\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "master",
						"type": "SqlOnDemand"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_SearchLog_table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- type your sql script here, we now have intellisense\nCREATE TABLE SearchLog(\n   id           INTEGER  NOT NULL\n  ,time         DATETIME  NOT NULL\n  ,market       VARCHAR(16) NOT NULL\n  ,searchtext   VARCHAR(255) NOT NULL\n  ,latency      INTEGER  NOT NULL\n  ,links        VARCHAR(255) NOT NULL\n  ,clickedlinks VARCHAR(255)\n);\n\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (399266,'2019-10-15T11:53:04Z','en-us','how to make nachos',73,'www.nachos.com;www.wikipedia.com',NULL);\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (382045,'2019-10-15T11:53:25Z','en-gb','best ski resorts',614,'skiresorts.com;ski-europe.com;www.travelersdigest.com/ski_resorts.htm','ski-europe.com;www.travelersdigest.com/ski_resorts.htm');\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (382045,'2019-10-16T11:53:42Z','en-gb','broken leg',74,'mayoclinic.com/health;webmd.com/a-to-z-guides;mybrokenleg.com;wikipedia.com/Bone_fracture','mayoclinic.com/health;webmd.com/a-to-z-guides;mybrokenleg.com;wikipedia.com/Bone_fracture');\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (106479,'2019-10-16T11:53:10Z','en-ca','south park episodes',24,'southparkstudios.com;wikipedia.org/wiki/Sout_Park;imdb.com/title/tt0121955;simon.com/mall','southparkstudios.com');\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (906441,'2019-10-16T11:54:18Z','en-us','cosmos',1213,'cosmos.com;wikipedia.org/wiki/Cosmos:_A_Personal_Voyage;hulu.com/cosmos',NULL);\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (351530,'2019-10-16T11:54:29Z','en-fr','microsoft',241,'microsoft.com;wikipedia.org/wiki/Microsoft;xbox.com',NULL);\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (640806,'2019-10-16T11:54:32Z','en-us','wireless headphones',502,'www.amazon.com;reviews.cnet.com/wireless-headphones;store.apple.com','www.amazon.com;store.apple.com');\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (304305,'2019-10-16T11:54:45Z','en-us','dominos pizza',60,'dominos.com;wikipedia.org/wiki/Domino''s_Pizza;facebook.com/dominos','dominos.com');\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (460748,'2019-10-16T11:54:58Z','en-us','yelp',1270,'yelp.com;apple.com/us/app/yelp;wikipedia.org/wiki/Yelp,_Inc.;facebook.com/yelp','yelp.com');\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (354841,'2019-10-16T11:59:00Z','en-us','how to run',610,'running.about.com;ehow.com;go.com','running.about.com;ehow.com');\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (354068,'2019-10-16T12:00:07Z','en-mx','what is sql',422,'wikipedia.org/wiki/SQL;sqlcourse.com/intro.html;wikipedia.org/wiki/Microsoft_SQL','wikipedia.org/wiki/SQL');\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (674364,'2019-10-16T12:00:21Z','en-us','mexican food redmond',283,'eltoreador.com;yelp.com/c/redmond-wa/mexican;agaverest.com',NULL);\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (347413,'2019-10-16T12:11:34Z','en-gr','microsoft',305,'microsoft.com;wikipedia.org/wiki/Microsoft;xbox.com',NULL);\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (848434,'2019-10-16T12:12:14Z','en-ch','facebook',10,'facebook.com;facebook.com/login;wikipedia.org/wiki/Facebook','facebook.com');\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (604846,'2019-10-16T12:13:18Z','en-us','wikipedia',612,'wikipedia.org;en.wikipedia.org;en.wikipedia.org/wiki/Wikipedia','wikipedia.org');\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (840614,'2019-10-16T12:13:41Z','en-us','xbox',1220,'xbox.com;en.wikipedia.org/wiki/Xbox;xbox.com/xbox360','xbox.com/xbox360');\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (656666,'2019-10-16T12:15:19Z','en-us','hotmail',691,'hotmail.com;login.live.com;msn.com;en.wikipedia.org/wiki/Hotmail',NULL);\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (951513,'2019-10-16T12:17:37Z','en-us','pokemon',63,'pokemon.com;pokemon.com/us;serebii.net','pokemon.com');\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (350350,'2019-10-16T12:18:17Z','en-us','wolfram',30,'wolframalpha.com;wolfram.com;mathworld.wolfram.com;en.wikipedia.org/wiki/Stephen_Wolfram',NULL);\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (641615,'2019-10-16T12:19:21Z','en-us','kahn',119,'khanacademy.org;en.wikipedia.org/wiki/Khan_(title);answers.com/topic/genghis-khan;en.wikipedia.org/wiki/Khan_(name)','khanacademy.org');\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (321065,'2019-10-16T12:20:19Z','en-us','clothes',732,'gap.com;overstock.com;forever21.com;footballfanatics.com/college_washington_state_cougars','footballfanatics.com/college_washington_state_cougars');\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (651777,'2019-10-16T12:20:49Z','en-us','food recipes',183,'allrecipes.com;foodnetwork.com;simplyrecipes.com','foodnetwork.com');\nINSERT INTO SearchLog(id,time,market,searchtext,latency,links,clickedlinks) VALUES (666380,'2019-10-16T12:21:16Z','en-us','weight loss',630,'en.wikipedia.org/wiki/Weight_loss;webmd.com/diet;exercise.about.com','webmd.com/diet');\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLpool",
						"poolName": "SQLpool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Database-level_Permissions-SQLDedicated')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "/*\nTitle: Grant access to a user to a single SQL dedicated/provisioned database\n\nDescription: This sample shows how to creat a user and grant permissions using T-SQL.\n\nTags: Security, Login, Roles, SQL Dedicated, SQL Pools\n\n*/\n--db_datareader and db_datawriter can work for read/write permissions if granting db_owner permission is undesired. \n--For a Spark user to read and write directly from Spark into/from a SQL pool, db_owner permission is required.\n\n--Create the user in the database by running the following command targeting the desired database in the context selector (dropdown to select databases)  for an Azure AD account:\n\nCREATE USER [v-weiszh@microsoft.com] FROM EXTERNAL PROVIDER;\n\n--Grant the user a role to access the database:\n\nEXEC sp_addrolemember 'db_owner', 'v-weiszh@microsoft.com';",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "SQLpool",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Database-level_Permissions-SQLOD')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "/*\nTitle: Grant access to a user to a single SQL on-demand database\n\nDescription: This sample shows how to creat a user and grant permissions using T-SQL.\n\nTags: Security, Login, Roles, SQL On Demand\n\n*/\n\n--1.a Create LOGIN for an Azure AD account.To create a login, you must be connected to the master database.\n\nCREATE LOGIN [alias@domain.com] FROM EXTERNAL PROVIDER;\n\n\n--1.b Create a Login for a SQL authuser. To create a login, you must be connected to the master database.\nCREATE LOGIN <login_alias> WITH PASSWORD = 'enter_your_password';\nCREATE USER <user_alias> FROM LOGIN <login_alias>\n\n--2.a Create USER for the Azure AD account\n\nuse yourdb -- Use your DB name\nCREATE USER <user_alias> FROM LOGIN [alias@domain.com]\n\n--2.b Create USER for the SQL Auth account\nuse yourdb -- Use your DB name\nCREATE USER <user_alias> FROM LOGIN <login_alias>\n\n--3.Add USER to members of the specified role\n\nuse yourdb -- Use your DB name\ngo\nalter role db_owner Add <user_alias> -- Type user alias from step 2",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "master",
						"type": "SqlOnDemand"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dynamic_Data_Masking-SQLDedicated')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "/*\nTitle: Dynamic Data Masking for SQL dedicated/provisioned Pools\n\nDescription: This sample shows how to implement Dynamic Data Masking to limit sensitive data exposure by masking it to non-privileged users.\n-- Lijing please use this link for DDM\n--https://docs.microsoft.com/en-us/sql/relational-databases/security/dynamic-data-masking?view=azure-sqldw-latest#examples\nTags: Security, Dynamic Data Masking, SQL Dedicated, SQL Pools\n\n*/\nCREATE TABLE Membership1  \n  (MemberID int IDENTITY,  \n   FirstName varchar(100) MASKED WITH (FUNCTION = 'partial(1,\"XXXXXXX\",0)') NULL,  \n   LastName varchar(100) NOT NULL,  \n   Phone varchar(12) MASKED WITH (FUNCTION = 'default()') NULL,  \n   Email varchar(100) MASKED WITH (FUNCTION = 'email()') NULL);  \n\n --Insert sample data in the table \nINSERT Membership VALUES (1,'Roberto', 'Tamburello', '555.123.4567', 'RTamburello@contoso.com');  \nINSERT Membership VALUES (2,'Janice', 'Galvin', '555.123.4568', 'JGalvin@contoso.com.co'); \nINSERT Membership VALUES (3,'Zheng', 'Mu', '555.123.4569', 'ZMu@contoso.net');  \nSELECT * FROM Membership;\n\n--A new user is created and granted SELECT permission on the table. Queries executed as the TestUser view masked data.\nCREATE USER TestUser WITHOUT LOGIN;  \nGRANT SELECT ON Membership TO TestUser;  \n \n --Test user permissions\nEXECUTE AS USER = 'TestUser';  \nSELECT * FROM Membership;  \nREVERT;\n\n--The following example adds a masking function to the LastName column\nALTER TABLE Membership  \nALTER COLUMN LastName ADD MASKED WITH (FUNCTION = 'partial(2,\"XXX\",0)');\n\n--The following example changes a masking function on the LastName column\nALTER TABLE Membership  \nALTER COLUMN LastName varchar(100) MASKED WITH (FUNCTION = 'default()');\n\n--Granting the UNMASK permission allows TestUser to see the data unmasked\nGRANT UNMASK TO TestUser;  \nEXECUTE AS USER = 'TestUser';  \nSELECT * FROM Membership;  \nREVERT;   \n  \n-- Removing the UNMASK permission  \nREVOKE UNMASK TO TestUser;\n\n--The following statement drops the mask on the LastName column created in the previous example\nALTER TABLE Membership   \nALTER COLUMN LastName DROP MASKED;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "SQLpool",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/EXECUTE PROC_ExtParquet')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "\n -- TODO: Set parameter values here \n\n EXECUTE [dbo].[PROC_ExtParquet]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "SQLpool",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/EXECUTE PROC_Read_ExtParquet_DBZ')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "\n -- TODO: Set parameter values here \n\n EXECUTE [dbo].[PROC_Read_ExtParquet_DBZ]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "SQLpool",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/EXECUTE PROC_Read_SearchLog')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "\n -- TODO: Set parameter values here \n\n EXECUTE [dbo].[PROC_Read_SearchLog]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "SQLpool",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/EXECUTE PROC_Read_SearchLogExtParquet')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "\n -- TODO: Set parameter values here \n\n EXECUTE [dbo].[PROC_Read_SearchLogExtParquet]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "SQLpool",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/EXECUTE PROC_Read_SearchLogExtParquet_DBZ')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "\n -- TODO: Set parameter values here \n\n EXECUTE [dbo].[PROC_Read_SearchLogExtParquet_DBZ]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "SQLpool",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/EXECUTE PROC_Read_SearchLog_DBZ')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "\n -- TODO: Set parameter values here \n\n EXECUTE [dbo].[PROC_Read_SearchLog_DBZ]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "SQLpool",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/EXECUTE PROC_SELECT_1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "\n -- TODO: Set parameter values here \n\n EXECUTE [dbo].[PROC_SELECT_1]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "SQLpool",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Query CSV files')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "/* Covid-19 ECDC cases opendata set */\n\n/* Read a csv file */\nselect top 10 *\nfrom openrowset(\n    bulk 'https://pandemicdatalake.blob.core.windows.net/public/curated/covid-19/ecdc_cases/latest/ecdc_cases.csv',\n    format = 'csv',\n    parser_version = '2.0',\n    firstrow = 2 ) as rows\n\n\n\n/* Explicitly specify schema */\nselect top 10 *\nfrom openrowset(\n        bulk 'https://pandemicdatalake.blob.core.windows.net/public/curated/covid-19/ecdc_cases/latest/ecdc_cases.csv',\n        format = 'csv',\n        parser_version ='2.0',\n        firstrow = 2\n    ) with (\n        date_rep date 1,\n        cases int 5,\n        geo_id varchar(6) 8\n    ) as rows\n\n/* Windows style new line */\nSELECT *\nFROM OPENROWSET(\n        BULK 'https://sqlondemandstorage.blob.core.windows.net/public-csv/population/population.csv',\n        FORMAT = 'CSV', PARSER_VERSION = '2.0',\n        FIELDTERMINATOR =',',\n        ROWTERMINATOR = '\n'\n    )\nWITH (\n    [country_code] VARCHAR (5) COLLATE Latin1_General_BIN2,\n    [country_name] VARCHAR (100) COLLATE Latin1_General_BIN2,\n    [year] smallint,\n    [population] bigint\n) AS [r]\nWHERE\n    country_name = 'Luxembourg'\n    AND year = 2017;\n\n\n/* Unix-style new line */\nSELECT *\nFROM OPENROWSET(\n        BULK 'https://sqlondemandstorage.blob.core.windows.net/public-csv/population-unix/population.csv',\n        FORMAT = 'CSV', PARSER_VERSION = '2.0',\n        FIELDTERMINATOR =',',\n        ROWTERMINATOR = '0x0a'\n    )\nWITH (\n    [country_code] VARCHAR (5) COLLATE Latin1_General_BIN2,\n    [country_name] VARCHAR (100) COLLATE Latin1_General_BIN2,\n    [year] smallint,\n    [population] bigint\n) AS [r]\nWHERE\n    country_name = 'Luxembourg'\n    AND year = 2017;\n\n\n/* Header row */\nSELECT *\nFROM OPENROWSET(\n        BULK 'https://sqlondemandstorage.blob.core.windows.net/public-csv/population-unix-hdr/population.csv',\n        FORMAT = 'CSV', PARSER_VERSION = '2.0',\n        FIELDTERMINATOR =',',\n        FIRSTROW = 2\n    )\n    WITH (\n        [country_code] VARCHAR (5) COLLATE Latin1_General_BIN2,\n        [country_name] VARCHAR (100) COLLATE Latin1_General_BIN2,\n        [year] smallint,\n        [population] bigint\n    ) AS [r]\nWHERE\n    country_name = 'Luxembourg'\n    AND year = 2017;\n\n\n/* Custom quote character */\nSELECT *\nFROM OPENROWSET(\n        BULK 'https://sqlondemandstorage.blob.core.windows.net/public-csv/population-unix-hdr-quoted/population.csv',\n        FORMAT = 'CSV', PARSER_VERSION = '2.0',\n        FIELDTERMINATOR =',',\n        ROWTERMINATOR = '0x0a',\n        FIRSTROW = 2,\n        FIELDQUOTE = '\"'\n    )\n    WITH (\n        [country_code] VARCHAR (5) COLLATE Latin1_General_BIN2,\n        [country_name] VARCHAR (100) COLLATE Latin1_General_BIN2,\n        [year] smallint,\n        [population] bigint\n    ) AS [r]\nWHERE\n    country_name = 'Luxembourg'\n    AND year = 2017;\n\n\n/* Escape characters */\nSELECT *\nFROM OPENROWSET(\n        BULK 'https://sqlondemandstorage.blob.core.windows.net/public-csv/population-unix-hdr-escape/population.csv',\n        FORMAT = 'CSV', PARSER_VERSION = '2.0',\n        FIELDTERMINATOR =',',\n        ROWTERMINATOR = '0x0a',\n        FIRSTROW = 2,\n        ESCAPECHAR = ''\n    )\n    WITH (\n        [country_code] VARCHAR (5) COLLATE Latin1_General_BIN2,\n        [country_name] VARCHAR (100) COLLATE Latin1_General_BIN2,\n        [year] smallint,\n        [population] bigint\n    ) AS [r]\nWHERE\n    country_name = 'Slovenia';\n\n\n/* Escape quoting characters */\nSELECT *\nFROM OPENROWSET(\n        BULK 'https://sqlondemandstorage.blob.core.windows.net/public-csv/population-unix-hdr-escape-quoted/population.csv',\n        FORMAT = 'CSV', PARSER_VERSION = '2.0',\n        FIELDTERMINATOR =',',\n        ROWTERMINATOR = '0x0a',\n        FIRSTROW = 2\n    )\n    WITH (\n        [country_code] VARCHAR (5) COLLATE Latin1_General_BIN2,\n        [country_name] VARCHAR (100) COLLATE Latin1_General_BIN2,\n        [year] smallint,\n        [population] bigint\n    ) AS [r]\nWHERE\n    country_name = 'Slovenia';\n\n\n/* Tab-delimited files */\nSELECT *\nFROM OPENROWSET(\n        BULK 'https://sqlondemandstorage.blob.core.windows.net/public-csv/population-unix-hdr-tsv/population.csv',\n        FORMAT = 'CSV', PARSER_VERSION = '2.0',\n        FIELDTERMINATOR ='\t',\n        ROWTERMINATOR = '0x0a',\n        FIRSTROW = 2\n    )\n    WITH (\n        [country_code] VARCHAR (5) COLLATE Latin1_General_BIN2,\n        [country_name] VARCHAR (100) COLLATE Latin1_General_BIN2,\n        [year] smallint,\n        [population] bigint\n    ) AS [r]\nWHERE\n    country_name = 'Luxembourg'\n    AND year = 2017\n\n\n/* Return a subset of columns */\nSELECT\n    COUNT(DISTINCT country_name) AS countries\nFROM OPENROWSET(\n        BULK 'https://sqlondemandstorage.blob.core.windows.net/public-csv/population/population.csv',\n        FORMAT = 'CSV', PARSER_VERSION = '2.0',\n        FIELDTERMINATOR =',',\n        ROWTERMINATOR = '\n'\n    )\nWITH (\n    --[country_code] VARCHAR (5),\n    [country_name] VARCHAR (100) 2\n    --[year] smallint,\n    --[population] bigint\n) AS [r]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "master",
						"type": "SqlOnDemand"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL rename2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "select 123456",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 11')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.objects O JOIN sys.schemas S ON O.schema_id = S.schema_id WHERE O.NAME = 'bulk1' AND O.TYPE = 'U' AND S.NAME = 'dbo')\nCREATE TABLE dbo.bulk1\n\t(\n\t [vendorID] nvarchar(4000),\n\t [tpepPickupDateTime] datetime2(7),\n\t [tpepDropoffDateTime] datetime2(7),\n\t [passengerCount] int,\n\t [tripDistance] float,\n\t [puLocationId] nvarchar(4000),\n\t [doLocationId] nvarchar(4000),\n\t [startLon] float,\n\t [startLat] float,\n\t [endLon] float,\n\t [endLat] float,\n\t [rateCodeId] int,\n\t [storeAndFwdFlag] nvarchar(4000),\n\t [paymentType] nvarchar(4000),\n\t [fareAmount] float,\n\t [extra] float,\n\t [mtaTax] float,\n\t [improvementSurcharge] nvarchar(4000),\n\t [tipAmount] float,\n\t [tollsAmount] float,\n\t [totalAmount] float\n\t)\nWITH\n\t(\n\tDISTRIBUTION = ROUND_ROBIN,\n\t CLUSTERED COLUMNSTORE INDEX\n\t -- HEAP\n\t)\nGO\n\n--Uncomment the 4 lines below to create a stored procedure for data pipeline orchestration\n--CREATE PROC bulk_load_bulk1\n--AS\n--BEGIN\nCOPY INTO dbo.bulk1\n(vendorID 1, tpepPickupDateTime 2, tpepDropoffDateTime 3, passengerCount 4, tripDistance 5, puLocationId 6, doLocationId 7, startLon 8, startLat 9, endLon 10, endLat 11, rateCodeId 12, storeAndFwdFlag 13, paymentType 14, fareAmount 15, extra 16, mtaTax 17, improvementSurcharge 18, tipAmount 19, tollsAmount 20, totalAmount 21)\nFROM 'https://honghaigen2.dfs.core.windows.net/default/part-00036-tid-210938564719836543-aea5b543-5e83-4a7d-8d31-69f72c50b05d-15156-1.c000.snappy.parquet'\nWITH\n(\n\tFILE_TYPE = 'PARQUET'\n\t,MAXERRORS = 0\n\t,IDENTITY_INSERT = 'OFF'\n)\n--END\nGO\n\nSELECT TOP 100 * FROM dbo.bulk1\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "123",
						"poolName": "123"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 12')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'default_honghaigen2_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [default_honghaigen2_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://default@honghaigen2.dfs.core.windows.net', \n\t\tTYPE     = HADOOP \n\t)\nGO\n\nCREATE EXTERNAL TABLE exter1 (\n\t[vendorID] varchar(8000),\n\t[tpepPickupDateTime] datetime2(7),\n\t[tpepDropoffDateTime] datetime2(7),\n\t[passengerCount] int,\n\t[tripDistance] float,\n\t[puLocationId] varchar(8000),\n\t[doLocationId] varchar(8000),\n\t[startLon] float,\n\t[startLat] float,\n\t[endLon] float,\n\t[endLat] float,\n\t[rateCodeId] int,\n\t[storeAndFwdFlag] varchar(8000),\n\t[paymentType] varchar(8000),\n\t[fareAmount] float,\n\t[extra] float,\n\t[mtaTax] float,\n\t[improvementSurcharge] varchar(8000),\n\t[tipAmount] float,\n\t[tollsAmount] float,\n\t[totalAmount] float\n\t)\n\tWITH (\n\tLOCATION = 'part-00036-tid-210938564719836543-aea5b543-5e83-4a7d-8d31-69f72c50b05d-15156-1.c000.snappy.parquet',\n\tDATA_SOURCE = [default_honghaigen2_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat],\n\tREJECT_TYPE = VALUE,\n\tREJECT_VALUE = 0\n\t)\nGO\n\nSELECT TOP 150 * FROM exter1\nGO\n\nSELECT 2/([passengerCount]-1) FROM exter1 WHERE [passengerCount] = 1\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "123",
						"poolName": "123"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1rename')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE TABLE test(\n    id int,\n    name  CHAR(100)\n)\n\nSELECT 123",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "ws",
						"poolName": "ws"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- type your sql script here, we now have intellisense\nCREATE TABLE users12(\nLastName varchar(255),\nFirstName varchar(255),\nCity varchar(255)\n)",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- type your sql script here, we now have intellisense\nCREATE TABLE users12(\nLastName varchar(255),\nFirstName varchar(255),\nCity varchar(255)\n)",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2rename')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "sacasexzc",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 4')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- type your sql script here, we now have intellisense\n--Create Table\nCREATE TABLE [dbo].[personscat6]\n(\nId_P INT NOT NULL,\nLastName varchar(255) NOT NULL,\nFirstName varchar(255) NOT NULL,\nAddress varchar(255) NOT NULL,\nCity varchar(255) NOT NULL\n)\nCREATE TABLE [dbo].[orderscat6]\n(\nId_O INT NOT NULL,\nOrderNo INT NOT NULL,\nId_P INT NOT NULL,\nCompany varchar(255),\nSalary INT NOT NULL\n)\nGO\n\n\n--insert into dbo.persons--\nINSERT INTO [dbo].[personscat6] VALUES ('3','Gates', 'Bill', 'Xuanwumen 10', 'Beijing')\nINSERT INTO [dbo].[personscat6] VALUES ('2','xiaoqiang', 'wang', 'danlingjie', 'Beijing')\nINSERT INTO [dbo].[personscat6] VALUES ('1','maomao', 'dou', 'folower', 'shenzhen')\nINSERT INTO [dbo].[personscat6] VALUES ('4','mike', 'dou', 'folower', 'shenzhen')\nINSERT INTO [dbo].[personscat6] VALUES ('5','jack', 'dou', '333', 'shenzhen')\nINSERT INTO [dbo].[personscat6] VALUES ('7','amy', 'dou', 'folower13', 'shenzhen')\nINSERT INTO [dbo].[personscat6] VALUES ('6','Athena', 'dou', 'folower789', 'shenzhen')\n--insert into dbo.orders--\nINSERT INTO [dbo].[orderscat6] VALUES ('1','77895','3','company1','150000')\nINSERT INTO [dbo].[orderscat6] VALUES ('2','44678','3','company2','200000')\nINSERT INTO [dbo].[orderscat6] VALUES ('3','22456','1','company3','185000')\nINSERT INTO [dbo].[orderscat6] VALUES ('4','34765','1','company4','220000')\nINSERT INTO [dbo].[orderscat6] VALUES ('5','42307','10','company4','120000')\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "xiaocw0512",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 5')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- type your sql script here, we now have intellisense\nCREATE TABLE [DBO].[TEST](\n    ID INT,\n    MSG VARCHAR(255)\n);",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "SQLpool",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 7')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE VIEW [dbo].[View]\n\tAS SELECT  TOP 100 * FROM [SomeTableOrView]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "xssc",
						"poolName": "xssc"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 8')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "master",
						"type": "SqlOnDemand"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script for power bi')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "--Create Table\nCREATE TABLE [dbo].[personsxssc]\n(\nId_P INT NOT NULL,\nLastName varchar(255) NOT NULL,\nFirstName varchar(255) NOT NULL,\nAddress varchar(255) NOT NULL,\nCity varchar(255) NOT NULL\n)\nCREATE TABLE [dbo].[ordersxssc]\n(\nId_O INT NOT NULL,\nOrderNo INT NOT NULL,\nId_P INT NOT NULL,\nCompany varchar(255),\nSalary INT NOT NULL\n)\nGO\n--insert into dbo.persons--\nINSERT INTO [dbo].[personsxssc] VALUES ('3','Gates', 'Bill', 'Xuanwumen 10', 'Beijing')\nINSERT INTO [dbo].[personsxssc] VALUES ('2','xiaoqiang', 'wang', 'danlingjie', 'Beijing')\nINSERT INTO [dbo].[personsxssc] VALUES ('1','maomao', 'dou', 'folower', 'shenzhen')\nINSERT INTO [dbo].[personsxssc] VALUES ('4','mike', 'dou', 'folower', 'shenzhen')\nINSERT INTO [dbo].[personsxssc] VALUES ('5','jack', 'dou', '333', 'shenzhen2')\nINSERT INTO [dbo].[personsxssc] VALUES ('7','amy', 'dou', 'folower13', 'shenzhen')\nINSERT INTO [dbo].[personsxssc] VALUES ('6','Athena', 'dou', 'folower789', 'shenzhen')\n\n--insert into dbo.orders--\nINSERT INTO [dbo].[ordersxssc] VALUES ('1','77895','3','company1','150000')\nINSERT INTO [dbo].[ordersxssc] VALUES ('2','44678','3','company2','200000')\nINSERT INTO [dbo].[ordersxssc] VALUES ('3','22456','1','company3','185000')\nINSERT INTO [dbo].[ordersxssc] VALUES ('4','34765','1','company4','220000')\nINSERT INTO [dbo].[ordersxssc] VALUES ('5','42307','10','company4','120000')",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlpool",
						"poolName": "sqlpool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script rename')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "master",
						"type": "SqlOnDemand"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script rename3')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "master",
						"type": "SqlOnDemand"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "master",
						"type": "SqlOnDemand"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQLCopy1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE TABLE test(\n    id int,\n    name  CHAR(100)\n)\n\nSELECT 123",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "ws",
						"poolName": "ws"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Server-level_Permissions-SQLOD')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "/*\nTitle: Grant access to a user at the server-level for SQL on-demand \n\nDescription: This sample shows how to creat a user and grant permissions using T-SQL.\n\nTags: Security, Login, Roles, SQL On Demand\n\n*/\n--To grant full access to a user to all SQL on-demand database for an Azure AD account\n\nCREATE LOGIN [v-weiszh@microsoft.com] FROM EXTERNAL PROVIDER;\nALTER SERVER ROLE  sysadmin  ADD MEMBER [v-weiszh@microsoft.com];\n\n--Grant full access to a user to all SQL on-demand database for SQl Auth account. To create a login, you must be connected to the master database.\nCREATE LOGIN [v-weiszh@bdbj.onmicrosoft.com] WITH PASSWORD = '';\n\nCREATE USER weishu FROM LOGIN [v-weiszh@bdbj.onmicrosoft.com]\n--Example to add the user to the sysadmin role\nALTER SERVER ROLE sysadmin ADD MEMBER weishu",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "test01",
						"type": "SqlOnDemand"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/alert-table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "ALTER TABLE [dbo].[BigData]\n    ADD msg2 varchar(255)",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "master",
						"type": "SqlOnDemand"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bigdata test')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "/**/\n--create table\n\nCREATE TABLE [dbo].[BigData2]\n(\n    Id INT NOT NULL,\n    created_time DATETIME NOT NULL\n)\n\n--add msg column\nALTER TABLE [dbo].[BigData2]\n    ADD msg varchar(255)\n\n\n--insert some test data\nDECLARE @i int = 1;\nDECLARE @alphabet varchar(36) = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\nWHILE @i <= 500\nBegin\nDECLARE @stamp datetime = CURRENT_TIMESTAMP;\nDECLARE @msg varchar(36)=\nsubstring(@alphabet, convert(int, rand()*36), 1) +\nsubstring(@alphabet, convert(int, rand()*36), 1) +\nsubstring(@alphabet, convert(int, rand()*36), 1) +\nsubstring(@alphabet, convert(int, rand()*36), 1) +\nsubstring(@alphabet, convert(int, rand()*36), 1);\nINSERT INTO [dbo].[BigData2] VALUES(@i,@stamp,@msg);\nSET @i+=1;\nEND\n\n\n--select\nSELECT * FROM BigData2 ORDER BY [BigData2].[Id] \nSELECT Id,msg FROM BigData2 WHERE Id=10 OR Id=20\nSELECT * FROM BigData2 WHERE Id>=200 ORDER BY Id\nSELECT * FROM BigData2 WHERE Id>=10 AND Id<240 ORDER BY Id\n\n\n--DELETE\n--DELETE FROM BigData2",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bigdata test_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "/**/\n--create table\n\nCREATE TABLE [dbo].[BigData2]\n(\n    Id INT NOT NULL,\n    created_time DATETIME NOT NULL\n)\n\n--add msg column\nALTER TABLE [dbo].[BigData2]\n    ADD msg varchar(255)\n\n\n--insert some test data\nDECLARE @i int = 1;\nDECLARE @alphabet varchar(36) = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\nWHILE @i <= 500\nBegin\nDECLARE @stamp datetime = CURRENT_TIMESTAMP;\nDECLARE @msg varchar(36)=\nsubstring(@alphabet, convert(int, rand()*36), 1) +\nsubstring(@alphabet, convert(int, rand()*36), 1) +\nsubstring(@alphabet, convert(int, rand()*36), 1) +\nsubstring(@alphabet, convert(int, rand()*36), 1) +\nsubstring(@alphabet, convert(int, rand()*36), 1);\nINSERT INTO [dbo].[BigData2] VALUES(@i,@stamp,@msg);\nSET @i+=1;\nEND\n\n\n--select\nSELECT * FROM BigData2 ORDER BY [BigData2].[Id] \nSELECT Id,msg FROM BigData2 WHERE Id=10 OR Id=20\nSELECT * FROM BigData2 WHERE Id>=200 ORDER BY Id\nSELECT BigData2.msg FROM BigData2 WHERE Id>=10 AND Id<240 ORDER BY Id\n\n\n--DELETE\n--DELETE FROM BigData2",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLpool",
						"poolName": "SQLpool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/create-table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE TABLE [dbo].[persons7]\n(\n    Id_P INT NOT NULL,\n    LastName varchar(255) NOT NULL,\n    FirstName varchar(255) NOT NULL,\n    Address varchar(255) NOT NULL,\n    City varchar(255) NOT NULL\n)\n\nCREATE TABLE [dbo].[orders7]\n(\n    Id_O INT NOT NULL, \n    OrderNo INT NOT NULL,\n    Id_P INT NOT NULL,\n    Company varchar(255)\n)\n\nGO\nselect 11 as res\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "database02",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/insert-into-table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "--insert into dbo.persons--\n\nINSERT INTO [dbo].[persons7] VALUES ('3','Gates', 'Bill', 'Xuanwumen 10', 'Beijing')\nINSERT INTO [dbo].[persons7] VALUES ('2','xiaoqiang', 'wang', 'danlingjie', 'Beijing')\nINSERT INTO [dbo].[persons7] VALUES ('1','maomao', 'dou', 'folower','shenzhen')\nINSERT INTO [dbo].[persons7] VALUES ('4','mike', 'dou', 'folower', 'shenzhen')\nINSERT INTO [dbo].[persons7] VALUES ('5','jack', 'dou', '333', 'shenzhen')\nINSERT INTO [dbo].[persons7] VALUES ('7','amy', 'dou', 'folower13', 'shenzhen')\nINSERT INTO [dbo].[persons7] VALUES ('6','Athena', 'dou', 'folower789', 'shenzhen')\n\n\n--insert into dbo.persons--\nINSERT INTO [dbo].[orders7] VALUES ('1','77895','3','company1')\nINSERT INTO [dbo].[orders7] VALUES ('2','44678','3','company2')\nINSERT INTO [dbo].[orders7] VALUES ('3','22456','1','company3')\nINSERT INTO [dbo].[orders7] VALUES ('4','34765','1','company4')\nINSERT INTO [dbo].[orders7] VALUES ('5','42307','10','company4')\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "SQLpool",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/select-script')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "\n--count function--\nSELECT COUNT(t2.[OrderNo]) as [OrderCount]\n    FROM [dbo].[persons] as t1 \n    left join [dbo].[orders] as t2 on t1.[Id_P]=t2.[Id_P]\n    WHERE T2.[Id_P]=3\n\n--order by--\nSELECT T1.[Id_P] AS [ip],t1.[LastName],t1.[FirstName],t2.[company] AS [orderclient]\n    FROM [dbo].[persons] as t1 \n    left join [dbo].[orders] as t2 on t1.[Id_P]=t2.[Id_P]\n    ORDER BY T1.[Id_P]\n\n--group by--\nSELECT [orders].[company] AS [clinet]\n    FROM [dbo].[orders] \n    GROUP BY [orders].[company]\n\nSELECT*FROM [dbo].[persons7] WHERE ID_P = 5  go",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLpool",
						"poolName": "SQLpool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/update table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "UPDATE [dbo].[persons7] \nSET FirstName = 'wan' \nWHERE ID_P = 5",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "SQLpool",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/KQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/kqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "| where timestamp > ago(7d)\n| where name == \"PBI.Navigation.RouteChange\"\n| extend toPath = tostring(customDimensions.toPath)\n| where toPath has \"manageembed\" \n| summarize count() by toPath\n",
					"metadata": {
						"language": "kql"
					}
				},
				"type": "KqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01 Read and write data from Azure Data Lake Storage Gen2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder/python"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8b80cf79-326a-438a-bd67-aa4eaef4a358"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Access data on Azure Data Lake Storage Gen2 (ADLS Gen2) with Synapse Spark\n",
							"\n",
							"Azure Data Lake Storage Gen2 (ADLS Gen2) is used as the storage account associated with a Synapse workspace. A synapse workspace can have a default ADLS Gen2 storage account and additional linked storage accounts. \n",
							"\n",
							"You can access data on ADLS Gen2 with Synapse Spark via following URL:\n",
							"    \n",
							"    abfss://<container_name>@<storage_account_name>.dfs.core.windows.net/<path>\n",
							"\n",
							"This notebook provides examples of how to read data from ADLS Gen2 account into a Spark context and how to write the output of Spark jobs directly into an ADLS Gen2 location.\n",
							"\n",
							"## Pre-requisites\n",
							"Synapse leverage AAD pass-through to access any ADLS Gen2 account (or folder) to which you have a **Blob Storage Contributor** permission. No credentials or access token is required. "
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Load a sample data\n",
							"\n",
							"Let's first load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) of last 6 months from Azure Open datasets as a sample."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import PublicHolidays\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"from dateutil.relativedelta import relativedelta\n",
							"\n",
							"\n",
							"end_date = datetime.today()\n",
							"start_date = datetime.today() - relativedelta(months=6)\n",
							"hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
							"hol_df = hol.to_spark_dataframe()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"# Display 5 rows\n",
							"hol_df.show(5, truncate = False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write data to the default ADLS Gen2 storage\n",
							"\n",
							"We are going to write the spark dateframe to your default ADLS Gen2 storage account.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.types import *\n",
							"\n",
							"# Primary storage info\n",
							"account_name = 'Your primary storage account name' # fill in your primary account name\n",
							"container_name = 'Your container name' # fill in your container name\n",
							"relative_path = 'Your relative path' # fill in your relative folder path\n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path)\n",
							"print('Primary storage account path: ' + adls_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Save a dataframe as Parquet, JSON or CSV\n",
							"If you have a dataframe, you can save it to Parquet or JSON with the .write.parquet(), .write.json() and .write.csv() methods respectively.\n",
							"\n",
							"Dataframes can be saved in any format, regardless of the input format.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_path = adls_path + 'holiday.parquet'\n",
							"json_path = adls_path + 'holiday.json'\n",
							"csv_path = adls_path + 'holiday.csv'\n",
							"print('parquet file path: ' + parquet_path)\n",
							"print('json file path ' + json_path)\n",
							"print('csv file path: ' + csv_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"hol_df.write.parquet(parquet_path, mode = 'overwrite')\n",
							"hol_df.write.json(json_path, mode = 'overwrite')\n",
							"hol_df.write.csv(csv_path, mode = 'overwrite', header = 'true')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Save a dataframe as text files\n",
							"If you have a dataframe that you want ot save as text file, you must first covert it to an RDD and then save that RDD as a text file.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Define the text file path\n",
							"text_path = adls_path + 'holiday.txt'\n",
							"print('text file path: ' + text_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"# Covert spark dataframe into RDD \n",
							"hol_RDD = hol_df.rdd\n",
							"type(hol_RDD)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"source": [
							"If you have an RDD, you can convert it to a text file like the following:\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							" # Save RDD as text file\n",
							"hol_RDD.saveAsTextFile(text_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Read data from the default ADLS Gen2 storage\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create a dataframe from parquet files\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df_parquet = spark.read.parquet(parquet_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create a dataframe from JSON files\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df_json = spark.read.json(json_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create a dataframe from CSV files\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df_csv = spark.read.csv(csv_path, header = 'true')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create an RDD from text file\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"text = sc.textFile(text_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 17
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/02 Read and write data from Azure Blob Storage WASB')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder/python"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "608f4f25-69b7-4fb5-ab75-a7ec9683e83f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Access data on Azure Storage Blob (WASB) with Synapse Spark\n",
							"\n",
							"You can access data on Azure Storage Blob (WASB) with Synapse Spark via following URL:\n",
							"\n",
							"    wasb[s]://<container_name>@<storage_account_name>.blob.core.windows.net/<path>\n",
							"\n",
							"This notebook provides examples of how to read data from WASB into a Spark context and how to write the output of Spark jobs directly into a WASB location."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Load a sample data\n",
							"\n",
							"Let's first load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) of last 6 months from Azure Open datasets as a sample."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import PublicHolidays\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"from dateutil.relativedelta import relativedelta\n",
							"\n",
							"\n",
							"end_date = datetime.today()\n",
							"start_date = datetime.today() - relativedelta(months=6)\n",
							"hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
							"hol_df = hol.to_spark_dataframe()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"# Display 5 rows\n",
							"hol_df.show(5, truncate = False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write data to Azure Storage Blob\n",
							"\n",
							"We are going to write the spark dateframe to your Azure Blob Storage (WASB) path using **shared access signature (sas)**. Go to [Azure Portal](https://portal.azure.com/), open your Azure storage blob, select **shared access signature** in the **settings** and generate your sas token. Please make sure to allow contatiner level read and write permission. Fill in the access info for your Azure storage blob in the cell below. \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.types import *\n",
							"\n",
							"# Azure storage access info\n",
							"blob_account_name = 'Your blob name' # replace with your blob name\n",
							"blob_container_name = 'Your container name' # replace with your container name\n",
							"blob_relative_path = 'Your relative path' # replace with your relative folder path\n",
							"blob_sas_token = r'Your sas token' # replace with your access key"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Allow SPARK to access from Blob remotely\n",
							"wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
							"spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\n",
							"print('Remote blob path: ' + wasbs_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Save a dataframe as Parquet, JSON or CSV\n",
							"If you have a dataframe, you can save it to Parquet or JSON with the .write.parquet(), .write.json() and .write.csv() methods respectively.\n",
							"\n",
							"Dataframes can be saved in any format, regardless of the input format.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_path = wasbs_path + 'holiday.parquet'\n",
							"json_path = wasbs_path + 'holiday.json'\n",
							"csv_path = wasbs_path + 'holiday.csv'\n",
							"print('parquet file path: ' + parquet_path)\n",
							"print('json file path ' + json_path)\n",
							"print('csv file path: ' + csv_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"hol_df.write.parquet(parquet_path, mode = 'overwrite')\n",
							"hol_df.write.json(json_path, mode = 'overwrite')\n",
							"hol_df.write.csv(csv_path, mode = 'overwrite', header = 'true')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Save a dataframe as text files\n",
							"If you have a dataframe that you want ot save as text file, you must first covert it to an RDD and then save that RDD as a text file.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Define the text file path\n",
							"text_path = wasbs_path + 'holiday.txt'\n",
							"print('text file path: ' + text_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							"# Covert spark dataframe into RDD \n",
							"hol_RDD = hol_df.rdd\n",
							"type(hol_RDD)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"source": [
							"If you have an RDD, you can convert it to a text file like the following:\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							" # Save RDD as text file\n",
							"hol_RDD.saveAsTextFile(text_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Read data from Azure Storage Blob\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create a dataframe from parquet files\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df_parquet = spark.read.parquet(parquet_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create a dataframe from JSON files\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df_json = spark.read.json(json_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create a dataframe from CSV files\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df_csv = spark.read.csv(csv_path, header = 'true')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create an RDD from text file\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"text = sc.textFile(text_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/03 Read and write from SQL pool table')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder/python"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "24b908c1-415a-47a3-b45d-fd74c0e71904"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "scala"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Access Synapse SQL table from Synapse Spark\n",
							"\n",
							"This notebook provides examples of how to read data from Synapse SQL into a Spark context and how to write the output of Spark jobs into an Synapse SQL table.\n",
							"\n",
							"\n",
							"## Limits\n",
							"- Scala is the only supported language by the Spark-SQL connector.\n",
							"- The Spark connector can only read colummns without space in its header in the sql pool.\n",
							"- Columns with time definition in the sql pool not yet supported.\n",
							"- You need to define a container on the workspace's primary or linked storage as the temp data folder.\n",
							"\n",
							"## Pre-requisites\n",
							"You need to be db_owner to read and write in sql pool. Ask your admin to run the following command with your AAD credential:\n",
							"\n",
							"    \n",
							"    EXEC sp_addrolemember 'db_owner', 'AAD@contoso.com'"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Load a sample data\n",
							"\n",
							"Let's first load the [Public Holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) of last 6 months from Azure Open datasets as a sample.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark \n",
							"# Load sample data from azure open dataset in pyspark\n",
							"from azureml.opendatasets import PublicHolidays\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"from dateutil.relativedelta import relativedelta\n",
							"\n",
							"\n",
							"end_date = datetime.today()\n",
							"start_date = datetime.today() - relativedelta(months=6)\n",
							"hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
							"hol_df = hol.to_spark_dataframe()\n",
							"\n",
							"print('Register the DataFrame as a SQL temporary view: source')\n",
							"hol_df.createOrReplaceTempView('source')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"// Remove datetime from the data source\n",
							"val holiday_nodate = spark.sql(\"SELECT countryOrRegion, holidayName, normalizeHolidayName,isPaidTimeOff,countryRegionCode FROM source\")\n",
							"holiday_nodate.show(5,truncate = false)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write a Spark dataframe into your sql pool\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Write the dataframe into your sql pool\n",
							"import org.apache.spark.sql.SqlAnalyticsConnector._\n",
							"import com.microsoft.spark.sqlanalytics.utils.Constants\n",
							"\n",
							"val sql_pool_name = \"Your sql pool name\" //fill in your sql pool name\n",
							"\n",
							"holiday_nodate.write\n",
							"    .sqlanalytics(s\"$sql_pool_name.dbo.PublicHoliday\", Constants.INTERNAL)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"Now open Synapse object explorer and go to **Data**->**Databases**->**<your sql pool name>**->**Tables**, you will see the new **dbo.PublicHoliday** table show up there."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read from a SQL Pool table with Spark\n",
							"\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Read  the table we just created in the sql pool as a Spark dataframe\n",
							"val spark_read = spark.read.\n",
							"    sqlanalytics(s\"$sql_pool_name.dbo.PublicHoliday\")\n",
							"spark_read.show(5, truncate = false)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/04 Using Delta Lake in Azure Synapse')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder/python"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b7101080-b01d-4239-9f58-733411612a10"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Using Linux Foundation Delta Lake in Azure Synapse Analytics Spark\n",
							"Azure Synapse is compatible with Linux Foundation Delta Lake. Delta Lake is an open-source storage layer that brings ACID (atomicity, consistency, isolation, and durability) transactions to Apache Spark and big data workloads.\n",
							"\n",
							"This notebook provides examples of how to update, merge and delete delta lake tables in Synapse.\n",
							"\n",
							"## Pre-requisites\n",
							"In this notebook you will save your tables in Delta Lake format to your workspace's primary storage account. You are required to be a **Blob Storage Contributor** in the ADLS Gen2 account (or folder) you will access.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Load sample data\n",
							"\n",
							"First you will load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) data from last 6 months via Azure Open datasets.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import PublicHolidays\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"from dateutil.relativedelta import relativedelta\n",
							"\n",
							"\n",
							"end_date = datetime.today()\n",
							"start_date = datetime.today() - relativedelta(months=6)\n",
							"hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
							"hol_df = hol.to_spark_dataframe()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"display(hol_df)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write data to the Delta Lake table\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Set the strorage path info\n",
							"# Primary storage info\n",
							"account_name = '' # fill in your primary storage account name\n",
							"container_name = '' # fill in your container name\n",
							"relative_path = '' # fill in your relative folder path\n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path)\n",
							"print('Primary storage account path: ' + adls_path)\n",
							"\n",
							"# Delta Lake relative path\n",
							"delta_relative_path = adls_path + 'delta/holiday/'\n",
							"print('Delta Lake path: ' + delta_relative_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Filter out indian holidays\n",
							"hol_df_IN = hol_df[(hol_df.countryRegionCode == \"IN\")]\n",
							"hol_df_IN.show(5, truncate = False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"#Let's write the data in the Delta Lake table. \n",
							"hol_df_IN.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"holidayName\").save(delta_relative_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"source": [
							"delta_data = spark.read.format(\"delta\").load(delta_relative_path)\n",
							"delta_data.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Overwrite the entire Delta Lake table\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"#Let's overwrite the entire delta file with 1 record\n",
							"\n",
							"hol_df_JP= hol_df[(hol_df.countryRegionCode == \"JP\")]\n",
							"hol_df_JP.write.format(\"delta\").mode(\"overwrite\").save(delta_relative_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"source": [
							"delta_data = spark.read.format(\"delta\").load(delta_relative_path)\n",
							"delta_data.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Merge new data based on given merge condition "
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Upsert (merge) the United States' holiday data with Japan's\n",
							" \n",
							"from delta.tables import *\n",
							"\n",
							"deltaTable = DeltaTable.forPath(spark,delta_relative_path)\n",
							"\n",
							"hol_df_US= hol_df[(hol_df.countryRegionCode == \"US\")]\n",
							"\n",
							"\n",
							"deltaTable.alias(\"hol_df_JP\").merge(\n",
							"     source = hol_df_US.alias(\"hol_df_US\"),\n",
							"     condition = \"hol_df_JP.countryRegionCode = hol_df_US.countryRegionCode\"\n",
							"    ).whenMatchedUpdate(set = \n",
							"    {}).whenNotMatchedInsert( values = \n",
							"    {\n",
							"        \"countryOrRegion\" : \"hol_df_US.countryOrRegion\",\n",
							"        \"holidayName\" : \"hol_df_US.holidayName\",\n",
							"        \"normalizeHolidayName\" : \"hol_df_US.normalizeHolidayName\",\n",
							"        \"isPaidTimeOff\":\"hol_df_US.isPaidTimeOff\",\n",
							"        \"countryRegionCode\":\"hol_df_US.countryRegionCode\",\n",
							"        \"date\":\"hol_df_US.date\"\n",
							"    }\n",
							"    ).execute()\n",
							"\n",
							"\n",
							"deltaTable.toDF().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Update table on the rows that match the given condition\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Update column the 'null' value in 'isPaidTimeOff' with 'false'\n",
							"\n",
							"from pyspark.sql.functions import *\n",
							"deltaTable.update(\n",
							"    condition = (col(\"isPaidTimeOff\").isNull()),\n",
							"    set = {\"isPaidTimeOff\": \"false\"})\n",
							"\n",
							"deltaTable.toDF().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 71
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Delete data from the table that match the given condition\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"print(\"Row count before delete: \")\n",
							"print(deltaTable.toDF().count())\n",
							"\n",
							"\n",
							"# Delte data with date later than 2020-01-01\n",
							"deltaTable.delete (\"date > '2020-01-01'\")\n",
							"\n",
							"\n",
							"print(\"Row count after delete:  \")\n",
							"print(deltaTable.toDF().count())\n",
							"deltaTable.toDF().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 72
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Get the operation history of the delta table\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"fullHistoryDF = deltaTable.history()\n",
							"lastOperationDF = deltaTable.history(1)\n",
							"\n",
							"print('Full history DF: ')\n",
							"fullHistoryDF.show(truncate = False)\n",
							"\n",
							"print('lastOperationDF: ')\n",
							"lastOperationDF.show(truncate = False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 73
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/05 Using Azure Open Datasets in Synapse')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder/python"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4fbb21c5-5905-4ab9-95b5-be7bc82a0b20"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Using Azure Open Datasets in Synapse - Enrich NYC Green Taxi Data with Holiday and Weather\n",
							"\n",
							"Synapse has [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/) package pre-installed. This notebook provides examples of how to enrich NYC Green Taxi Data with Holiday and Weather with focusing on :\n",
							"- read Azure Open Dataset\n",
							"- manipulate the data to prepare for further analysis, including column projection, filtering, grouping and joins etc. \n",
							"- create a Spark table to be used in other notebooks for modeling training"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data loading \n",
							"Let's first load the [NYC green taxi trip records](https://azure.microsoft.com/en-us/services/open-datasets/catalog/nyc-taxi-limousine-commission-green-taxi-trip-records/). The Open Datasets package contains a class representing each data source (NycTlcGreen for example) to easily filter date parameters before downloading."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import NycTlcGreen\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"end_date = parser.parse('2018-06-06')\n",
							"start_date = parser.parse('2018-05-01')\n",
							"\n",
							"nyc_tlc = NycTlcGreen(start_date=start_date, end_date=end_date)\n",
							"nyc_tlc_df = nyc_tlc.to_spark_dataframe()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"# Display 5 rows\n",
							"\n",
							"nyc_tlc_df.show(5, truncate = False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"Now that the initial data is loaded. Let's do some projection on the data to \n",
							"- create new columns for the month number, day of month, day of week, and hour of day. These info is going to be used in the training model to factor in time-based seasonality.\n",
							"- add a static feature for the country code to join holiday data. "
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Extract month, day of month, and day of week from pickup datetime and add a static column for the country code to join holiday data. \n",
							"\n",
							"import pyspark.sql.functions as f\n",
							"\n",
							"nyc_tlc_df_expand = nyc_tlc_df.withColumn('datetime',f.to_date('lpepPickupDatetime'))\\\n",
							"                .withColumn('month_num',f.month(nyc_tlc_df.lpepPickupDatetime))\\\n",
							"                .withColumn('day_of_month',f.dayofmonth(nyc_tlc_df.lpepPickupDatetime))\\\n",
							"                .withColumn('day_of_week',f.dayofweek(nyc_tlc_df.lpepPickupDatetime))\\\n",
							"                .withColumn('hour_of_day',f.hour(nyc_tlc_df.lpepPickupDatetime))\\\n",
							"                .withColumn('country_code',f.lit('US'))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"Remove some of the columns that won't need for modeling or additional feature building.\n",
							"\n",
							"\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Remove unused columns from nyc green taxi data\n",
							"\n",
							"columns_to_remove = [\"lpepDropoffDatetime\", \"puLocationId\", \"doLocationId\", \"pickupLongitude\", \n",
							"                     \"pickupLatitude\", \"dropoffLongitude\",\"dropoffLatitude\" ,\"rateCodeID\", \n",
							"                     \"storeAndFwdFlag\",\"paymentType\", \"fareAmount\", \"extra\", \"mtaTax\",\n",
							"                     \"improvementSurcharge\", \"tollsAmount\", \"ehailFee\", \"tripType \"  \n",
							"                    ]\n",
							"\n",
							"nyc_tlc_df_clean = nyc_tlc_df_expand.select([column for column in nyc_tlc_df_expand.columns if column not in columns_to_remove])"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"# Display 5 rows\n",
							"nyc_tlc_df_clean.show(5)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Enrich with holiday data\n",
							"Now that we have taxi data downloaded and roughly prepared, add in holiday data as additional features. Holiday-specific features will assist model accuracy, as major holidays are times where taxi demand increases dramatically and supply becomes limited. \n",
							"\n",
							"Let's load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) from Azure Open datasets.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import PublicHolidays\n",
							"\n",
							"hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
							"hol_df = hol.to_spark_dataframe()\n",
							"\n",
							"# Display data\n",
							"hol_df.show(5, truncate = False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"Rename the countryRegionCode and date columns to match the respective field names from the taxi data, and also normalize the time so it can be used as a key. "
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"hol_df_clean = hol_df.withColumnRenamed('countryRegionCode','country_code')\\\n",
							"            .withColumn('datetime',f.to_date('date'))\n",
							"\n",
							"hol_df_clean.show(5)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"Next, join the holiday data with the taxi data by performing a left-join. This will preserve all records from taxi data, but add in holiday data where it exists for the corresponding datetime and country_code, which in this case is always \"US\". Preview the data to verify that they were merged correctly."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# enrich taxi data with holiday data\n",
							"nyc_taxi_holiday_df = nyc_tlc_df_clean.join(hol_df_clean, on = ['datetime', 'country_code'] , how = 'left')\n",
							"\n",
							"nyc_taxi_holiday_df.show(5)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"# Create a temp table and filter out non empty holiday rows\n",
							"\n",
							"nyc_taxi_holiday_df.createOrReplaceTempView(\"nyc_taxi_holiday_df\")\n",
							"spark.sql(\"SELECT * from nyc_taxi_holiday_df WHERE holidayName is NOT NULL \").show(5, truncate = False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Enrich with weather data\n",
							"\n",
							"Now we append NOAA surface weather data to the taxi and holiday data. Use a similar approach to fetch the [NOAA weather history data](https://azure.microsoft.com/en-us/services/open-datasets/catalog/noaa-integrated-surface-data/) from Azure Open Datasets. "
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import NoaaIsdWeather\n",
							"\n",
							"isd = NoaaIsdWeather(start_date, end_date)\n",
							"isd_df = isd.to_spark_dataframe()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"isd_df.show(5, truncate = False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							"# Filter out weather info for new york city, remove the recording with null temperature \n",
							"\n",
							"weather_df = isd_df.filter(isd_df.latitude >= '40.53')\\\n",
							"                        .filter(isd_df.latitude <= '40.88')\\\n",
							"                        .filter(isd_df.longitude >= '-74.09')\\\n",
							"                        .filter(isd_df.longitude <= '-73.72')\\\n",
							"                        .filter(isd_df.temperature.isNotNull())\\\n",
							"                        .withColumnRenamed('datetime','datetime_full')\n",
							"                         "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"# Remove unused columns\n",
							"\n",
							"columns_to_remove_weather = [\"usaf\", \"wban\", \"longitude\", \"latitude\"]\n",
							"weather_df_clean = weather_df.select([column for column in weather_df.columns if column not in columns_to_remove_weather])\\\n",
							"                        .withColumn('datetime',f.to_date('datetime_full'))\n",
							"\n",
							"weather_df_clean.show(5, truncate = False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"source": [
							"Next group the weather data so that you have daily aggregated weather values. \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Enrich weather data with aggregation statistics\n",
							"\n",
							"aggregations = {\"snowDepth\": \"mean\", \"precipTime\": \"max\", \"temperature\": \"mean\", \"precipDepth\": \"max\"}\n",
							"weather_df_grouped = weather_df_clean.groupby(\"datetime\").agg(aggregations)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"source": [
							"weather_df_grouped.show(5)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"# Rename columns\n",
							"\n",
							"weather_df_grouped = weather_df_grouped.withColumnRenamed('avg(snowDepth)','avg_snowDepth')\\\n",
							"                                       .withColumnRenamed('avg(temperature)','avg_temperature')\\\n",
							"                                       .withColumnRenamed('max(precipTime)','max_precipTime')\\\n",
							"                                       .withColumnRenamed('max(precipDepth)','max_precipDepth')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"source": [
							"Merge the taxi and holiday data you prepared with the new weather data. This time you only need the datetime key, and again perform a left-join of the data. Run the describe() function on the new dataframe to see summary statistics for each field."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# enrich taxi data with weather\n",
							"nyc_taxi_holiday_weather_df = nyc_taxi_holiday_df.join(weather_df_grouped, on = 'datetime' , how = 'left')\n",
							"nyc_taxi_holiday_weather_df.cache()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"source": [
							"nyc_taxi_holiday_weather_df.show(5)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"source": [
							"# Run the describe() function on the new dataframe to see summary statistics for each field.\n",
							"\n",
							"display(nyc_taxi_holiday_weather_df.describe())"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "markdown",
						"source": [
							"The summary statistics shows that the totalAmount field has negative values, which don't make sense in the context.\n",
							"\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Remove invalid rows with less than 0 taxi fare or tip\n",
							"final_df = nyc_taxi_holiday_weather_df.filter(nyc_taxi_holiday_weather_df.tipAmount > 0)\\\n",
							"                                      .filter(nyc_taxi_holiday_weather_df.totalAmount > 0)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Cleaning up the existing Database\n",
							"\n",
							"First we need to drop the tables since Spark requires that a database is empty before we can drop the Database.\n",
							"\n",
							"Then we recreate the database and set the default database context to it."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DROP TABLE IF EXISTS NYCTaxi.nyc_taxi_holiday_weather\"); "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DROP DATABASE IF EXISTS NYCTaxi\"); \n",
							"spark.sql(\"CREATE DATABASE NYCTaxi\"); \n",
							"spark.sql(\"USE NYCTaxi\");"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Creating a new table\n",
							"We create a nyc_taxi_holiday_weather table from the nyc_taxi_holiday_weather dataframe.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.types import *\n",
							"\n",
							"final_df.write.saveAsTable(\"nyc_taxi_holiday_weather\");\n",
							"spark.sql(\"SELECT COUNT(*) FROM nyc_taxi_holiday_weather\").show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 25
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/06 Charting in Synapse Notebook')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder/python"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b8b50a4f-630f-4a5e-9146-96f996488397"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Charting in Synapse Notebook\n",
							"\n",
							"Synapse has common used data visualization packages pre installed, such as **matplotlib**, **bokeh**, **seaborn**, **altair**, **plotly**. This notebook provides examples to do data visualization using charts in Synapse notebook. \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Matplotlib\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Line charts\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							" \n",
							"x  = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
							"y1 = [1, 3, 5, 3, 1, 3, 5, 3, 1]\n",
							"y2 = [2, 4, 6, 4, 2, 4, 6, 4, 2]\n",
							"plt.plot(x, y1, label=\"line L\")\n",
							"plt.plot(x, y2, label=\"line H\")\n",
							"plt.plot()\n",
							"\n",
							"plt.xlabel(\"x axis\")\n",
							"plt.ylabel(\"y axis\")\n",
							"plt.title(\"Line Graph Example\")\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"# Bar chart\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"# Look at index 4 and 6, which demonstrate overlapping cases.\n",
							"x1 = [1, 3, 4, 5, 6, 7, 9]\n",
							"y1 = [4, 7, 2, 4, 7, 8, 3]\n",
							"\n",
							"x2 = [2, 4, 6, 8, 10]\n",
							"y2 = [5, 6, 2, 6, 2]\n",
							"\n",
							"# Colors: https://matplotlib.org/api/colors_api.html\n",
							"\n",
							"plt.bar(x1, y1, label=\"Blue Bar\", color='b')\n",
							"plt.bar(x2, y2, label=\"Green Bar\", color='g')\n",
							"plt.plot()\n",
							"\n",
							"plt.xlabel(\"bar number\")\n",
							"plt.ylabel(\"bar height\")\n",
							"plt.title(\"Bar Chart Example\")\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"# Histogram\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"# Use numpy to generate a bunch of random data in a bell curve around 5.\n",
							"n = 5 + np.random.randn(1000)\n",
							"\n",
							"m = [m for m in range(len(n))]\n",
							"plt.bar(m, n)\n",
							"plt.title(\"Raw Data\")\n",
							"plt.show()\n",
							"\n",
							"plt.hist(n, bins=20)\n",
							"plt.title(\"Histogram\")\n",
							"plt.show()\n",
							"\n",
							"plt.hist(n, cumulative=True, bins=20)\n",
							"plt.title(\"Cumulative Histogram\")\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"# Scatter chart\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"x1 = [2, 3, 4]\n",
							"y1 = [5, 5, 5]\n",
							"\n",
							"x2 = [1, 2, 3, 4, 5]\n",
							"y2 = [2, 3, 2, 3, 4]\n",
							"y3 = [6, 8, 7, 8, 7]\n",
							"\n",
							"# Markers: https://matplotlib.org/api/markers_api.html\n",
							"\n",
							"plt.scatter(x1, y1)\n",
							"plt.scatter(x2, y2, marker='v', color='r')\n",
							"plt.scatter(x2, y3, marker='^', color='m')\n",
							"plt.title('Scatter Plot Example')\n",
							"plt.show()\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"# Stack plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"idxes = [ 1,  2,  3,  4,  5,  6,  7,  8,  9]\n",
							"arr1  = [23, 40, 28, 43,  8, 44, 43, 18, 17]\n",
							"arr2  = [17, 30, 22, 14, 17, 17, 29, 22, 30]\n",
							"arr3  = [15, 31, 18, 22, 18, 19, 13, 32, 39]\n",
							"\n",
							"# Adding legend for stack plots is tricky.\n",
							"plt.plot([], [], color='r', label = 'D 1')\n",
							"plt.plot([], [], color='g', label = 'D 2')\n",
							"plt.plot([], [], color='b', label = 'D 3')\n",
							"\n",
							"plt.stackplot(idxes, arr1, arr2, arr3, colors= ['r', 'g', 'b'])\n",
							"plt.title('Stack Plot Example')\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"# Pie charts\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"labels = 'S1', 'S2', 'S3'\n",
							"sections = [56, 66, 24]\n",
							"colors = ['c', 'g', 'y']\n",
							"\n",
							"plt.pie(sections, labels=labels, colors=colors,\n",
							"        startangle=90,\n",
							"        explode = (0, 0.1, 0),\n",
							"        autopct = '%1.2f%%')\n",
							"\n",
							"plt.axis('equal') # Try commenting this out.\n",
							"plt.title('Pie Chart Example')\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							"# fill_between and alpha\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"ys = 200 + np.random.randn(100)\n",
							"x = [x for x in range(len(ys))]\n",
							"\n",
							"plt.plot(x, ys, '-')\n",
							"plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)\n",
							"\n",
							"plt.title(\"Fills and Alpha Example\")\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"# Subplotting using Subplot2grid\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"def random_plots():\n",
							"  xs = []\n",
							"  ys = []\n",
							"  \n",
							"  for i in range(20):\n",
							"    x = i\n",
							"    y = np.random.randint(10)\n",
							"    \n",
							"    xs.append(x)\n",
							"    ys.append(y)\n",
							"  \n",
							"  return xs, ys\n",
							"\n",
							"fig = plt.figure()\n",
							"ax1 = plt.subplot2grid((5, 2), (0, 0), rowspan=1, colspan=2)\n",
							"ax2 = plt.subplot2grid((5, 2), (1, 0), rowspan=3, colspan=2)\n",
							"ax3 = plt.subplot2grid((5, 2), (4, 0), rowspan=1, colspan=1)\n",
							"ax4 = plt.subplot2grid((5, 2), (4, 1), rowspan=1, colspan=1)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax1.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax2.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax3.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax4.plot(x, y)\n",
							"\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"# 3D Scatter Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"from mpl_toolkits.mplot3d import axes3d\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"y1 = np.random.randint(10, size=10)\n",
							"z1 = np.random.randint(10, size=10)\n",
							"\n",
							"x2 = [-1, -2, -3, -4, -5, -6, -7, -8, -9, -10]\n",
							"y2 = np.random.randint(-10, 0, size=10)\n",
							"z2 = np.random.randint(10, size=10)\n",
							"\n",
							"ax.scatter(x1, y1, z1, c='b', marker='o', label='blue')\n",
							"ax.scatter(x2, y2, z2, c='g', marker='D', label='green')\n",
							"\n",
							"ax.set_xlabel('x axis')\n",
							"ax.set_ylabel('y axis')\n",
							"ax.set_zlabel('z axis')\n",
							"plt.title(\"3D Scatter Plot Example\")\n",
							"plt.legend()\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							"# 3D Bar Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"y = np.random.randint(10, size=10)\n",
							"z = np.zeros(10)\n",
							"\n",
							"dx = np.ones(10)\n",
							"dy = np.ones(10)\n",
							"dz = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"\n",
							"ax.bar3d(x, y, z, dx, dy, dz, color='g')\n",
							"\n",
							"ax.set_xlabel('x axis')\n",
							"ax.set_ylabel('y axis')\n",
							"ax.set_zlabel('z axis')\n",
							"plt.title(\"3D Bar Chart Example\")\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"# Wireframe Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x, y, z = axes3d.get_test_data()\n",
							"\n",
							"ax.plot_wireframe(x, y, z, rstride = 2, cstride = 2)\n",
							"\n",
							"plt.title(\"Wireframe Plot Example\")\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Seaborn\n",
							"Seaborn is a library layered on top of Matplotlib that you can use."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Scatterplot with a nice regression line fit to it, all with just one call to Seaborn's regplot.\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"import seaborn as sns\n",
							"\n",
							"# Generate some random data\n",
							"num_points = 20\n",
							"# x will be 5, 6, 7... but also twiddled randomly\n",
							"x = 5 + np.arange(num_points) + np.random.randn(num_points)\n",
							"# y will be 10, 11, 12... but twiddled even more randomly\n",
							"y = 10 + np.arange(num_points) + 5 * np.random.randn(num_points)\n",
							"sns.regplot(x, y)\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"# Seanborn heatmap\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"# Make a 10 x 10 heatmap of some random data\n",
							"side_length = 10\n",
							"# Start with a 10 x 10 matrix with values randomized around 5\n",
							"data = 5 + np.random.randn(side_length, side_length)\n",
							"# The next two lines make the values larger as we get closer to (9, 9)\n",
							"data += np.arange(side_length)\n",
							"data += np.reshape(np.arange(side_length), (side_length, 1))\n",
							"# Generate the heatmap\n",
							"sns.heatmap(data)\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Bokeh\n",
							"You can render HTML or interactive libraries, like **bokeh**, using the **displayHTML()**.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"import numpy as np\n",
							"from bokeh.plotting import figure, show\n",
							"from bokeh.io import output_notebook\n",
							"from bokeh.embed import file_html\n",
							"from bokeh.resources import CDN\n",
							"\n",
							"N = 4000\n",
							"x = np.random.random(size=N) * 100\n",
							"y = np.random.random(size=N) * 100\n",
							"radii = np.random.random(size=N) * 1.5\n",
							"colors = [\"#%02x%02x%02x\" % (r, g, 150) for r, g in zip(np.floor(50+2*x).astype(int), np.floor(30+2*y).astype(int))]\n",
							"\n",
							"p = figure()\n",
							"p.circle(x, y, radius=radii, fill_color=colors, fill_alpha=0.6, line_color=None)\n",
							"show(p)\n",
							"\n",
							"# create an html document that embeds the Bokeh plot\n",
							"html = file_html(p, CDN, \"my plot1\")\n",
							"\n",
							"# display this html\n",
							"displayHTML(html)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"source": [
							"# Plotting glyphs over a map using bokeh.\n",
							"\n",
							"from bokeh.plotting import figure, output_file\n",
							"from bokeh.tile_providers import get_provider, Vendors\n",
							"from bokeh.embed import file_html\n",
							"from bokeh.resources import CDN\n",
							"from bokeh.models import ColumnDataSource\n",
							"\n",
							"tile_provider = get_provider(Vendors.CARTODBPOSITRON)\n",
							"\n",
							"# range bounds supplied in web mercator coordinates\n",
							"p = figure(x_range=(-9000000,-8000000), y_range=(4000000,5000000),\n",
							"           x_axis_type=\"mercator\", y_axis_type=\"mercator\")\n",
							"p.add_tile(tile_provider)\n",
							"\n",
							"# plot datapoints on the map\n",
							"source = ColumnDataSource(\n",
							"    data=dict(x=[ -8800000, -8500000 , -8800000],\n",
							"              y=[4200000, 4500000, 4900000])\n",
							")\n",
							"\n",
							"p.circle(x=\"x\", y=\"y\", size=15, fill_color=\"blue\", fill_alpha=0.8, source=source)\n",
							"\n",
							"# create an html document that embeds the Bokeh plot\n",
							"html = file_html(p, CDN, \"my plot1\")\n",
							"\n",
							"# display this html\n",
							"displayHTML(html)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 27
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/07 Data Exploration and ML Modeling - NYC taxi predict using Spark MLlib')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder/python"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3767c52f-5ca1-45e3-97f6-eda2d28d2009"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Predict NYC Taxi Tips using Spark ML and Azure Open Datasets\n",
							"\n",
							"The notebook ingests, visualizes, prepares and then trains a model based on an Open Dataset that tracks NYC Yellow Taxi trips and various attributes around them.\n",
							"The goal is to predict for a given trip whether there will be a trip or not.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"import matplotlib.pyplot as plt\n",
							"\n",
							"from pyspark.sql.functions import unix_timestamp\n",
							"\n",
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.types import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"from pyspark.ml import Pipeline\n",
							"from pyspark.ml import PipelineModel\n",
							"from pyspark.ml.feature import RFormula\n",
							"from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer\n",
							"from pyspark.ml.classification import LogisticRegression\n",
							"from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
							"from pyspark.ml.evaluation import BinaryClassificationEvaluator"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Ingest Data \n",
							"\n",
							"Get a sample data of nyc yellow taxi to make it faster/easier to evaluate different approaches to prep for the modelling phase later in the notebook."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Import NYC yellow cab data from Azure Open Datasets\n",
							"from azureml.opendatasets import NycTlcYellow\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"\n",
							"end_date = parser.parse('2018-05-08 00:00:00')\n",
							"start_date = parser.parse('2018-05-01 00:00:00')\n",
							"\n",
							"nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
							"nyc_tlc_df = nyc_tlc.to_spark_dataframe()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"#To make development easier, faster and less expensive downsample for now\n",
							"sampled_taxi_df = nyc_tlc_df.sample(True, 0.001, seed=1234)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Exploratory Data Analysis\n",
							"\n",
							"Look at the data and evaluate its suitability for use in a model, do this via some basic charts focussed on tip values and relationships."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"#The charting package needs a Pandas dataframe or numpy array do the conversion\n",
							"sampled_taxi_pd_df = sampled_taxi_df.toPandas()\n",
							"\n",
							"# Look at tips by amount count histogram\n",
							"ax1 = sampled_taxi_pd_df['tipAmount'].plot(kind='hist', bins=25, facecolor='lightblue')\n",
							"ax1.set_title('Tip amount distribution')\n",
							"ax1.set_xlabel('Tip Amount ($)')\n",
							"ax1.set_ylabel('Counts')\n",
							"plt.suptitle('')\n",
							"plt.show()\n",
							"\n",
							"# How many passengers tip'd by various amounts\n",
							"ax2 = sampled_taxi_pd_df.boxplot(column=['tipAmount'], by=['passengerCount'])\n",
							"ax2.set_title('Tip amount by Passenger count')\n",
							"ax2.set_xlabel('Passenger count') \n",
							"ax2.set_ylabel('Tip Amount ($)')\n",
							"plt.suptitle('')\n",
							"plt.show()\n",
							"\n",
							"# Look at the relationship between fare and tip amounts\n",
							"ax = sampled_taxi_pd_df.plot(kind='scatter', x= 'fareAmount', y = 'tipAmount', c='blue', alpha = 0.10, s=2.5*(sampled_taxi_pd_df['passengerCount']))\n",
							"ax.set_title('Tip amount by Fare amount')\n",
							"ax.set_xlabel('Fare Amount ($)')\n",
							"ax.set_ylabel('Tip Amount ($)')\n",
							"plt.axis([-2, 80, -2, 20])\n",
							"plt.suptitle('')\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data Prep and Featurization\n",
							"\n",
							"It's clear from the visualizations above that there are a bunch of outliers in the data. These will need to be filtered out in addition there are extra variables that are not going to be useful in the model we build at the end.\n",
							"\n",
							"Finally there is a need to create some new (derived) variables that will work better with the model.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"taxi_df = sampled_taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'rateCodeId', 'passengerCount'\\\n",
							"                                , 'tripDistance', 'tpepPickupDateTime', 'tpepDropoffDateTime'\\\n",
							"                                , date_format('tpepPickupDateTime', 'hh').alias('pickupHour')\\\n",
							"                                , date_format('tpepPickupDateTime', 'EEEE').alias('weekdayString')\\\n",
							"                                , (unix_timestamp(col('tpepDropoffDateTime')) - unix_timestamp(col('tpepPickupDateTime'))).alias('tripTimeSecs')\\\n",
							"                                , (when(col('tipAmount') > 0, 1).otherwise(0)).alias('tipped')\n",
							"                                )\\\n",
							"                        .filter((sampled_taxi_df.passengerCount > 0) & (sampled_taxi_df.passengerCount < 8)\\\n",
							"                                & (sampled_taxi_df.tipAmount >= 0) & (sampled_taxi_df.tipAmount <= 25)\\\n",
							"                                & (sampled_taxi_df.fareAmount >= 1) & (sampled_taxi_df.fareAmount <= 250)\\\n",
							"                                & (sampled_taxi_df.tipAmount < sampled_taxi_df.fareAmount)\\\n",
							"                                & (sampled_taxi_df.tripDistance > 0) & (sampled_taxi_df.tripDistance <= 100)\\\n",
							"                                & (sampled_taxi_df.rateCodeId <= 5)\n",
							"                                & (sampled_taxi_df.paymentType.isin({\"1\", \"2\"}))\n",
							"                                )"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data Prep and Featurization Part 2\n",
							"\n",
							"Having created new variables its now possible to drop the columns they were derived from so that the dataframe that goes into the model is the smallest in terms of number of variables, that is required.\n",
							"\n",
							"Also create some more features based on new columns from the first round.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"taxi_featurised_df = taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'passengerCount'\\\n",
							"                                                , 'tripDistance', 'weekdayString', 'pickupHour','tripTimeSecs','tipped'\\\n",
							"                                                , when((taxi_df.pickupHour <= 6) | (taxi_df.pickupHour >= 20),\"Night\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 7) & (taxi_df.pickupHour <= 10), \"AMRush\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 11) & (taxi_df.pickupHour <= 15), \"Afternoon\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 16) & (taxi_df.pickupHour <= 19), \"PMRush\")\\\n",
							"                                                .otherwise(0).alias('trafficTimeBins')\n",
							"                                              )\\\n",
							"                                       .filter((taxi_df.tripTimeSecs >= 30) & (taxi_df.tripTimeSecs <= 7200))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Encoding\n",
							"\n",
							"Different ML algorithms support different types of input, for this example Logistic Regression is being used for Binary Classification. This means that any Categorical (string) variables must be converted to numbers.\n",
							"\n",
							"The process is not as simple as a \"map\" style function as the relationship between the numbers can introduce a bias in the resulting model, the approach is to index the variable and then encode using a std approach called One Hot Encoding.\n",
							"\n",
							"This approach requires the encoder to \"learn\"/fit a model over the data in the Spark instance and then transform based on what was learnt.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# The sample uses an algorithm that only works with numeric features convert them so they can be consumed\n",
							"sI1 = StringIndexer(inputCol=\"trafficTimeBins\", outputCol=\"trafficTimeBinsIndex\"); \n",
							"en1 = OneHotEncoder(dropLast=False, inputCol=\"trafficTimeBinsIndex\", outputCol=\"trafficTimeBinsVec\");\n",
							"sI2 = StringIndexer(inputCol=\"weekdayString\", outputCol=\"weekdayIndex\"); \n",
							"en2 = OneHotEncoder(dropLast=False, inputCol=\"weekdayIndex\", outputCol=\"weekdayVec\");\n",
							"\n",
							"# Create a new dataframe that has had the encodings applied\n",
							"encoded_final_df = Pipeline(stages=[sI1, en1, sI2, en2]).fit(taxi_featurised_df).transform(taxi_featurised_df)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Generation of Testing and Training Data Sets\n",
							"Simple split, 70% for training and 30% for testing the model. Playing with this ratio may result in different models.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Decide on the split between training and testing data from the dataframe \n",
							"trainingFraction = 0.7\n",
							"testingFraction = (1-trainingFraction)\n",
							"seed = 1234\n",
							"\n",
							"# Split the dataframe into test and training dataframes\n",
							"train_data_df, test_data_df = encoded_final_df.randomSplit([trainingFraction, testingFraction], seed=seed)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Train the Model\n",
							"\n",
							"Train the Logistic Regression model and then evaluate it using Area under ROC as the metric."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"## Create a new LR object for the model\n",
							"logReg = LogisticRegression(maxIter=10, regParam=0.3, labelCol = 'tipped')\n",
							"\n",
							"## The formula for the model\n",
							"classFormula = RFormula(formula=\"tipped ~ pickupHour + weekdayVec + passengerCount + tripTimeSecs + tripDistance + fareAmount + paymentType+ trafficTimeBinsVec\")\n",
							"\n",
							"## Undertake training and create an LR model\n",
							"lrModel = Pipeline(stages=[classFormula, logReg]).fit(train_data_df)\n",
							"\n",
							"## Saving the model is optional but its another for of inter session cache\n",
							"datestamp = datetime.now().strftime('%m-%d-%Y-%s');\n",
							"fileName = \"lrModel_\" + datestamp;\n",
							"logRegDirfilename = fileName;\n",
							"lrModel.save(logRegDirfilename)\n",
							"\n",
							"## Predict tip 1/0 (yes/no) on the test dataset, evaluation using AUROC\n",
							"predictions = lrModel.transform(test_data_df)\n",
							"predictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\n",
							"metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
							"print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Evaluate and Visualize\n",
							"\n",
							"Plot the actual curve to develop a better understanding of the model.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"## Plot the ROC curve, no need for pandas as this uses the modelSummary object\n",
							"modelSummary = lrModel.stages[-1].summary\n",
							"\n",
							"plt.plot([0, 1], [0, 1], 'r--')\n",
							"plt.plot(modelSummary.roc.select('FPR').collect(),\n",
							"         modelSummary.roc.select('TPR').collect())\n",
							"plt.xlabel('False Positive Rate')\n",
							"plt.ylabel('True Positive Rate')\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/08 Creating an unmanaged Spark Table')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder/python"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a6c3bad4-c31b-4a56-901a-0283462734c0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Creating an unmanaged (external) Spark table\n",
							"This notebook describes how to create an unmanaged (also known as external) table from Spark. \n",
							"The table is created in /datalake/cities which may exist already (so you can attach to existing data) it can be created when you insert data."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"CREATE TABLE cities (name STRING, population INT) USING PARQUET  LOCATION \\'/datalake/cities\\' OPTIONS (\\'compression\\'=\\'snappy\\')\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"Insert a few rows into the table using a list of values.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"INSERT INTO cities VALUES (\\'Seattle\\', 730400), (\\'San Francisco\\', 881549), (\\'Beijing\\', 21540000), (\\'Bangalore\\', 10540000)\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"* Retrieve values back. Click on 'Chart' below to review the visualization.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [
										"name"
									],
									"values": [
										"population"
									],
									"yLabel": "population",
									"xLabel": "name",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{\"population\":{\"Bangalore\":21080000,\"Beijing\":43080000,\"San Francisco\":1763098,\"Seattle\":1460800}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": false
							}
						},
						"source": [
							"display(spark.sql(\"SELECT * FROM cities ORDER BY name\"))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"Drop the table. Please note the data will remain in the data lake.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DROP TABLE cities\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/09 Creating a managed Spark Table')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder/python"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f16e8f0f-ab23-481e-befd-bf5149627ace"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Creating a managed Spark table\n",
							"This notebook describes how to create a managed table from Spark. \n",
							"The table is created in the Synapse warehouse folder in your primary storage account. The table will be synchronized and available in Synapse SQL Pools. \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"CREATE TABLE cities  (name STRING, population INT) USING PARQUET\")\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"Insert a few rows into the table using a list of values.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"INSERT INTO cities VALUES (\\'Seattle\\', 730400), (\\'San Francisco\\', 881549), (\\'Beijing\\', 21540000), (\\'Bangalore\\', 10540000)\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"* Retrieve values back. Click on 'Chart' below to review the visualization.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [
										"name"
									],
									"values": [
										"population"
									],
									"yLabel": "population",
									"xLabel": "name",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{\"population\":{\"Bangalore\":10540000,\"Beijing\":21540000,\"San Francisco\":881549,\"Seattle\":730400}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": false
							}
						},
						"source": [
							"display(spark.sql(\"SELECT * FROM cities ORDER BY name\"))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"Drop the table. Please note the data will get deleted from the primary storage account associated with this workspace.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DROP TABLE cities\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/12create new notebook')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Charting in Synapse Note')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "dca13433-2497-4640-b304-8f0989bb40e5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Charting in Synapse Notebook\n",
							"\n",
							"Synapse has common used data visualization packages pre installed, such as **matplotlib**, **bokeh**, **seaborn**, **altair**, **plotly**. This notebook provides examples to do data visualization using charts in Synapse notebook. \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Matplotlib\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Line charts\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							" \n",
							"x  = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
							"y1 = [1, 3, 5, 3, 1, 3, 5, 3, 1]\n",
							"y2 = [2, 4, 6, 4, 2, 4, 6, 4, 2]\n",
							"plt.plot(x, y1, label=\"line L\")\n",
							"plt.plot(x, y2, label=\"line H\")\n",
							"plt.plot()\n",
							"\n",
							"plt.xlabel(\"x axis\")\n",
							"plt.ylabel(\"y axis\")\n",
							"plt.title(\"Line Graph Example\")\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"tags": []
						},
						"source": [
							"# Bar chart\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"# Look at index 4 and 6, which demonstrate overlapping cases.\n",
							"x1 = [1, 3, 4, 5, 6, 7, 9]\n",
							"y1 = [4, 7, 2, 4, 7, 8, 3]\n",
							"\n",
							"x2 = [2, 4, 6, 8, 10]\n",
							"y2 = [5, 6, 2, 6, 2]\n",
							"\n",
							"# Colors: https://matplotlib.org/api/colors_api.html\n",
							"\n",
							"plt.bar(x1, y1, label=\"Blue Bar\", color='b')\n",
							"plt.bar(x2, y2, label=\"Green Bar\", color='g')\n",
							"plt.plot()\n",
							"\n",
							"plt.xlabel(\"bar number\")\n",
							"plt.ylabel(\"bar height\")\n",
							"plt.title(\"Bar Chart Example\")\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"# Histogram\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"# Use numpy to generate a bunch of random data in a bell curve around 5.\n",
							"n = 5 + np.random.randn(1000)\n",
							"\n",
							"m = [m for m in range(len(n))]\n",
							"plt.bar(m, n)\n",
							"plt.title(\"Raw Data\")\n",
							"plt.show()\n",
							"\n",
							"plt.hist(n, bins=20)\n",
							"plt.title(\"Histogram\")\n",
							"plt.show()\n",
							"\n",
							"plt.hist(n, cumulative=True, bins=20)\n",
							"plt.title(\"Cumulative Histogram\")\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"# Scatter chart\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"x1 = [2, 3, 4]\n",
							"y1 = [5, 5, 5]\n",
							"\n",
							"x2 = [1, 2, 3, 4, 5]\n",
							"y2 = [2, 3, 2, 3, 4]\n",
							"y3 = [6, 8, 7, 8, 7]\n",
							"\n",
							"# Markers: https://matplotlib.org/api/markers_api.html\n",
							"\n",
							"plt.scatter(x1, y1)\n",
							"plt.scatter(x2, y2, marker='v', color='r')\n",
							"plt.scatter(x2, y3, marker='^', color='m')\n",
							"plt.title('Scatter Plot Example')\n",
							"plt.show()\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"# Stack plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"idxes = [ 1,  2,  3,  4,  5,  6,  7,  8,  9]\n",
							"arr1  = [23, 40, 28, 43,  8, 44, 43, 18, 17]\n",
							"arr2  = [17, 30, 22, 14, 17, 17, 29, 22, 30]\n",
							"arr3  = [15, 31, 18, 22, 18, 19, 13, 32, 39]\n",
							"\n",
							"# Adding legend for stack plots is tricky.\n",
							"plt.plot([], [], color='r', label = 'D 1')\n",
							"plt.plot([], [], color='g', label = 'D 2')\n",
							"plt.plot([], [], color='b', label = 'D 3')\n",
							"\n",
							"plt.stackplot(idxes, arr1, arr2, arr3, colors= ['r', 'g', 'b'])\n",
							"plt.title('Stack Plot Example')\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"# Pie charts\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"labels = 'S1', 'S2', 'S3'\n",
							"sections = [56, 66, 24]\n",
							"colors = ['c', 'g', 'y']\n",
							"\n",
							"plt.pie(sections, labels=labels, colors=colors,\n",
							"        startangle=90,\n",
							"        explode = (0, 0.1, 0),\n",
							"        autopct = '%1.2f%%')\n",
							"\n",
							"plt.axis('equal') # Try commenting this out.\n",
							"plt.title('Pie Chart Example')\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"# fill_between and alpha\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"ys = 200 + np.random.randn(100)\n",
							"x = [x for x in range(len(ys))]\n",
							"\n",
							"plt.plot(x, ys, '-')\n",
							"plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)\n",
							"\n",
							"plt.title(\"Fills and Alpha Example\")\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"# Subplotting using Subplot2grid\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"def random_plots():\n",
							"  xs = []\n",
							"  ys = []\n",
							"  \n",
							"  for i in range(20):\n",
							"    x = i\n",
							"    y = np.random.randint(10)\n",
							"    \n",
							"    xs.append(x)\n",
							"    ys.append(y)\n",
							"  \n",
							"  return xs, ys\n",
							"\n",
							"fig = plt.figure()\n",
							"ax1 = plt.subplot2grid((5, 2), (0, 0), rowspan=1, colspan=2)\n",
							"ax2 = plt.subplot2grid((5, 2), (1, 0), rowspan=3, colspan=2)\n",
							"ax3 = plt.subplot2grid((5, 2), (4, 0), rowspan=1, colspan=1)\n",
							"ax4 = plt.subplot2grid((5, 2), (4, 1), rowspan=1, colspan=1)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax1.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax2.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax3.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax4.plot(x, y)\n",
							"\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							"# 3D Scatter Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"from mpl_toolkits.mplot3d import axes3d\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"y1 = np.random.randint(10, size=10)\n",
							"z1 = np.random.randint(10, size=10)\n",
							"\n",
							"x2 = [-1, -2, -3, -4, -5, -6, -7, -8, -9, -10]\n",
							"y2 = np.random.randint(-10, 0, size=10)\n",
							"z2 = np.random.randint(10, size=10)\n",
							"\n",
							"ax.scatter(x1, y1, z1, c='b', marker='o', label='blue')\n",
							"ax.scatter(x2, y2, z2, c='g', marker='D', label='green')\n",
							"\n",
							"ax.set_xlabel('x axis')\n",
							"ax.set_ylabel('y axis')\n",
							"ax.set_zlabel('z axis')\n",
							"plt.title(\"3D Scatter Plot Example\")\n",
							"plt.legend()\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"# 3D Bar Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"y = np.random.randint(10, size=10)\n",
							"z = np.zeros(10)\n",
							"\n",
							"dx = np.ones(10)\n",
							"dy = np.ones(10)\n",
							"dz = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"\n",
							"ax.bar3d(x, y, z, dx, dy, dz, color='g')\n",
							"\n",
							"ax.set_xlabel('x axis')\n",
							"ax.set_ylabel('y axis')\n",
							"ax.set_zlabel('z axis')\n",
							"plt.title(\"3D Bar Chart Example\")\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"# Wireframe Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x, y, z = axes3d.get_test_data()\n",
							"\n",
							"ax.plot_wireframe(x, y, z, rstride = 2, cstride = 2)\n",
							"\n",
							"plt.title(\"Wireframe Plot Example\")\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Seaborn\n",
							"Seaborn is a library layered on top of Matplotlib that you can use."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Scatterplot with a nice regression line fit to it, all with just one call to Seaborn's regplot.\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"import seaborn as sns\n",
							"\n",
							"# Generate some random data\n",
							"num_points = 20\n",
							"# x will be 5, 6, 7... but also twiddled randomly\n",
							"x = 5 + np.arange(num_points) + np.random.randn(num_points)\n",
							"# y will be 10, 11, 12... but twiddled even more randomly\n",
							"y = 10 + np.arange(num_points) + 5 * np.random.randn(num_points)\n",
							"sns.regplot(x, y)\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							"# Seanborn heatmap\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"# Make a 10 x 10 heatmap of some random data\n",
							"side_length = 10\n",
							"# Start with a 10 x 10 matrix with values randomized around 5\n",
							"data = 5 + np.random.randn(side_length, side_length)\n",
							"# The next two lines make the values larger as we get closer to (9, 9)\n",
							"data += np.arange(side_length)\n",
							"data += np.reshape(np.arange(side_length), (side_length, 1))\n",
							"# Generate the heatmap\n",
							"sns.heatmap(data)\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Bokeh\n",
							"You can render HTML or interactive libraries, like **bokeh**, using the **displayHTML()**.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"import numpy as np\n",
							"from bokeh.plotting import figure, show\n",
							"from bokeh.io import output_notebook\n",
							"from bokeh.embed import file_html\n",
							"from bokeh.resources import CDN\n",
							"\n",
							"N = 4000\n",
							"x = np.random.random(size=N) * 100\n",
							"y = np.random.random(size=N) * 100\n",
							"radii = np.random.random(size=N) * 1.5\n",
							"colors = [\"#%02x%02x%02x\" % (r, g, 150) for r, g in zip(np.floor(50+2*x).astype(int), np.floor(30+2*y).astype(int))]\n",
							"\n",
							"p = figure()\n",
							"p.circle(x, y, radius=radii, fill_color=colors, fill_alpha=0.6, line_color=None)\n",
							"show(p)\n",
							"\n",
							"# create an html document that embeds the Bokeh plot\n",
							"html = file_html(p, CDN, \"my plot1\")\n",
							"\n",
							"# display this html\n",
							"displayHTML(html)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"# Plotting glyphs over a map using bokeh.\n",
							"\n",
							"from bokeh.plotting import figure, output_file\n",
							"from bokeh.tile_providers import get_provider, Vendors\n",
							"from bokeh.embed import file_html\n",
							"from bokeh.resources import CDN\n",
							"from bokeh.models import ColumnDataSource\n",
							"\n",
							"tile_provider = get_provider(Vendors.CARTODBPOSITRON)\n",
							"\n",
							"# range bounds supplied in web mercator coordinates\n",
							"p = figure(x_range=(-9000000,-8000000), y_range=(4000000,5000000),\n",
							"           x_axis_type=\"mercator\", y_axis_type=\"mercator\")\n",
							"p.add_tile(tile_provider)\n",
							"\n",
							"# plot datapoints on the map\n",
							"source = ColumnDataSource(\n",
							"    data=dict(x=[ -8800000, -8500000 , -8800000],\n",
							"              y=[4200000, 4500000, 4900000])\n",
							")\n",
							"\n",
							"p.circle(x=\"x\", y=\"y\", size=15, fill_color=\"blue\", fill_alpha=0.8, source=source)\n",
							"\n",
							"# create an html document that embeds the Bokeh plot\n",
							"html = file_html(p, CDN, \"my plot1\")\n",
							"\n",
							"# display this html\n",
							"displayHTML(html)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Charting in Synapse Notebook')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "dc4f1644-876a-4a52-a450-6ebfa80f6219"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Charting in Synapse Notebook\n",
							"\n",
							"Synapse has common used data visualization packages pre installed, such as **matplotlib**, **bokeh**, **seaborn**, **altair**, **plotly**. This notebook provides examples to do data visualization using charts in Synapse notebook. \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Matplotlib\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Line charts\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							" \n",
							"x  = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
							"y1 = [1, 3, 5, 3, 1, 3, 5, 3, 1]\n",
							"y2 = [2, 4, 6, 4, 2, 4, 6, 4, 2]\n",
							"plt.plot(x, y1, label=\"line L\")\n",
							"plt.plot(x, y2, label=\"line H\")\n",
							"plt.plot()\n",
							"\n",
							"plt.xlabel(\"x axis\")\n",
							"plt.ylabel(\"y axis\")\n",
							"plt.title(\"Line Graph Example\")\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"# Bar chart\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"# Look at index 4 and 6, which demonstrate overlapping cases.\n",
							"x1 = [1, 3, 4, 5, 6, 7, 9]\n",
							"y1 = [4, 7, 2, 4, 7, 8, 3]\n",
							"\n",
							"x2 = [2, 4, 6, 8, 10]\n",
							"y2 = [5, 6, 2, 6, 2]\n",
							"\n",
							"# Colors: https://matplotlib.org/api/colors_api.html\n",
							"\n",
							"plt.bar(x1, y1, label=\"Blue Bar\", color='b')\n",
							"plt.bar(x2, y2, label=\"Green Bar\", color='g')\n",
							"plt.plot()\n",
							"\n",
							"plt.xlabel(\"bar number\")\n",
							"plt.ylabel(\"bar height\")\n",
							"plt.title(\"Bar Chart Example\")\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"# Histogram\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"# Use numpy to generate a bunch of random data in a bell curve around 5.\n",
							"n = 5 + np.random.randn(1000)\n",
							"\n",
							"m = [m for m in range(len(n))]\n",
							"plt.bar(m, n)\n",
							"plt.title(\"Raw Data\")\n",
							"plt.show()\n",
							"\n",
							"plt.hist(n, bins=20)\n",
							"plt.title(\"Histogram\")\n",
							"plt.show()\n",
							"\n",
							"plt.hist(n, cumulative=True, bins=20)\n",
							"plt.title(\"Cumulative Histogram\")\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"# Scatter chart\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"x1 = [2, 3, 4]\n",
							"y1 = [5, 5, 5]\n",
							"\n",
							"x2 = [1, 2, 3, 4, 5]\n",
							"y2 = [2, 3, 2, 3, 4]\n",
							"y3 = [6, 8, 7, 8, 7]\n",
							"\n",
							"# Markers: https://matplotlib.org/api/markers_api.html\n",
							"\n",
							"plt.scatter(x1, y1)\n",
							"plt.scatter(x2, y2, marker='v', color='r')\n",
							"plt.scatter(x2, y3, marker='^', color='m')\n",
							"plt.title('Scatter Plot Example')\n",
							"plt.show()\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"# Stack plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"idxes = [ 1,  2,  3,  4,  5,  6,  7,  8,  9]\n",
							"arr1  = [23, 40, 28, 43,  8, 44, 43, 18, 17]\n",
							"arr2  = [17, 30, 22, 14, 17, 17, 29, 22, 30]\n",
							"arr3  = [15, 31, 18, 22, 18, 19, 13, 32, 39]\n",
							"\n",
							"# Adding legend for stack plots is tricky.\n",
							"plt.plot([], [], color='r', label = 'D 1')\n",
							"plt.plot([], [], color='g', label = 'D 2')\n",
							"plt.plot([], [], color='b', label = 'D 3')\n",
							"\n",
							"plt.stackplot(idxes, arr1, arr2, arr3, colors= ['r', 'g', 'b'])\n",
							"plt.title('Stack Plot Example')\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"# Pie charts\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"labels = 'S1', 'S2', 'S3'\n",
							"sections = [56, 66, 24]\n",
							"colors = ['c', 'g', 'y']\n",
							"\n",
							"plt.pie(sections, labels=labels, colors=colors,\n",
							"        startangle=90,\n",
							"        explode = (0, 0.1, 0),\n",
							"        autopct = '%1.2f%%')\n",
							"\n",
							"plt.axis('equal') # Try commenting this out.\n",
							"plt.title('Pie Chart Example')\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"# fill_between and alpha\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"ys = 200 + np.random.randn(100)\n",
							"x = [x for x in range(len(ys))]\n",
							"\n",
							"plt.plot(x, ys, '-')\n",
							"plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)\n",
							"\n",
							"plt.title(\"Fills and Alpha Example\")\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"# Subplotting using Subplot2grid\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"def random_plots():\n",
							"  xs = []\n",
							"  ys = []\n",
							"  \n",
							"  for i in range(20):\n",
							"    x = i\n",
							"    y = np.random.randint(10)\n",
							"    \n",
							"    xs.append(x)\n",
							"    ys.append(y)\n",
							"  \n",
							"  return xs, ys\n",
							"\n",
							"fig = plt.figure()\n",
							"ax1 = plt.subplot2grid((5, 2), (0, 0), rowspan=1, colspan=2)\n",
							"ax2 = plt.subplot2grid((5, 2), (1, 0), rowspan=3, colspan=2)\n",
							"ax3 = plt.subplot2grid((5, 2), (4, 0), rowspan=1, colspan=1)\n",
							"ax4 = plt.subplot2grid((5, 2), (4, 1), rowspan=1, colspan=1)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax1.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax2.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax3.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax4.plot(x, y)\n",
							"\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"# 3D Scatter Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"from mpl_toolkits.mplot3d import axes3d\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"y1 = np.random.randint(10, size=10)\n",
							"z1 = np.random.randint(10, size=10)\n",
							"\n",
							"x2 = [-1, -2, -3, -4, -5, -6, -7, -8, -9, -10]\n",
							"y2 = np.random.randint(-10, 0, size=10)\n",
							"z2 = np.random.randint(10, size=10)\n",
							"\n",
							"ax.scatter(x1, y1, z1, c='b', marker='o', label='blue')\n",
							"ax.scatter(x2, y2, z2, c='g', marker='D', label='green')\n",
							"\n",
							"ax.set_xlabel('x axis')\n",
							"ax.set_ylabel('y axis')\n",
							"ax.set_zlabel('z axis')\n",
							"plt.title(\"3D Scatter Plot Example\")\n",
							"plt.legend()\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							"# 3D Bar Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"y = np.random.randint(10, size=10)\n",
							"z = np.zeros(10)\n",
							"\n",
							"dx = np.ones(10)\n",
							"dy = np.ones(10)\n",
							"dz = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"\n",
							"ax.bar3d(x, y, z, dx, dy, dz, color='g')\n",
							"\n",
							"ax.set_xlabel('x axis')\n",
							"ax.set_ylabel('y axis')\n",
							"ax.set_zlabel('z axis')\n",
							"plt.title(\"3D Bar Chart Example\")\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"# Wireframe Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x, y, z = axes3d.get_test_data()\n",
							"\n",
							"ax.plot_wireframe(x, y, z, rstride = 2, cstride = 2)\n",
							"\n",
							"plt.title(\"Wireframe Plot Example\")\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Seaborn\n",
							"Seaborn is a library layered on top of Matplotlib that you can use."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Scatterplot with a nice regression line fit to it, all with just one call to Seaborn's regplot.\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"import seaborn as sns\n",
							"\n",
							"# Generate some random data\n",
							"num_points = 20\n",
							"# x will be 5, 6, 7... but also twiddled randomly\n",
							"x = 5 + np.arange(num_points) + np.random.randn(num_points)\n",
							"# y will be 10, 11, 12... but twiddled even more randomly\n",
							"y = 10 + np.arange(num_points) + 5 * np.random.randn(num_points)\n",
							"sns.regplot(x, y)\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"# Seanborn heatmap\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"# Make a 10 x 10 heatmap of some random data\n",
							"side_length = 10\n",
							"# Start with a 10 x 10 matrix with values randomized around 5\n",
							"data = 5 + np.random.randn(side_length, side_length)\n",
							"# The next two lines make the values larger as we get closer to (9, 9)\n",
							"data += np.arange(side_length)\n",
							"data += np.reshape(np.arange(side_length), (side_length, 1))\n",
							"# Generate the heatmap\n",
							"sns.heatmap(data)\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Bokeh\n",
							"You can render HTML or interactive libraries, like **bokeh**, using the **displayHTML()**.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"import numpy as np\n",
							"from bokeh.plotting import figure, show\n",
							"from bokeh.io import output_notebook\n",
							"from bokeh.embed import file_html\n",
							"from bokeh.resources import CDN\n",
							"\n",
							"N = 4000\n",
							"x = np.random.random(size=N) * 100\n",
							"y = np.random.random(size=N) * 100\n",
							"radii = np.random.random(size=N) * 1.5\n",
							"colors = [\"#%02x%02x%02x\" % (r, g, 150) for r, g in zip(np.floor(50+2*x).astype(int), np.floor(30+2*y).astype(int))]\n",
							"\n",
							"p = figure()\n",
							"p.circle(x, y, radius=radii, fill_color=colors, fill_alpha=0.6, line_color=None)\n",
							"show(p)\n",
							"\n",
							"# create an html document that embeds the Bokeh plot\n",
							"html = file_html(p, CDN, \"my plot1\")\n",
							"\n",
							"# display this html\n",
							"displayHTML(html)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Plotting glyphs over a map using bokeh.\n",
							"\n",
							"from bokeh.plotting import figure, output_file\n",
							"from bokeh.tile_providers import get_provider, Vendors\n",
							"from bokeh.embed import file_html\n",
							"from bokeh.resources import CDN\n",
							"from bokeh.models import ColumnDataSource\n",
							"\n",
							"tile_provider = get_provider(Vendors.CARTODBPOSITRON)\n",
							"\n",
							"# range bounds supplied in web mercator coordinates\n",
							"p = figure(x_range=(-9000000,-8000000), y_range=(4000000,5000000),\n",
							"           x_axis_type=\"mercator\", y_axis_type=\"mercator\")\n",
							"p.add_tile(tile_provider)\n",
							"\n",
							"# plot datapoints on the map\n",
							"source = ColumnDataSource(\n",
							"    data=dict(x=[ -8800000, -8500000 , -8800000],\n",
							"              y=[4200000, 4500000, 4900000])\n",
							")\n",
							"\n",
							"p.circle(x=\"x\", y=\"y\", size=15, fill_color=\"blue\", fill_alpha=0.8, source=source)\n",
							"\n",
							"# create an html document that embeds the Bokeh plot\n",
							"html = file_html(p, CDN, \"my plot1\")\n",
							"\n",
							"# display this html\n",
							"displayHTML(html)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Charting in Synapse Notebook_9d0')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "03cc45b1-db27-47b7-9754-9180e184fab7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Charting in Synapse Notebook\n",
							"\n",
							"Synapse has common used data visualization packages pre installed, such as **matplotlib**, **bokeh**, **seaborn**, **altair**, **plotly**. This notebook provides examples to do data visualization using charts in Synapse notebook. \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Matplotlib\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Line charts\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							" \n",
							"x  = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
							"y1 = [1, 3, 5, 3, 1, 3, 5, 3, 1]\n",
							"y2 = [2, 4, 6, 4, 2, 4, 6, 4, 2]\n",
							"plt.plot(x, y1, label=\"line L\")\n",
							"plt.plot(x, y2, label=\"line H\")\n",
							"plt.plot()\n",
							"\n",
							"plt.xlabel(\"x axis\")\n",
							"plt.ylabel(\"y axis\")\n",
							"plt.title(\"Line Graph Example\")\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"# Bar chart\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"# Look at index 4 and 6, which demonstrate overlapping cases.\n",
							"x1 = [1, 3, 4, 5, 6, 7, 9]\n",
							"y1 = [4, 7, 2, 4, 7, 8, 3]\n",
							"\n",
							"x2 = [2, 4, 6, 8, 10]\n",
							"y2 = [5, 6, 2, 6, 2]\n",
							"\n",
							"# Colors: https://matplotlib.org/api/colors_api.html\n",
							"\n",
							"plt.bar(x1, y1, label=\"Blue Bar\", color='b')\n",
							"plt.bar(x2, y2, label=\"Green Bar\", color='g')\n",
							"plt.plot()\n",
							"\n",
							"plt.xlabel(\"bar number\")\n",
							"plt.ylabel(\"bar height\")\n",
							"plt.title(\"Bar Chart Example\")\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"# Histogram\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"# Use numpy to generate a bunch of random data in a bell curve around 5.\n",
							"n = 5 + np.random.randn(1000)\n",
							"\n",
							"m = [m for m in range(len(n))]\n",
							"plt.bar(m, n)\n",
							"plt.title(\"Raw Data\")\n",
							"plt.show()\n",
							"\n",
							"plt.hist(n, bins=20)\n",
							"plt.title(\"Histogram\")\n",
							"plt.show()\n",
							"\n",
							"plt.hist(n, cumulative=True, bins=20)\n",
							"plt.title(\"Cumulative Histogram\")\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"# Scatter chart\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"x1 = [2, 3, 4]\n",
							"y1 = [5, 5, 5]\n",
							"\n",
							"x2 = [1, 2, 3, 4, 5]\n",
							"y2 = [2, 3, 2, 3, 4]\n",
							"y3 = [6, 8, 7, 8, 7]\n",
							"\n",
							"# Markers: https://matplotlib.org/api/markers_api.html\n",
							"\n",
							"plt.scatter(x1, y1)\n",
							"plt.scatter(x2, y2, marker='v', color='r')\n",
							"plt.scatter(x2, y3, marker='^', color='m')\n",
							"plt.title('Scatter Plot Example')\n",
							"plt.show()\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"# Stack plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"idxes = [ 1,  2,  3,  4,  5,  6,  7,  8,  9]\n",
							"arr1  = [23, 40, 28, 43,  8, 44, 43, 18, 17]\n",
							"arr2  = [17, 30, 22, 14, 17, 17, 29, 22, 30]\n",
							"arr3  = [15, 31, 18, 22, 18, 19, 13, 32, 39]\n",
							"\n",
							"# Adding legend for stack plots is tricky.\n",
							"plt.plot([], [], color='r', label = 'D 1')\n",
							"plt.plot([], [], color='g', label = 'D 2')\n",
							"plt.plot([], [], color='b', label = 'D 3')\n",
							"\n",
							"plt.stackplot(idxes, arr1, arr2, arr3, colors= ['r', 'g', 'b'])\n",
							"plt.title('Stack Plot Example')\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"# Pie charts\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"labels = 'S1', 'S2', 'S3'\n",
							"sections = [56, 66, 24]\n",
							"colors = ['c', 'g', 'y']\n",
							"\n",
							"plt.pie(sections, labels=labels, colors=colors,\n",
							"        startangle=90,\n",
							"        explode = (0, 0.1, 0),\n",
							"        autopct = '%1.2f%%')\n",
							"\n",
							"plt.axis('equal') # Try commenting this out.\n",
							"plt.title('Pie Chart Example')\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							"# fill_between and alpha\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"ys = 200 + np.random.randn(100)\n",
							"x = [x for x in range(len(ys))]\n",
							"\n",
							"plt.plot(x, ys, '-')\n",
							"plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)\n",
							"\n",
							"plt.title(\"Fills and Alpha Example\")\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"# Subplotting using Subplot2grid\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"def random_plots():\n",
							"  xs = []\n",
							"  ys = []\n",
							"  \n",
							"  for i in range(20):\n",
							"    x = i\n",
							"    y = np.random.randint(10)\n",
							"    \n",
							"    xs.append(x)\n",
							"    ys.append(y)\n",
							"  \n",
							"  return xs, ys\n",
							"\n",
							"fig = plt.figure()\n",
							"ax1 = plt.subplot2grid((5, 2), (0, 0), rowspan=1, colspan=2)\n",
							"ax2 = plt.subplot2grid((5, 2), (1, 0), rowspan=3, colspan=2)\n",
							"ax3 = plt.subplot2grid((5, 2), (4, 0), rowspan=1, colspan=1)\n",
							"ax4 = plt.subplot2grid((5, 2), (4, 1), rowspan=1, colspan=1)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax1.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax2.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax3.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax4.plot(x, y)\n",
							"\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"# 3D Scatter Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"from mpl_toolkits.mplot3d import axes3d\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"y1 = np.random.randint(10, size=10)\n",
							"z1 = np.random.randint(10, size=10)\n",
							"\n",
							"x2 = [-1, -2, -3, -4, -5, -6, -7, -8, -9, -10]\n",
							"y2 = np.random.randint(-10, 0, size=10)\n",
							"z2 = np.random.randint(10, size=10)\n",
							"\n",
							"ax.scatter(x1, y1, z1, c='b', marker='o', label='blue')\n",
							"ax.scatter(x2, y2, z2, c='g', marker='D', label='green')\n",
							"\n",
							"ax.set_xlabel('x axis')\n",
							"ax.set_ylabel('y axis')\n",
							"ax.set_zlabel('z axis')\n",
							"plt.title(\"3D Scatter Plot Example\")\n",
							"plt.legend()\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							"# 3D Bar Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"y = np.random.randint(10, size=10)\n",
							"z = np.zeros(10)\n",
							"\n",
							"dx = np.ones(10)\n",
							"dy = np.ones(10)\n",
							"dz = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"\n",
							"ax.bar3d(x, y, z, dx, dy, dz, color='g')\n",
							"\n",
							"ax.set_xlabel('x axis')\n",
							"ax.set_ylabel('y axis')\n",
							"ax.set_zlabel('z axis')\n",
							"plt.title(\"3D Bar Chart Example\")\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"# Wireframe Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x, y, z = axes3d.get_test_data()\n",
							"\n",
							"ax.plot_wireframe(x, y, z, rstride = 2, cstride = 2)\n",
							"\n",
							"plt.title(\"Wireframe Plot Example\")\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Seaborn\n",
							"Seaborn is a library layered on top of Matplotlib that you can use."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Scatterplot with a nice regression line fit to it, all with just one call to Seaborn's regplot.\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"import seaborn as sns\n",
							"\n",
							"# Generate some random data\n",
							"num_points = 20\n",
							"# x will be 5, 6, 7... but also twiddled randomly\n",
							"x = 5 + np.arange(num_points) + np.random.randn(num_points)\n",
							"# y will be 10, 11, 12... but twiddled even more randomly\n",
							"y = 10 + np.arange(num_points) + 5 * np.random.randn(num_points)\n",
							"sns.regplot(x, y)\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"# Seanborn heatmap\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"# Make a 10 x 10 heatmap of some random data\n",
							"side_length = 10\n",
							"# Start with a 10 x 10 matrix with values randomized around 5\n",
							"data = 5 + np.random.randn(side_length, side_length)\n",
							"# The next two lines make the values larger as we get closer to (9, 9)\n",
							"data += np.arange(side_length)\n",
							"data += np.reshape(np.arange(side_length), (side_length, 1))\n",
							"# Generate the heatmap\n",
							"sns.heatmap(data)\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Bokeh\n",
							"You can render HTML or interactive libraries, like **bokeh**, using the **displayHTML()**.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"import numpy as np\n",
							"from bokeh.plotting import figure, show\n",
							"from bokeh.io import output_notebook\n",
							"from bokeh.embed import file_html\n",
							"from bokeh.resources import CDN\n",
							"\n",
							"N = 4000\n",
							"x = np.random.random(size=N) * 100\n",
							"y = np.random.random(size=N) * 100\n",
							"radii = np.random.random(size=N) * 1.5\n",
							"colors = [\"#%02x%02x%02x\" % (r, g, 150) for r, g in zip(np.floor(50+2*x).astype(int), np.floor(30+2*y).astype(int))]\n",
							"\n",
							"p = figure()\n",
							"p.circle(x, y, radius=radii, fill_color=colors, fill_alpha=0.6, line_color=None)\n",
							"show(p)\n",
							"\n",
							"# create an html document that embeds the Bokeh plot\n",
							"html = file_html(p, CDN, \"my plot1\")\n",
							"\n",
							"# display this html\n",
							"displayHTML(html)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"source": [
							"# Plotting glyphs over a map using bokeh.\n",
							"\n",
							"from bokeh.plotting import figure, output_file\n",
							"from bokeh.tile_providers import get_provider, Vendors\n",
							"from bokeh.embed import file_html\n",
							"from bokeh.resources import CDN\n",
							"from bokeh.models import ColumnDataSource\n",
							"\n",
							"tile_provider = get_provider(Vendors.CARTODBPOSITRON)\n",
							"\n",
							"# range bounds supplied in web mercator coordinates\n",
							"p = figure(x_range=(-9000000,-8000000), y_range=(4000000,5000000),\n",
							"           x_axis_type=\"mercator\", y_axis_type=\"mercator\")\n",
							"p.add_tile(tile_provider)\n",
							"\n",
							"# plot datapoints on the map\n",
							"source = ColumnDataSource(\n",
							"    data=dict(x=[ -8800000, -8500000 , -8800000],\n",
							"              y=[4200000, 4500000, 4900000])\n",
							")\n",
							"\n",
							"p.circle(x=\"x\", y=\"y\", size=15, fill_color=\"blue\", fill_alpha=0.8, source=source)\n",
							"\n",
							"# create an html document that embeds the Bokeh plot\n",
							"html = file_html(p, CDN, \"my plot1\")\n",
							"\n",
							"# display this html\n",
							"displayHTML(html)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 27
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Creating a managed Spark Table_acd')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "scala"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5bb59572-c10a-413b-bc04-45eff6e23ae0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "scala"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Creating a managed Spark table\n",
							"This notebook describes how to create a managed table from Spark. \n",
							"The table is created in the Synapse warehouse folder in your primary storage account. The table will be synchronized and available in Synapse SQL Pools. \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"CREATE TABLE cities (name STRING, population INT) USING PARQUET\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"Insert a few rows into the table using a list of values.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"INSERT INTO cities VALUES ('Seattle', 730400), ('San Francisco', 881549), ('Beijing', 21540000), ('Bangalore', 10540000)\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"source": [
							"* Retrieve values back. Click on 'Chart' below to review the visualization.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [
										"name"
									],
									"values": [
										"population"
									],
									"yLabel": "population",
									"xLabel": "name",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{\"population\":{\"Bangalore\":10540000,\"Beijing\":21540000,\"San Francisco\":881549,\"Seattle\":730400}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": false
							},
							"collapsed": false
						},
						"source": [
							"display(spark.sql(\"SELECT * FROM cities ORDER BY name\"))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"Drop the table. Please note the data will get deleted from the primary storage account associated with this workspace.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DROP TABLE cities\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Creating a managed Spark Table_gyq')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3bc120d8-a9ce-4934-87ea-318d8195e9f3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "scala"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Creating a managed Spark table\n",
							"This notebook describes how to create a managed table from Spark. \n",
							"The table is created in the Synapse warehouse folder in your primary storage account. The table will be synchronized and available in Synapse SQL Pools. \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"CREATE TABLE cities (name STRING, population INT) USING PARQUET\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"Insert a few rows into the table using a list of values.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"INSERT INTO cities VALUES ('Seattle', 730400), ('San Francisco', 881549), ('Beijing', 21540000), ('Bangalore', 10540000)\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"* Retrieve values back. Click on 'Chart' below to review the visualization.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [
										"name"
									],
									"values": [
										"population"
									],
									"yLabel": "population",
									"xLabel": "name",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{\"population\":{\"Bangalore\":10540000,\"Beijing\":21540000,\"San Francisco\":881549,\"Seattle\":730400}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": false
							}
						},
						"source": [
							"display(spark.sql(\"SELECT * FROM cities ORDER BY name\"))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"Drop the table. Please note the data will get deleted from the primary storage account associated with this workspace.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DROP TABLE cities\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Creating an unmanaged Spark Table')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "python"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c69768c6-99d6-4eee-854c-7383fc0d6103"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Creating an unmanaged (external) Spark table\n",
							"This notebook describes how to create an unmanaged (also known as external) table from Spark. \n",
							"The table is created in /datalake/cities which may exist already (so you can attach to existing data) it can be created when you insert data."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"CREATE TABLE cities (name STRING, population INT) USING PARQUET  LOCATION \\'/datalake/cities\\' OPTIONS (\\'compression\\'=\\'snappy\\')\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"Insert a few rows into the table using a list of values.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"INSERT INTO cities VALUES (\\'Seattle\\', 730400), (\\'San Francisco\\', 881549), (\\'Beijing\\', 21540000), (\\'Bangalore\\', 10540000)\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"* Retrieve values back. Click on 'Chart' below to review the visualization.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [
										"name"
									],
									"values": [
										"population"
									],
									"yLabel": "population",
									"xLabel": "name",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{\"population\":{\"Bangalore\":21080000,\"Beijing\":43080000,\"San Francisco\":1763098,\"Seattle\":1460800}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": false
							}
						},
						"source": [
							"display(spark.sql(\"SELECT * FROM cities ORDER BY name\"))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"Drop the table. Please note the data will remain in the data lake.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DROP TABLE cities\")"
						],
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Creating an unmanaged Spark Table_9ln')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "scala"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8c3b8ca3-57f1-41a4-a4bc-1b9fb068da20"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "scala"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Creating an unmanaged (external) Spark table\n",
							"This notebook describes how to create an unmanaged (also known as external) table from Spark. \n",
							"The table is created in /datalake/cities which may exist already (so you can attach to existing data) it can be created when you insert data."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"CREATE TABLE cities (name STRING, population INT) USING PARQUET LOCATION '/datalake/cities' OPTIONS ('compression'='snappy')\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"Insert a few rows into the table using a list of values.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"INSERT INTO cities VALUES ('Seattle', 730400), ('San Francisco', 881549), ('Beijing', 21540000), ('Bangalore', 10540000)\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"source": [
							"* Retrieve values back. Click on 'Chart' below to review the visualization.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [
										"name"
									],
									"values": [
										"population"
									],
									"yLabel": "population",
									"xLabel": "name",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{\"population\":{\"Bangalore\":31620000,\"Beijing\":64620000,\"San Francisco\":2644647,\"Seattle\":2191200}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": false
							},
							"collapsed": false
						},
						"source": [
							"display(spark.sql(\"SELECT * FROM cities ORDER BY name\"))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"Drop the table. Please note the data will remain in the data lake.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DROP TABLE cities\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Data Exploration and ML Modeling - NYC taxi predict using Spark MLlib')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "python"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "648e7f35-6ecc-417f-bc39-946293c2d400"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Predict NYC Taxi Tips using Spark ML and Azure Open Datasets\n",
							"\n",
							"The notebook ingests, visualizes, prepares and then trains a model based on an Open Dataset that tracks NYC Yellow Taxi trips and various attributes around them.\n",
							"The goal is to predict for a given trip whether there will be a trip or not.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import matplotlib.pyplot as plt\n",
							"\n",
							"from pyspark.sql.functions import unix_timestamp\n",
							"\n",
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.types import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"from pyspark.ml import Pipeline\n",
							"from pyspark.ml import PipelineModel\n",
							"from pyspark.ml.feature import RFormula\n",
							"from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer\n",
							"from pyspark.ml.classification import LogisticRegression\n",
							"from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
							"from pyspark.ml.evaluation import BinaryClassificationEvaluator"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Ingest Data \n",
							"\n",
							"Get a sample data of nyc yellow taxi to make it faster/easier to evaluate different approaches to prep for the modelling phase later in the notebook."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Import NYC yellow cab data from Azure Open Datasets\n",
							"from azureml.opendatasets import NycTlcYellow\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"\n",
							"end_date = parser.parse('2018-05-08 00:00:00')\n",
							"start_date = parser.parse('2018-05-01 00:00:00')\n",
							"\n",
							"nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
							"nyc_tlc_df = nyc_tlc.to_spark_dataframe()"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"#To make development easier, faster and less expensive downsample for now\n",
							"sampled_taxi_df = nyc_tlc_df.sample(True, 0.001, seed=1234)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Exploratory Data Analysis\n",
							"\n",
							"Look at the data and evaluate its suitability for use in a model, do this via some basic charts focussed on tip values and relationships."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"#The charting package needs a Pandas dataframe or numpy array do the conversion\n",
							"sampled_taxi_pd_df = sampled_taxi_df.toPandas()\n",
							"\n",
							"# Look at tips by amount count histogram\n",
							"ax1 = sampled_taxi_pd_df['tipAmount'].plot(kind='hist', bins=25, facecolor='lightblue')\n",
							"ax1.set_title('Tip amount distribution')\n",
							"ax1.set_xlabel('Tip Amount ($)')\n",
							"ax1.set_ylabel('Counts')\n",
							"plt.suptitle('')\n",
							"plt.show()\n",
							"\n",
							"# How many passengers tip'd by various amounts\n",
							"ax2 = sampled_taxi_pd_df.boxplot(column=['tipAmount'], by=['passengerCount'])\n",
							"ax2.set_title('Tip amount by Passenger count')\n",
							"ax2.set_xlabel('Passenger count') \n",
							"ax2.set_ylabel('Tip Amount ($)')\n",
							"plt.suptitle('')\n",
							"plt.show()\n",
							"\n",
							"# Look at the relationship between fare and tip amounts\n",
							"ax = sampled_taxi_pd_df.plot(kind='scatter', x= 'fareAmount', y = 'tipAmount', c='blue', alpha = 0.10, s=2.5*(sampled_taxi_pd_df['passengerCount']))\n",
							"ax.set_title('Tip amount by Fare amount')\n",
							"ax.set_xlabel('Fare Amount ($)')\n",
							"ax.set_ylabel('Tip Amount ($)')\n",
							"plt.axis([-2, 80, -2, 20])\n",
							"plt.suptitle('')\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data Prep and Featurization\n",
							"\n",
							"It's clear from the visualizations above that there are a bunch of outliers in the data. These will need to be filtered out in addition there are extra variables that are not going to be useful in the model we build at the end.\n",
							"\n",
							"Finally there is a need to create some new (derived) variables that will work better with the model.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"taxi_df = sampled_taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'rateCodeId', 'passengerCount'\\\n",
							"                                , 'tripDistance', 'tpepPickupDateTime', 'tpepDropoffDateTime'\\\n",
							"                                , date_format('tpepPickupDateTime', 'hh').alias('pickupHour')\\\n",
							"                                , date_format('tpepPickupDateTime', 'EEEE').alias('weekdayString')\\\n",
							"                                , (unix_timestamp(col('tpepDropoffDateTime')) - unix_timestamp(col('tpepPickupDateTime'))).alias('tripTimeSecs')\\\n",
							"                                , (when(col('tipAmount') > 0, 1).otherwise(0)).alias('tipped')\n",
							"                                )\\\n",
							"                        .filter((sampled_taxi_df.passengerCount > 0) & (sampled_taxi_df.passengerCount < 8)\\\n",
							"                                & (sampled_taxi_df.tipAmount >= 0) & (sampled_taxi_df.tipAmount <= 25)\\\n",
							"                                & (sampled_taxi_df.fareAmount >= 1) & (sampled_taxi_df.fareAmount <= 250)\\\n",
							"                                & (sampled_taxi_df.tipAmount < sampled_taxi_df.fareAmount)\\\n",
							"                                & (sampled_taxi_df.tripDistance > 0) & (sampled_taxi_df.tripDistance <= 100)\\\n",
							"                                & (sampled_taxi_df.rateCodeId <= 5)\n",
							"                                & (sampled_taxi_df.paymentType.isin({\"1\", \"2\"}))\n",
							"                                )"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data Prep and Featurization Part 2\n",
							"\n",
							"Having created new variables its now possible to drop the columns they were derived from so that the dataframe that goes into the model is the smallest in terms of number of variables, that is required.\n",
							"\n",
							"Also create some more features based on new columns from the first round.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"taxi_featurised_df = taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'passengerCount'\\\n",
							"                                                , 'tripDistance', 'weekdayString', 'pickupHour','tripTimeSecs','tipped'\\\n",
							"                                                , when((taxi_df.pickupHour <= 6) | (taxi_df.pickupHour >= 20),\"Night\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 7) & (taxi_df.pickupHour <= 10), \"AMRush\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 11) & (taxi_df.pickupHour <= 15), \"Afternoon\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 16) & (taxi_df.pickupHour <= 19), \"PMRush\")\\\n",
							"                                                .otherwise(0).alias('trafficTimeBins')\n",
							"                                              )\\\n",
							"                                       .filter((taxi_df.tripTimeSecs >= 30) & (taxi_df.tripTimeSecs <= 7200))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Encoding\n",
							"\n",
							"Different ML algorithms support different types of input, for this example Logistic Regression is being used for Binary Classification. This means that any Categorical (string) variables must be converted to numbers.\n",
							"\n",
							"The process is not as simple as a \"map\" style function as the relationship between the numbers can introduce a bias in the resulting model, the approach is to index the variable and then encode using a std approach called One Hot Encoding.\n",
							"\n",
							"This approach requires the encoder to \"learn\"/fit a model over the data in the Spark instance and then transform based on what was learnt.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# The sample uses an algorithm that only works with numeric features convert them so they can be consumed\n",
							"sI1 = StringIndexer(inputCol=\"trafficTimeBins\", outputCol=\"trafficTimeBinsIndex\"); \n",
							"en1 = OneHotEncoder(dropLast=False, inputCol=\"trafficTimeBinsIndex\", outputCol=\"trafficTimeBinsVec\");\n",
							"sI2 = StringIndexer(inputCol=\"weekdayString\", outputCol=\"weekdayIndex\"); \n",
							"en2 = OneHotEncoder(dropLast=False, inputCol=\"weekdayIndex\", outputCol=\"weekdayVec\");\n",
							"\n",
							"# Create a new dataframe that has had the encodings applied\n",
							"encoded_final_df = Pipeline(stages=[sI1, en1, sI2, en2]).fit(taxi_featurised_df).transform(taxi_featurised_df)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Generation of Testing and Training Data Sets\n",
							"Simple split, 70% for training and 30% for testing the model. Playing with this ratio may result in different models.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Decide on the split between training and testing data from the dataframe \n",
							"trainingFraction = 0.7\n",
							"testingFraction = (1-trainingFraction)\n",
							"seed = 1234\n",
							"\n",
							"# Split the dataframe into test and training dataframes\n",
							"train_data_df, test_data_df = encoded_final_df.randomSplit([trainingFraction, testingFraction], seed=seed)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Train the Model\n",
							"\n",
							"Train the Logistic Regression model and then evaluate it using Area under ROC as the metric."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"## Create a new LR object for the model\n",
							"logReg = LogisticRegression(maxIter=10, regParam=0.3, labelCol = 'tipped')\n",
							"\n",
							"## The formula for the model\n",
							"classFormula = RFormula(formula=\"tipped ~ pickupHour + weekdayVec + passengerCount + tripTimeSecs + tripDistance + fareAmount + paymentType+ trafficTimeBinsVec\")\n",
							"\n",
							"## Undertake training and create an LR model\n",
							"lrModel = Pipeline(stages=[classFormula, logReg]).fit(train_data_df)\n",
							"\n",
							"## Saving the model is optional but its another for of inter session cache\n",
							"datestamp = datetime.now().strftime('%m-%d-%Y-%s');\n",
							"fileName = \"lrModel_\" + datestamp;\n",
							"logRegDirfilename = fileName;\n",
							"lrModel.save(logRegDirfilename)\n",
							"\n",
							"## Predict tip 1/0 (yes/no) on the test dataset, evaluation using AUROC\n",
							"predictions = lrModel.transform(test_data_df)\n",
							"predictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\n",
							"metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
							"print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Evaluate and Visualize\n",
							"\n",
							"Plot the actual curve to develop a better understanding of the model.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"## Plot the ROC curve, no need for pandas as this uses the modelSummary object\n",
							"modelSummary = lrModel.stages[-1].summary\n",
							"\n",
							"plt.plot([0, 1], [0, 1], 'r--')\n",
							"plt.plot(modelSummary.roc.select('FPR').collect(),\n",
							"         modelSummary.roc.select('TPR').collect())\n",
							"plt.xlabel('False Positive Rate')\n",
							"plt.ylabel('True Positive Rate')\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Display function sample')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9e2c8008-9ca6-49e0-9f0d-93ba73ec202e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#display(DataFrame) sample:\n",
							"\n",
							"# import pyspark class Row from module sql\n",
							"from pyspark.sql import *\n",
							"# Create Example Data - Departments and Employees\n",
							"# Create the Departments\n",
							"department1 = Row(id='123456', name='Computer Science')\n",
							"department2 = Row(id='789012', name='Mechanical Engineering')\n",
							"department3 = Row(id='345678', name='Theater and Drama')\n",
							"department4 = Row(id='901234', name='Indoor Recreation')\n",
							"# Create the Employees\n",
							"Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
							"employee1 = Employee('michael', 'armbrust', 'noreply@berkeley.edore', 130000)\n",
							"employee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\n",
							"employee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\n",
							"employee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\n",
							"# Create the DepartmentWithEmployees instances from Departments and Employees\n",
							"departmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\n",
							"departmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\n",
							"departmentWithEmployees3 = Row(department=department3, employees=[employee1, employee4])\n",
							"departmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\n",
							"\n",
							"departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\n",
							"departmentsWithEmployeesSeq2 = [departmentWithEmployees2, departmentWithEmployees3]\n",
							"df1 = spark.createDataFrame(departmentsWithEmployeesSeq1)\n",
							"\n",
							"display(df1)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# display(List) sample:\n",
							"students = [ ['jack', 34, '36'] ,\n",
							"['Riti', 30, 'Delhi' ] ,\n",
							"['Aadi', 16, 'New York'] ]\n",
							"\n",
							"display(students)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# display(pandas.DataFrame) sample:\n",
							"import numpy as np\n",
							"import pandas as pd\n",
							"from pyspark.sql import *\n",
							"d = {'col11': [1, 2, 3, 4, 5, 6, 7, 8], 'col12': [3, 4, 5, 6, 8, 3, 16, 20]}\n",
							"pdf = pd.DataFrame(data=d)\n",
							"\n",
							"display(pdf)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\n",
							"display(spark.range(10))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\n",
							"case class MapEntry(key: String, value: Int)\n",
							"val largeSeries = for (x <- 1 to 5000) yield MapEntry(\"k_%04d\".format(x), x)\n",
							"val largeDataFrame = sc.parallelize(largeSeries).toDF()\n",
							"largeDataFrame.registerTempTable(\"largeTable\")\n",
							"display(spark.sqlContext.sql(\"select * from largeTable\"))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"displayHTML(\"\"\"\n",
							"<!DOCTYPE html>\n",
							" <meta charset=\"utf-8\">\n",
							" <style>\n",
							" path {\n",
							" fill: white;\n",
							" stroke: #000;\n",
							" }\n",
							" circle {\n",
							" fill: #fff;\n",
							" stroke: #000;\n",
							" pointer-events: none;\n",
							" }\n",
							" .PiYG .q0-9{fill:rgb${colors(0)}}\n",
							" .PiYG .q1-9{fill:rgb${colors(1)}}\n",
							" .PiYG .q2-9{fill:rgb${colors(2)}}\n",
							" .PiYG .q3-9{fill:rgb${colors(3)}}\n",
							" .PiYG .q4-9{fill:rgb${colors(4)}}\n",
							" .PiYG .q5-9{fill:rgb${colors(5)}}\n",
							" .PiYG .q6-9{fill:rgb${colors(6)}}\n",
							" .PiYG .q7-9{fill:rgb${colors(7)}}\n",
							" .PiYG .q8-9{fill:rgb${colors(8)}}\n",
							" </style>\n",
							" <body>\n",
							" <script src=\"https://d3js.org/d3.v3.min.js\"></script>\n",
							" <script>\n",
							" var width = 960,\n",
							" height = 500;\n",
							" var vertices = d3.range(100).map(function(d) {\n",
							" return [Math.random() * width, Math.random() * height];\n",
							" });\n",
							" var svg = d3.select(\"body\").append(\"svg\")\n",
							" .attr(\"width\", width)\n",
							" .attr(\"height\", height)\n",
							" .attr(\"class\", \"PiYG\")\n",
							" .on(\"mousemove\", function() { vertices[0] = d3.mouse(this); redraw(); });\n",
							" var path = svg.append(\"g\").selectAll(\"path\");\n",
							" svg.selectAll(\"circle\")\n",
							" .data(vertices.slice(1))\n",
							" .enter().append(\"circle\")\n",
							" .attr(\"transform\", function(d) { return \"translate(\" + d + \")\"; })\n",
							" .attr(\"r\", 2);\n",
							" redraw();\n",
							" function redraw() {\n",
							" path = path.data(d3.geom.delaunay(vertices).map(function(d) { return \"M\" + d.join(\"L\") + \"Z\"; }), String);\n",
							" path.exit().remove();\n",
							" path.enter().append(\"path\").attr(\"class\", function(d, i) { return \"q\" + (i % 9) + \"-9\"; }).attr(\"d\", String);\n",
							" }\n",
							" </script>\n",
							"\"\"\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"from bokeh.plotting import figure\n",
							"from bokeh.embed import components, file_html\n",
							"from bokeh.resources import CDN\n",
							"# prepare some data\n",
							"x = [1, 2, 3, 4, 5]\n",
							"y = [6, 7, 2, 4, 5]\n",
							"# create a new plot with a title and axis labels\n",
							"p = figure(title=\"simple line example\", x_axis_label='x', y_axis_label='y')\n",
							"# add a line renderer with legend and line thickness\n",
							"p.line(x, y, legend=\"Temp.\", line_width=2)\n",
							"# create an html document that embeds the Bokeh plot\n",
							"html = file_html(p, CDN, \"my plot1\")\n",
							"# display this html\n",
							"displayHTML(html)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"#display(List) sample: \n",
							"students = [ ['jack', 34,4,3154284,304,'10.0', '36','Delhi','Delhi','Delhi','Delhi','Delhi','Delhi','Delhi','Delhi','Delhi','100.0123','New York','New York','New York','New York','1248700.01','New York','New York','New York','New York','1248700.01', '1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01' ] , \n",
							"['Riti', 30,369864,3244,3424525, '20.01', 'Delhi','New York','New York','New York','New York','New York','New York','New York','New York','New York','1248700.01','New York','New York','New York','New York','1248700.01','New York','New York','New York','New York','1248700.01', '1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01'] , \n",
							"['Aadi', 16,34745,3414,3000004, '30.02', 'New York','New York','Riti','Riti','Riti','Riti','Riti','Riti','Riti','Riti','100000.021','New York','New York','New York','New York','1248700.01','New York','New York','New York','New York','1248700.01', '1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01'] ] \n",
							"\n",
							"display(students)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"displayHTML(\"\"\" \n",
							"<!DOCTYPE html>\n",
							"<head lang=\"en\">\n",
							"  <meta charset=\"utf-8\">\n",
							"  <title></title>\n",
							"  <style type=\"text/css\">\n",
							"         div {\n",
							"              width: 100px;\n",
							"              height: 2200px;\n",
							"              background-color: white;\n",
							"              margin-top: 20px;\n",
							"              padding-top: 20px;\n",
							"              border: 20px solid red;\n",
							"              border: 5px dashed red;/**/\n",
							"             }\n",
							"  </style>\n",
							" \n",
							"</head>\n",
							"<body>\n",
							" <div>Test</div>\n",
							" <div></div>\n",
							"</body>\n",
							"</html>\n",
							" \n",
							"\"\"\")\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Getting Started with Delta Lake')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "544ec8ca-7446-47e9-a1e4-c4745ff7b70a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Hitchhiker's Guide to Delta Lake (Python)\n",
							"\n",
							"This tutorial has been adapted for more clarity from its original counterpart [here](https://docs.delta.io/latest/quick-start.html). This notebook helps you quickly explore the main features of [Delta Lake](https://github.com/delta-io/delta). It provides code snippets that show how to read from and write to Delta Lake tables from interactive, batch, and streaming queries.\n",
							"\n",
							"Here's what we will cover:\n",
							"* Create a table\n",
							"* Understanding meta-data\n",
							"* Read data\n",
							"* Update table data\n",
							"* Overwrite table data\n",
							"* Conditional update without overwrite\n",
							"* Read older versions of data using Time Travel\n",
							"* Write a stream of data to a table\n",
							"* Read a stream of changes from a table"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Configuration\n",
							"Make sure you modify this as appropriate."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"import random\n",
							"\n",
							"session_id = random.randint(0,1000000)\n",
							"delta_table_path = \"/delta/delta-table-{0}\".format(session_id)\n",
							"\n",
							"delta_table_path"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Create a table\n",
							"To create a Delta Lake table, write a DataFrame out in the **delta** format. You can use existing Spark SQL code and change the format from parquet, csv, json, and so on, to delta.\n",
							"\n",
							"These operations create a new Delta Lake table using the schema that was inferred from your DataFrame. For the full set of options available when you create a new Delta Lake table, see Create a table and Write to a table (subsequent cells in this notebook)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"data = spark.range(0,5)\n",
							"data.show()\n",
							"data.write.format(\"delta\").save(delta_table_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Understanding Meta-data\n",
							"\n",
							"In Delta Lake, meta-data is no different from data i.e., it is stored next to the data. Therefore, an interesting side-effect here is that you can peek into meta-data using regular Spark APIs. "
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"[log_line.value for log_line in spark.read.text(delta_table_path + \"/_delta_log/\").collect()]"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read data\n",
							"\n",
							"You read data in your Delta Lake table by specifying the path to the files."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.format(\"delta\").load(delta_table_path)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Update table data\n",
							"\n",
							"Delta Lake supports several operations to modify tables using standard DataFrame APIs. This example runs a batch job to overwrite the data in the table.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"data = spark.range(5,10)\n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"When you now inspect the meta-data, what you will notice is that the original data is over-written. Well, not in a true sense but appropriate entries are added to Delta's transaction log so it can provide an \"illusion\" that the original data was deleted. We can verify this by re-inspecting the meta-data. You will see several entries indicating reference removal to the original data."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"[log_line.value for log_line in spark.read.text(delta_table_path + \"/_delta_log/\").collect()]"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Save as catalog tables\n",
							"\n",
							"Delta Lake can write to managed or external catalog tables."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Write data to a new managed catalog table.\n",
							"data.write.format(\"delta\").saveAsTable(\"ManagedDeltaTable\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							"# Define an external catalog table that points to the existing Delta Lake data in storage.\n",
							"spark.sql(\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '{0}'\".format(delta_table_path))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"# List the 2 new tables.\n",
							"spark.sql(\"SHOW TABLES\").show()\n",
							"\n",
							"# Explore their properties.\n",
							"spark.sql(\"DESCRIBE EXTENDED ManagedDeltaTable\").show(truncate=False)\n",
							"spark.sql(\"DESCRIBE EXTENDED ExternalDeltaTable\").show(truncate=False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Conditional update without overwrite\n",
							"\n",
							"Delta Lake provides programmatic APIs to conditional update, delete, and merge (upsert) data into tables. For more information on these operations, see [Table Deletes, Updates, and Merges](https://docs.delta.io/latest/delta-update.html)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from delta.tables import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"delta_table = DeltaTable.forPath(spark, delta_table_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"# Update every even value by adding 100 to it\n",
							"delta_table.update(\n",
							"  condition = expr(\"id % 2 == 0\"),\n",
							"  set = { \"id\": expr(\"id + 100\") })\n",
							"delta_table.toDF().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							"# Delete every even value\n",
							"delta_table.delete(\"id % 2 == 0\")\n",
							"delta_table.toDF().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"# Upsert (merge) new data\n",
							"new_data = spark.range(0,20).alias(\"newData\")\n",
							"\n",
							"delta_table.alias(\"oldData\")\\\n",
							"    .merge(new_data.alias(\"newData\"), \"oldData.id = newData.id\")\\\n",
							"    .whenMatchedUpdate(set = { \"id\": lit(\"-1\")})\\\n",
							"    .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\") })\\\n",
							"    .execute()\n",
							"\n",
							"delta_table.toDF().show(100)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"source": [
							"## History\n",
							"Delta's most powerful feature is the ability to allow looking into history i.e., the changes that were made to the underlying Delta Table. The cell below shows how simple it is to inspect the history."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().show(20, 1000, False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read older versions of data using Time Travel\n",
							"\n",
							"You can query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option.\n",
							"\n",
							"Once you run the cell below, you should see the first set of data, from before you overwrote it. Time Travel is an extremely powerful feature that takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table. Removing the version 0 option (or specifying version 1) would let you see the newer data again. For more information, see [Query an older snapshot of a table (time travel)](https://docs.delta.io/latest/delta-batch.html#deltatimetravel)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write a stream of data to a table\n",
							"\n",
							"You can also write to a Delta Lake table using Spark's Structured Streaming. The Delta Lake transaction log guarantees exactly-once processing, even when there are other streams or batch queries running concurrently against the table. By default, streams run in append mode, which adds new records to the table.\n",
							"\n",
							"For more information about Delta Lake integration with Structured Streaming, see [Table Streaming Reads and Writes](https://docs.delta.io/latest/delta-streaming.html).\n",
							"\n",
							"In the cells below, here's what we are doing:\n",
							"\n",
							"1. *Cell 28* Setup a simple Spark Structured Streaming job to generate a sequence and make the job write into our Delta Table\n",
							"2. *Cell 30* Show the newly appended data\n",
							"3. *Cell 31* Inspect history\n",
							"4. *Cell 32* Stop the structured streaming job\n",
							"5. *Cell 33* Inspect history <-- You'll notice appends have stopped"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"streaming_df = spark.readStream.format(\"rate\").load()\n",
							"stream = streaming_df\\\n",
							"    .selectExpr(\"value as id\")\\\n",
							"    .writeStream\\\n",
							"    .format(\"delta\")\\\n",
							"    .option(\"checkpointLocation\", \"/tmp/checkpoint-{0}\".format(session_id))\\\n",
							"    .start(delta_table_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read a stream of changes from a table\n",
							"\n",
							"While the stream is writing to the Delta Lake table, you can also read from that table as streaming source. For example, you can start another streaming query that prints all the changes made to the Delta Lake table."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.toDF().sort(col(\"id\").desc()).show(100)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(20, 1000, False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"source": [
							"stream.stop()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(100, 1000, False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Convert Parquet to Delta\n",
							"You can do an in-place conversion from the Parquet format to Delta."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_path = \"/parquet/parquet-table-{0}\".format(session_id)\n",
							"\n",
							"data = spark.range(0,5)\n",
							"data.write.parquet(parquet_path)\n",
							"\n",
							"# Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"source": [
							"DeltaTable.convertToDelta(spark, \"parquet.`{0}`\".format(parquet_path))\n",
							"\n",
							"# Confirm that the converted data is now in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "markdown",
						"source": [
							"## SQL Support\n",
							"Delta supports table utility commands through SQL.  You can use SQL to:\n",
							"* Get a DeltaTable's history\n",
							"* Vacuum a DeltaTable\n",
							"* Convert a Parquet file to Delta\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DESCRIBE HISTORY delta.`{0}`\".format(delta_table_path)).show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"VACUUM delta.`{0}`\".format(delta_table_path)).show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_id = random.randint(0,1000)\n",
							"parquet_path = \"/parquet/parquet-table-{0}-{1}\".format(session_id, parquet_path)\n",
							"\n",
							"data = spark.range(0,5)\n",
							"data.write.parquet(parquet_path)\n",
							"\n",
							"# Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)\n",
							"\n",
							"# Use SQL to convert the parquet table to Delta\n",
							"spark.sql(\"CONVERT TO DELTA parquet.`{0}`\".format(parquet_path))\n",
							"\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 27
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Getting Started with Delta Lake_d09')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "scala"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5d5eb5dd-60c3-4188-a464-e6ef932ab102"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "Synapse Spark"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Hitchhiker's Guide to Delta Lake (Scala)\n",
							"\n",
							"This tutorial has been adapted for more clarity from its original counterpart [here](https://docs.delta.io/latest/quick-start.html). This notebook helps you quickly explore the main features of [Delta Lake](https://github.com/delta-io/delta). It provides code snippets that show how to read from and write to Delta Lake tables from interactive, batch, and streaming queries.\n",
							"\n",
							"Here's what we will cover:\n",
							"* Create a table\n",
							"* Understanding meta-data\n",
							"* Read data\n",
							"* Update table data\n",
							"* Overwrite table data\n",
							"* Conditional update without overwrite\n",
							"* Read older versions of data using Time Travel\n",
							"* Write a stream of data to a table\n",
							"* Read a stream of changes from a table"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Configuration\n",
							"Make sure you modify this as appropriate."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val sessionId = scala.util.Random.nextInt(1000000)\n",
							"val deltaTablePath = s\"/delta/delta-table-$sessionId\";"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Create a table\n",
							"To create a Delta Lake table, write a DataFrame out in the **delta** format. You can use existing Spark SQL code and change the format from parquet, csv, json, and so on, to delta.\n",
							"\n",
							"These operations create a new Delta Lake table using the schema that was inferred from your DataFrame. For the full set of options available when you create a new Delta Lake table, see Create a table and Write to a table (subsequent cells in this notebook)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val data = spark.range(0, 5)\n",
							"data.show\n",
							"data.write.format(\"delta\").save(deltaTablePath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Understanding Meta-data\n",
							"\n",
							"In Delta Lake, meta-data is no different from data i.e., it is stored next to the data. Therefore, an interesting side-effect here is that you can peek into meta-data using regular Spark APIs. "
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.read.text(s\"$deltaTablePath/_delta_log/\").collect.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read data\n",
							"\n",
							"You read data in your Delta Lake table by specifying the path to the files: \"/tmp/delta-table-scala\""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val df = spark.read.format(\"delta\").load(deltaTablePath)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Update table data\n",
							"\n",
							"Delta Lake supports several operations to modify tables using standard DataFrame APIs. This example runs a batch job to overwrite the data in the table.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val data = spark.range(5, 10)\n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(deltaTablePath)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "markdown",
						"source": [
							"When you now inspect the meta-data, what you will notice is that the original data is over-written. Well, not in a true sense but appropriate entries are added to Delta's transaction log so it can provide an \"illusion\" that the original data was deleted. We can verify this by re-inspecting the meta-data. You will see several entries indicating reference removal to the original data."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.read.text(s\"$deltaTablePath/_delta_log/\").collect.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Save as catalog tables\n",
							"\n",
							"Delta Lake can write to managed or external catalog tables."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Write data to a new managed catalog table.\n",
							"data.write.format(\"delta\").saveAsTable(\"ManagedDeltaTable\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"source": [
							"// Define an external catalog table that points to the existing Delta Lake data in storage.\n",
							"spark.sql(s\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '$deltaTablePath'\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"source": [
							"// List the 2 new tables.\n",
							"spark.sql(\"SHOW TABLES\").show\n",
							"\n",
							"// Explore their properties.\n",
							"spark.sql(\"DESCRIBE EXTENDED ManagedDeltaTable\").show(truncate=false)\n",
							"spark.sql(\"DESCRIBE EXTENDED ExternalDeltaTable\").show(truncate=false)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Conditional update without overwrite\n",
							"\n",
							"Delta Lake provides programmatic APIs to conditional update, delete, and merge (upsert) data into tables. For more information on these operations, see [Table Deletes, Updates, and Merges](https://docs.delta.io/latest/delta-update.html)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"import io.delta.tables._\n",
							"import org.apache.spark.sql.functions._\n",
							"\n",
							"val deltaTable = DeltaTable.forPath(deltaTablePath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"source": [
							"// Update every even value by adding 100 to it\n",
							"deltaTable.update(\n",
							"  condition = expr(\"id % 2 == 0\"),\n",
							"  set = Map(\"id\" -> expr(\"id + 100\")))\n",
							"deltaTable.toDF.show"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"source": [
							"// Delete every even value\n",
							"deltaTable.delete(condition = expr(\"id % 2 == 0\"))\n",
							"deltaTable.toDF.show"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"source": [
							"// Upsert (merge) new data\n",
							"val newData = spark.range(0, 20).toDF\n",
							"\n",
							"deltaTable.as(\"oldData\").\n",
							"  merge(\n",
							"    newData.as(\"newData\"),\n",
							"    \"oldData.id = newData.id\").\n",
							"  whenMatched.\n",
							"  update(Map(\"id\" -> lit(-1))).\n",
							"  whenNotMatched.\n",
							"  insert(Map(\"id\" -> col(\"newData.id\"))).\n",
							"  execute()\n",
							"\n",
							"deltaTable.toDF.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "markdown",
						"source": [
							"## History\n",
							"Delta's most powerful feature is the ability to allow looking into history i.e., the changes that were made to the underlying Delta Table. The cell below shows how simple it is to inspect the history."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.history.show(false)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read older versions of data using Time Travel\n",
							"\n",
							"You can query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option.\n",
							"\n",
							"Once you run the cell below, you should see the first set of data, from before you overwrote it. Time Travel is an extremely powerful feature that takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table. Removing the version 0 option (or specifying version 1) would let you see the newer data again. For more information, see [Query an older snapshot of a table (time travel)](https://docs.delta.io/latest/delta-batch.html#deltatimetravel)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(deltaTablePath)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write a stream of data to a table\n",
							"\n",
							"You can also write to a Delta Lake table using Spark's Structured Streaming. The Delta Lake transaction log guarantees exactly-once processing, even when there are other streams or batch queries running concurrently against the table. By default, streams run in append mode, which adds new records to the table.\n",
							"\n",
							"For more information about Delta Lake integration with Structured Streaming, see [Table Streaming Reads and Writes](https://docs.delta.io/latest/delta-streaming.html).\n",
							"\n",
							"In the cells below, here's what we are doing:\n",
							"\n",
							"1. *Cell 28* Setup a simple Spark Structured Streaming job to generate a sequence and make the job write into our Delta Table\n",
							"2. *Cell 30* Show the newly appended data\n",
							"3. *Cell 31* Inspect history\n",
							"4. *Cell 32* Stop the structured streaming job\n",
							"5. *Cell 33* Inspect history <-- You'll notice appends have stopped"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val streamingDf = spark.readStream.format(\"rate\").load()\n",
							"val stream = streamingDf.select($\"value\" as \"id\").writeStream.format(\"delta\").option(\"checkpointLocation\", s\"/tmp/checkpoint-$sessionId\").start(deltaTablePath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read a stream of changes from a table\n",
							"\n",
							"While the stream is writing to the Delta Lake table, you can also read from that table as streaming source. For example, you can start another streaming query that prints all the changes made to the Delta Lake table."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.toDF.sort($\"id\".desc).show"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.history.show"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"source": [
							"stream.stop"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.history.show"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Compaction\n",
							"\n",
							"If a Delta Table is growing too large, you can compact it by repartitioning into a smaller number of files.\n",
							"\n",
							"The option `dataChange = false` is an optimization that tells Delta Lake to do the repartition without marking the underlying data as \"modified\". This ensures that any other concurrent operations (such as streaming reads/writes) aren't negatively impacted.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val partitionCount = 2\n",
							"\n",
							"spark.\n",
							"    read.\n",
							"    format(\"delta\").\n",
							"    load(deltaTablePath).\n",
							"    repartition(partitionCount).\n",
							"    write.\n",
							"    option(\"dataChange\", \"false\").\n",
							"    format(\"delta\").\n",
							"    mode(\"overwrite\").\n",
							"    save(deltaTablePath)    "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Convert Parquet to Delta\n",
							"You can do an in-place conversion from the Parquet format to Delta.## Cell title\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val parquetPath = s\"/parquet/parquet-table-$sessionId\"\n",
							"\n",
							"val data = spark.range(0,5)\n",
							"data.write.parquet(parquetPath)\n",
							"\n",
							"// Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(parquetPath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"source": [
							"DeltaTable.convertToDelta(spark, s\"parquet.`$parquetPath`\")\n",
							"\n",
							"// Confirm that the converted data is now in the Delta format\n",
							"DeltaTable.isDeltaTable(parquetPath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "markdown",
						"source": [
							"## SQL Support\n",
							"Delta supports table utility commands through SQL.  You can use SQL to:\n",
							"* Get a DeltaTable's history\n",
							"* Vacuum a DeltaTable\n",
							"* Convert a Parquet file to Delta\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(s\"DESCRIBE HISTORY delta.`$deltaTablePath`\").show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(s\"VACUUM delta.`$deltaTablePath`\").show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"source": [
							"val parquetId = scala.util.Random.nextInt(1000)\n",
							"val parquetPath = s\"/parquet/parquet-table-$sessionId-$parquetId\"\n",
							"\n",
							"val data = spark.range(0,5)\n",
							"data.write.parquet(parquetPath)\n",
							"\n",
							"// Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(parquetPath)\n",
							"\n",
							"// Use SQL to convert the parquet table to Delta\n",
							"spark.sql(s\"CONVERT TO DELTA parquet.`$parquetPath`\")\n",
							"\n",
							"DeltaTable.isDeltaTable(parquetPath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 56
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Getting Started with Delta Lake_db0')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3173a447-daef-45e0-b603-8fdca35b5463"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Hitchhiker's Guide to Delta Lake (Python)\n",
							"\n",
							"This tutorial has been adapted for more clarity from its original counterpart [here](https://docs.delta.io/latest/quick-start.html). This notebook helps you quickly explore the main features of [Delta Lake](https://github.com/delta-io/delta). It provides code snippets that show how to read from and write to Delta Lake tables from interactive, batch, and streaming queries.\n",
							"\n",
							"Here's what we will cover:\n",
							"* Create a table\n",
							"* Understanding meta-data\n",
							"* Read data\n",
							"* Update table data\n",
							"* Overwrite table data\n",
							"* Conditional update without overwrite\n",
							"* Read older versions of data using Time Travel\n",
							"* Write a stream of data to a table\n",
							"* Read a stream of changes from a table"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Configuration\n",
							"Make sure you modify this as appropriate."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"import random\n",
							"\n",
							"session_id = random.randint(0,1000000)\n",
							"delta_table_path = \"/delta/delta-table-{0}\".format(session_id)\n",
							"\n",
							"delta_table_path"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Create a table\n",
							"To create a Delta Lake table, write a DataFrame out in the **delta** format. You can use existing Spark SQL code and change the format from parquet, csv, json, and so on, to delta.\n",
							"\n",
							"These operations create a new Delta Lake table using the schema that was inferred from your DataFrame. For the full set of options available when you create a new Delta Lake table, see Create a table and Write to a table (subsequent cells in this notebook)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"data = spark.range(0,5)\n",
							"data.show()\n",
							"data.write.format(\"delta\").save(delta_table_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Understanding Meta-data\n",
							"\n",
							"In Delta Lake, meta-data is no different from data i.e., it is stored next to the data. Therefore, an interesting side-effect here is that you can peek into meta-data using regular Spark APIs. "
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"[log_line.value for log_line in spark.read.text(delta_table_path + \"/_delta_log/\").collect()]"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read data\n",
							"\n",
							"You read data in your Delta Lake table by specifying the path to the files."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.format(\"delta\").load(delta_table_path)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Update table data\n",
							"\n",
							"Delta Lake supports several operations to modify tables using standard DataFrame APIs. This example runs a batch job to overwrite the data in the table.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"data = spark.range(5,10)\n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"When you now inspect the meta-data, what you will notice is that the original data is over-written. Well, not in a true sense but appropriate entries are added to Delta's transaction log so it can provide an \"illusion\" that the original data was deleted. We can verify this by re-inspecting the meta-data. You will see several entries indicating reference removal to the original data."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"[log_line.value for log_line in spark.read.text(delta_table_path + \"/_delta_log/\").collect()]"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Save as catalog tables\n",
							"\n",
							"Delta Lake can write to managed or external catalog tables."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Write data to a new managed catalog table.\n",
							"data.write.format(\"delta\").saveAsTable(\"ManagedDeltaTable\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"# Define an external catalog table that points to the existing Delta Lake data in storage.\n",
							"spark.sql(\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '{0}'\".format(delta_table_path))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"# List the 2 new tables.\n",
							"spark.sql(\"SHOW TABLES\").show()\n",
							"\n",
							"# Explore their properties.\n",
							"spark.sql(\"DESCRIBE EXTENDED ManagedDeltaTable\").show(truncate=False)\n",
							"spark.sql(\"DESCRIBE EXTENDED ExternalDeltaTable\").show(truncate=False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Conditional update without overwrite\n",
							"\n",
							"Delta Lake provides programmatic APIs to conditional update, delete, and merge (upsert) data into tables. For more information on these operations, see [Table Deletes, Updates, and Merges](https://docs.delta.io/latest/delta-update.html)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from delta.tables import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"delta_table = DeltaTable.forPath(spark, delta_table_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"# Update every even value by adding 100 to it\n",
							"delta_table.update(\n",
							"  condition = expr(\"id % 2 == 0\"),\n",
							"  set = { \"id\": expr(\"id + 100\") })\n",
							"delta_table.toDF().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"# Delete every even value\n",
							"delta_table.delete(\"id % 2 == 0\")\n",
							"delta_table.toDF().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"# Upsert (merge) new data\n",
							"new_data = spark.range(0,20).alias(\"newData\")\n",
							"\n",
							"delta_table.alias(\"oldData\")\\\n",
							"    .merge(new_data.alias(\"newData\"), \"oldData.id = newData.id\")\\\n",
							"    .whenMatchedUpdate(set = { \"id\": lit(\"-1\")})\\\n",
							"    .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\") })\\\n",
							"    .execute()\n",
							"\n",
							"delta_table.toDF().show(100)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"source": [
							"## History\n",
							"Delta's most powerful feature is the ability to allow looking into history i.e., the changes that were made to the underlying Delta Table. The cell below shows how simple it is to inspect the history."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().show(20, 1000, False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read older versions of data using Time Travel\n",
							"\n",
							"You can query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option.\n",
							"\n",
							"Once you run the cell below, you should see the first set of data, from before you overwrote it. Time Travel is an extremely powerful feature that takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table. Removing the version 0 option (or specifying version 1) would let you see the newer data again. For more information, see [Query an older snapshot of a table (time travel)](https://docs.delta.io/latest/delta-batch.html#deltatimetravel)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write a stream of data to a table\n",
							"\n",
							"You can also write to a Delta Lake table using Spark's Structured Streaming. The Delta Lake transaction log guarantees exactly-once processing, even when there are other streams or batch queries running concurrently against the table. By default, streams run in append mode, which adds new records to the table.\n",
							"\n",
							"For more information about Delta Lake integration with Structured Streaming, see [Table Streaming Reads and Writes](https://docs.delta.io/latest/delta-streaming.html).\n",
							"\n",
							"In the cells below, here's what we are doing:\n",
							"\n",
							"1. *Cell 28* Setup a simple Spark Structured Streaming job to generate a sequence and make the job write into our Delta Table\n",
							"2. *Cell 30* Show the newly appended data\n",
							"3. *Cell 31* Inspect history\n",
							"4. *Cell 32* Stop the structured streaming job\n",
							"5. *Cell 33* Inspect history <-- You'll notice appends have stopped"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"streaming_df = spark.readStream.format(\"rate\").load()\n",
							"stream = streaming_df\\\n",
							"    .selectExpr(\"value as id\")\\\n",
							"    .writeStream\\\n",
							"    .format(\"delta\")\\\n",
							"    .option(\"checkpointLocation\", \"/tmp/checkpoint-{0}\".format(session_id))\\\n",
							"    .start(delta_table_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read a stream of changes from a table\n",
							"\n",
							"While the stream is writing to the Delta Lake table, you can also read from that table as streaming source. For example, you can start another streaming query that prints all the changes made to the Delta Lake table."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.toDF().sort(col(\"id\").desc()).show(100)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(20, 1000, False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"source": [
							"stream.stop()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(100, 1000, False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Compaction\n",
							"\n",
							"If a Delta Table is growing too large, you can compact it by repartitioning into a smaller number of files.\n",
							"\n",
							"The option `dataChange = false` is an optimization that tells Delta Lake to do the repartition without marking the underlying data as \"modified\". This ensures that any other concurrent operations (such as streaming reads/writes) aren't negatively impacted.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"partition_count = 2\n",
							"\n",
							"spark.read\\\n",
							"    .format(\"delta\")\\\n",
							"    .load(delta_table_path)\\\n",
							"    .repartition(partition_count)\\\n",
							"    .write.option(\"dataChange\", \"false\")\\\n",
							"    .format(\"delta\")\\\n",
							"    .mode(\"overwrite\")\\\n",
							"    .save(delta_table_path)    "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Convert Parquet to Delta\n",
							"You can do an in-place conversion from the Parquet format to Delta."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_path = \"/parquet/parquet-table-{0}\".format(session_id)\n",
							"\n",
							"data = spark.range(0,5)\n",
							"data.write.parquet(parquet_path)\n",
							"\n",
							"# Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"source": [
							"DeltaTable.convertToDelta(spark, \"parquet.`{0}`\".format(parquet_path))\n",
							"\n",
							"# Confirm that the converted data is now in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "markdown",
						"source": [
							"## SQL Support\n",
							"Delta supports table utility commands through SQL.  You can use SQL to:\n",
							"* Get a DeltaTable's history\n",
							"* Vacuum a DeltaTable\n",
							"* Convert a Parquet file to Delta\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DESCRIBE HISTORY delta.`{0}`\".format(delta_table_path)).show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"VACUUM delta.`{0}`\".format(delta_table_path)).show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_id = random.randint(0,1000)\n",
							"parquet_path = \"/parquet/parquet-table-{0}-{1}\".format(session_id, parquet_path)\n",
							"\n",
							"data = spark.range(0,5)\n",
							"data.write.parquet(parquet_path)\n",
							"\n",
							"# Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)\n",
							"\n",
							"# Use SQL to convert the parquet table to Delta\n",
							"spark.sql(\"CONVERT TO DELTA parquet.`{0}`\".format(parquet_path))\n",
							"\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 26
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Getting Started with Delta Lake_faw')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "scala"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5fc1b3e0-90fa-4e4d-b8b5-4c907b046b34"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "scala"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Hitchhiker's Guide to Delta Lake (Scala)\n",
							"\n",
							"This tutorial has been adapted for more clarity from its original counterpart [here](https://docs.delta.io/latest/quick-start.html). This notebook helps you quickly explore the main features of [Delta Lake](https://github.com/delta-io/delta). It provides code snippets that show how to read from and write to Delta Lake tables from interactive, batch, and streaming queries.\n",
							"\n",
							"Here's what we will cover:\n",
							"* Create a table\n",
							"* Understanding meta-data\n",
							"* Read data\n",
							"* Update table data\n",
							"* Overwrite table data\n",
							"* Conditional update without overwrite\n",
							"* Read older versions of data using Time Travel\n",
							"* Write a stream of data to a table\n",
							"* Read a stream of changes from a table"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Configuration\n",
							"Make sure you modify this as appropriate."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val sessionId = scala.util.Random.nextInt(1000000)\n",
							"val deltaTablePath = s\"/delta/delta-table-$sessionId\";"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Create a table\n",
							"To create a Delta Lake table, write a DataFrame out in the **delta** format. You can use existing Spark SQL code and change the format from parquet, csv, json, and so on, to delta.\n",
							"\n",
							"These operations create a new Delta Lake table using the schema that was inferred from your DataFrame. For the full set of options available when you create a new Delta Lake table, see Create a table and Write to a table (subsequent cells in this notebook)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val data = spark.range(0, 5)\n",
							"data.show\n",
							"data.write.format(\"delta\").save(deltaTablePath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Understanding Meta-data\n",
							"\n",
							"In Delta Lake, meta-data is no different from data i.e., it is stored next to the data. Therefore, an interesting side-effect here is that you can peek into meta-data using regular Spark APIs. "
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.read.text(s\"$deltaTablePath/_delta_log/\").collect.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read data\n",
							"\n",
							"You read data in your Delta Lake table by specifying the path to the files: \"/tmp/delta-table-scala\""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val df = spark.read.format(\"delta\").load(deltaTablePath)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Update table data\n",
							"\n",
							"Delta Lake supports several operations to modify tables using standard DataFrame APIs. This example runs a batch job to overwrite the data in the table.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val data = spark.range(5, 10)\n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(deltaTablePath)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"When you now inspect the meta-data, what you will notice is that the original data is over-written. Well, not in a true sense but appropriate entries are added to Delta's transaction log so it can provide an \"illusion\" that the original data was deleted. We can verify this by re-inspecting the meta-data. You will see several entries indicating reference removal to the original data."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.read.text(s\"$deltaTablePath/_delta_log/\").collect.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Save as catalog tables\n",
							"\n",
							"Delta Lake can write to managed or external catalog tables."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Write data to a new managed catalog table.\n",
							"data.write.format(\"delta\").saveAsTable(\"ManagedDeltaTable\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Define an external catalog table that points to the existing Delta Lake data in storage.\n",
							"spark.sql(s\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '$deltaTablePath'\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"// List the 2 new tables.\n",
							"spark.sql(\"SHOW TABLES\").show\n",
							"\n",
							"// Explore their properties.\n",
							"spark.sql(\"DESCRIBE EXTENDED ManagedDeltaTable\").show(truncate=false)\n",
							"spark.sql(\"DESCRIBE EXTENDED ExternalDeltaTable\").show(truncate=false)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Conditional update without overwrite\n",
							"\n",
							"Delta Lake provides programmatic APIs to conditional update, delete, and merge (upsert) data into tables. For more information on these operations, see [Table Deletes, Updates, and Merges](https://docs.delta.io/latest/delta-update.html)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"import io.delta.tables._\n",
							"import org.apache.spark.sql.functions._\n",
							"\n",
							"val deltaTable = DeltaTable.forPath(deltaTablePath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Update every even value by adding 100 to it\n",
							"deltaTable.update(\n",
							"  condition = expr(\"id % 2 == 0\"),\n",
							"  set = Map(\"id\" -> expr(\"id + 100\")))\n",
							"deltaTable.toDF.show"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Delete every even value\n",
							"deltaTable.delete(condition = expr(\"id % 2 == 0\"))\n",
							"deltaTable.toDF.show"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Upsert (merge) new data\n",
							"val newData = spark.range(0, 20).toDF\n",
							"\n",
							"deltaTable.as(\"oldData\").\n",
							"  merge(\n",
							"    newData.as(\"newData\"),\n",
							"    \"oldData.id = newData.id\").\n",
							"  whenMatched.\n",
							"  update(Map(\"id\" -> lit(-1))).\n",
							"  whenNotMatched.\n",
							"  insert(Map(\"id\" -> col(\"newData.id\"))).\n",
							"  execute()\n",
							"\n",
							"deltaTable.toDF.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"## History\n",
							"Delta's most powerful feature is the ability to allow looking into history i.e., the changes that were made to the underlying Delta Table. The cell below shows how simple it is to inspect the history."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.history.show(false)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read older versions of data using Time Travel\n",
							"\n",
							"You can query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option.\n",
							"\n",
							"Once you run the cell below, you should see the first set of data, from before you overwrote it. Time Travel is an extremely powerful feature that takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table. Removing the version 0 option (or specifying version 1) would let you see the newer data again. For more information, see [Query an older snapshot of a table (time travel)](https://docs.delta.io/latest/delta-batch.html#deltatimetravel)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(deltaTablePath)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write a stream of data to a table\n",
							"\n",
							"You can also write to a Delta Lake table using Spark's Structured Streaming. The Delta Lake transaction log guarantees exactly-once processing, even when there are other streams or batch queries running concurrently against the table. By default, streams run in append mode, which adds new records to the table.\n",
							"\n",
							"For more information about Delta Lake integration with Structured Streaming, see [Table Streaming Reads and Writes](https://docs.delta.io/latest/delta-streaming.html).\n",
							"\n",
							"In the cells below, here's what we are doing:\n",
							"\n",
							"1. *Cell 28* Setup a simple Spark Structured Streaming job to generate a sequence and make the job write into our Delta Table\n",
							"2. *Cell 30* Show the newly appended data\n",
							"3. *Cell 31* Inspect history\n",
							"4. *Cell 32* Stop the structured streaming job\n",
							"5. *Cell 33* Inspect history <-- You'll notice appends have stopped"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val streamingDf = spark.readStream.format(\"rate\").load()\n",
							"val stream = streamingDf.select($\"value\" as \"id\").writeStream.format(\"delta\").option(\"checkpointLocation\", s\"/tmp/checkpoint-$sessionId\").start(deltaTablePath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read a stream of changes from a table\n",
							"\n",
							"While the stream is writing to the Delta Lake table, you can also read from that table as streaming source. For example, you can start another streaming query that prints all the changes made to the Delta Lake table."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.toDF.sort($\"id\".desc).show"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.history.show"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"stream.stop"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.history.show"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Compaction\n",
							"\n",
							"If a Delta Table is growing too large, you can compact it by repartitioning into a smaller number of files.\n",
							"\n",
							"The option `dataChange = false` is an optimization that tells Delta Lake to do the repartition without marking the underlying data as \"modified\". This ensures that any other concurrent operations (such as streaming reads/writes) aren't negatively impacted.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val partitionCount = 2\n",
							"\n",
							"spark.\n",
							"    read.\n",
							"    format(\"delta\").\n",
							"    load(deltaTablePath).\n",
							"    repartition(partitionCount).\n",
							"    write.\n",
							"    option(\"dataChange\", \"false\").\n",
							"    format(\"delta\").\n",
							"    mode(\"overwrite\").\n",
							"    save(deltaTablePath)    "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Convert Parquet to Delta\n",
							"You can do an in-place conversion from the Parquet format to Delta.## Cell title\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val parquetPath = s\"/parquet/parquet-table-$sessionId\"\n",
							"\n",
							"val data = spark.range(0,5)\n",
							"data.write.parquet(parquetPath)\n",
							"\n",
							"// Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(parquetPath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DeltaTable.convertToDelta(spark, s\"parquet.`$parquetPath`\")\n",
							"\n",
							"// Confirm that the converted data is now in the Delta format\n",
							"DeltaTable.isDeltaTable(parquetPath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## SQL Support\n",
							"Delta supports table utility commands through SQL.  You can use SQL to:\n",
							"* Get a DeltaTable's history\n",
							"* Vacuum a DeltaTable\n",
							"* Convert a Parquet file to Delta\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(s\"DESCRIBE HISTORY delta.`$deltaTablePath`\").show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(s\"VACUUM delta.`$deltaTablePath`\").show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"val parquetId = scala.util.Random.nextInt(1000)\n",
							"val parquetPath = s\"/parquet/parquet-table-$sessionId-$parquetId\"\n",
							"\n",
							"val data = spark.range(0,5)\n",
							"data.write.parquet(parquetPath)\n",
							"\n",
							"// Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(parquetPath)\n",
							"\n",
							"// Use SQL to convert the parquet table to Delta\n",
							"spark.sql(s\"CONVERT TO DELTA parquet.`$parquetPath`\")\n",
							"\n",
							"DeltaTable.isDeltaTable(parquetPath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Getting Started with Delta Lake_wvp')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5a9f0d81-41b5-47b5-ba74-01543ba3f155"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparkdotnet",
						"display_name": "csharp"
					},
					"language_info": {
						"name": "csharp"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Hitchhiker's Guide to Delta Lake (.NET for Spark C#)\n",
							"\n",
							"This tutorial has been adapted for more clarity from its original counterpart [here](https://docs.delta.io/latest/quick-start.html). This notebook helps you quickly explore the main features of [Delta Lake](https://github.com/delta-io/delta). It provides code snippets that show how to read from and write to Delta Lake tables from interactive, batch, and streaming queries.\n",
							"\n",
							"Here's what we will cover:\n",
							"* Create a table\n",
							"* Understanding meta-data\n",
							"* Read data\n",
							"* Update table data\n",
							"* Overwrite table data\n",
							"* Conditional update without overwrite\n",
							"* Read older versions of data using Time Travel\n",
							"* Write a stream of data to a table\n",
							"* Read a stream of changes from a table"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Configuration\n",
							"Make sure you modify this as appropriate."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"var sessionId = (new Random()).Next(10000000);\n",
							"var deltaTablePath = $\"/delta/delta-table-{sessionId}\";\n",
							"\n",
							"deltaTablePath"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Create a table\n",
							"To create a Delta Lake table, write a DataFrame out in the **delta** format. You can use existing Spark SQL code and change the format from parquet, csv, json, and so on, to delta.\n",
							"\n",
							"These operations create a new Delta Lake table using the schema that was inferred from your DataFrame. For the full set of options available when you create a new Delta Lake table, see Create a table and Write to a table (subsequent cells in this notebook)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"var data = spark.Range(0,5);\n",
							"data.Show();\n",
							"data.Write().Format(\"delta\").Save(deltaTablePath);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Understanding Meta-data\n",
							"\n",
							"In Delta Lake, meta-data is no different from data i.e., it is stored next to the data. Therefore, an interesting side-effect here is that you can peek into meta-data using regular Spark APIs. "
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"using System.Linq;\n",
							"spark.Read().Text($\"{deltaTablePath}/_delta_log/\").Collect().ToList().ForEach(x => Console.WriteLine(x.GetAs<string>(\"value\")));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read data\n",
							"\n",
							"You read data in your Delta Lake table by specifying the path to the files."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"var df = spark.Read().Format(\"delta\").Load(deltaTablePath);\n",
							"df.Show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Update table data\n",
							"\n",
							"Delta Lake supports several operations to modify tables using standard DataFrame APIs. This example runs a batch job to overwrite the data in the table.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"var data = spark.Range(5,10);\n",
							"data.Write().Format(\"delta\").Mode(\"overwrite\").Save(deltaTablePath);\n",
							"df.Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"source": [
							"When you now inspect the meta-data, what you will notice is that the original data is over-written. Well, not in a true sense but appropriate entries are added to Delta's transaction log so it can provide an \"illusion\" that the original data was deleted. We can verify this by re-inspecting the meta-data. You will see several entries indicating reference removal to the original data."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.Read().Text($\"{deltaTablePath}/_delta_log/\").Collect().ToList().ForEach(x => Console.WriteLine(x.GetAs<string>(\"value\")));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Save as catalog tables\n",
							"\n",
							"Delta Lake can write to managed or external catalog tables.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Write data to a new managed catalog table.\n",
							"data.Write().Format(\"delta\").SaveAsTable(\"ManagedDeltaTable\");"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"// Define an external catalog table that points to the existing Delta Lake data in storage.\n",
							"spark.Sql($\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '{deltaTablePath}'\");"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"// List the 2 new tables.\n",
							"spark.Sql(\"SHOW TABLES\").Show();\n",
							"\n",
							"// Explore their properties.\n",
							"spark.Sql(\"DESCRIBE EXTENDED ManagedDeltaTable\").Show(truncate: 0);\n",
							"spark.Sql(\"DESCRIBE EXTENDED ExternalDeltaTable\").Show(truncate: 0);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Conditional update without overwrite\n",
							"\n",
							"Delta Lake provides programmatic APIs to conditional update, delete, and merge (upsert) data into tables. For more information on these operations, see [Table Deletes, Updates, and Merges](https://docs.delta.io/latest/delta-update.html)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"using Microsoft.Spark.Extensions.Delta;\n",
							"using Microsoft.Spark.Extensions.Delta.Tables;\n",
							"using Microsoft.Spark.Sql;\n",
							"using static Microsoft.Spark.Sql.Functions;\n",
							"\n",
							"var deltaTable = DeltaTable.ForPath(deltaTablePath);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Update every even value by adding 100 to it\n",
							"deltaTable.Update(\n",
							"  condition: Expr(\"id % 2 == 0\"),\n",
							"  set: new Dictionary<string, Column>(){{ \"id\", Expr(\"id + 100\") }});\n",
							"deltaTable.ToDF().Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Delete every even value\n",
							"deltaTable.Delete(condition: Expr(\"id % 2 == 0\"));\n",
							"deltaTable.ToDF().Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Upsert (merge) new data\n",
							"var newData = spark.Range(20).As(\"newData\");\n",
							"\n",
							"deltaTable\n",
							"    .As(\"oldData\")\n",
							"    .Merge(newData, \"oldData.id = newData.id\")\n",
							"    .WhenMatched()\n",
							"        .Update(new Dictionary<string, Column>() {{\"id\", Lit(\"-1\")}})\n",
							"    .WhenNotMatched()\n",
							"        .Insert(new Dictionary<string, Column>() {{\"id\", Col(\"newData.id\")}})\n",
							"    .Execute();\n",
							"\n",
							"deltaTable.ToDF().Show(100);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## History\n",
							"Delta's most powerful feature is the ability to allow looking into history i.e., the changes that were made to the underlying Delta Table. The cell below shows how simple it is to inspect the history."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.History().Show(20, 1000, false);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read older versions of data using Time Travel\n",
							"\n",
							"You can query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option.\n",
							"\n",
							"Once you run the cell below, you should see the first set of data, from before you overwrote it. Time Travel is an extremely powerful feature that takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table. Removing the version 0 option (or specifying version 1) would let you see the newer data again. For more information, see [Query an older snapshot of a table (time travel)](https://docs.delta.io/latest/delta-batch.html#deltatimetravel)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"var df = spark.Read().Format(\"delta\").Option(\"versionAsOf\", 0).Load(deltaTablePath);\n",
							"df.Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write a stream of data to a table\n",
							"\n",
							"You can also write to a Delta Lake table using Spark's Structured Streaming. The Delta Lake transaction log guarantees exactly-once processing, even when there are other streams or batch queries running concurrently against the table. By default, streams run in append mode, which adds new records to the table.\n",
							"\n",
							"For more information about Delta Lake integration with Structured Streaming, see [Table Streaming Reads and Writes](https://docs.delta.io/latest/delta-streaming.html).\n",
							"\n",
							"In the cells below, here's what we are doing:\n",
							"\n",
							"1. *Cell 28* Setup a simple Spark Structured Streaming job to generate a sequence and make the job write into our Delta Table\n",
							"2. *Cell 30* Show the newly appended data\n",
							"3. *Cell 31* Inspect history\n",
							"4. *Cell 32* Stop the structured streaming job\n",
							"5. *Cell 33* Inspect history <-- You'll notice appends have stopped"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"var streamingDf = spark.ReadStream().Format(\"rate\").Load();\n",
							"var stream = streamingDf.SelectExpr(\"value as id\").WriteStream().Format(\"delta\").Option(\"checkpointLocation\", $\"/tmp/checkpoint-{sessionId}\").Start(deltaTablePath);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read a stream of changes from a table\n",
							"\n",
							"While the stream is writing to the Delta Lake table, you can also read from that table as streaming source. For example, you can start another streaming query that prints all the changes made to the Delta Lake table."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.ToDF().Sort(Col(\"id\").Desc()).Show(100);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.History().Drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").Show(20, 1000, false);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"stream.Stop();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.History().Drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").Show(100, 1000, false);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Compaction\n",
							"\n",
							"If a Delta Table is growing too large, you can compact it by repartitioning into a smaller number of files.\n",
							"\n",
							"The option `dataChange = false` is an optimization that tells Delta Lake to do the repartition without marking the underlying data as \"modified\". This ensures that any other concurrent operations (such as streaming reads/writes) aren't negatively impacted.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"int partitionCount = 2;\n",
							"\n",
							"spark.Read()\n",
							"    .Format(\"delta\")\n",
							"    .Load(deltaTablePath)\n",
							"    .Repartition(partitionCount)\n",
							"    .Write()\n",
							"    .Option(\"dataChange\", \"false\")\n",
							"    .Format(\"delta\")\n",
							"    .Mode(\"overwrite\")\n",
							"    .Save(deltaTablePath);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Convert Parquet to Delta\n",
							"You can do an in-place conversion from the Parquet format to Delta."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"var parquetPath = $\"/parquet/parquet-table-{sessionId}\";\n",
							"\n",
							"var data = spark.Range(0,5);\n",
							"data.Write().Parquet(parquetPath);\n",
							"\n",
							"// Confirm that the data isn't in the Delta format\n",
							"DeltaTable.IsDeltaTable(parquetPath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DeltaTable.ConvertToDelta(spark, $\"parquet.`{parquetPath}`\");\n",
							"\n",
							"//Confirm that the converted data is now in the Delta format\n",
							"DeltaTable.IsDeltaTable(parquetPath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## SQL Support\n",
							"Delta supports table utility commands through SQL.  You can use SQL to:\n",
							"* Get a DeltaTable's history\n",
							"* Vacuum a DeltaTable\n",
							"* Convert a Parquet file to Delta\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.Sql($\"DESCRIBE HISTORY delta.`{deltaTablePath}`\").Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.Sql($\"VACUUM delta.`{deltaTablePath}`\").Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"var parquetId =  (new Random()).Next(10000000);\n",
							"var parquetPath = $\"/parquet/parquet-table-{sessionId}-{parquetId}\";\n",
							"\n",
							"var data = spark.Range(0,5);\n",
							"data.Write().Parquet(parquetPath);\n",
							"\n",
							"// Confirm that the data isn't in the Delta format\n",
							"DeltaTable.IsDeltaTable(parquetPath);\n",
							"\n",
							"// Use SQL to convert the parquet table to Delta\n",
							"spark.Sql($\"CONVERT TO DELTA parquet.`{parquetPath}`\");\n",
							"\n",
							"DeltaTable.IsDeltaTable(parquetPath);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Getting Started with Hyperspace Indexing')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "python"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f5c37713-7f65-4677-8e70-52ade1ce6437"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Hitchhiker's Guide to Hyperspace (Python)\n",
							"## An Indexing Subsystem for Apache Spark\n",
							"\n",
							"<img src=\"https://raw.githubusercontent.com/rapoth/hyperspace/master/docs/assets/images/hyperspace-small-banner.png\" alt=\"Hyperspace Indexing Sub-System Logo\" width=\"1000\"/>\n",
							"\n",
							"[Hyperspace](https://github.com/microsoft/hyperspace) introduces the ability for Apache Spark users to create indexes on their datasets (e.g., CSV, JSON, Parquet etc.) and leverage them for potential query and workload acceleration.\n",
							"\n",
							"In this notebook, we highlight the basics of Hyperspace, emphasizing on its simplicity and show how it can be used by just anyone.\n",
							"\n",
							"**Disclaimer**: Hyperspace helps accelerate your workloads/queries under two circumstances:\n",
							"\n",
							"  1. Queries contain filters on predicates with high selectivity (e.g., you want to select 100 matching rows from a million candidate rows)\n",
							"  2. Queries contain a join that requires heavy-shuffles (e.g., you want to join a 100 GB dataset with a 10 GB dataset)\n",
							"\n",
							"You may want to carefully monitor your workloads and determine whether indexing is helping you on a case-by-case basis."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Setup\n",
							"To begin with, let's start a new Spark session. Since this notebook is a tutorial merely to illustrate what Hyperspace can offer, we will make a configuration change that allow us to highlight what Hyperspace is doing on small datasets. By default, Spark uses *broadcast join* to optimize join queries when the data size for one side of join is small (which is the case for the sample data we use in this tutorial). Therefore, we disable broadcast joins so that later when we run join queries, Spark uses *sort-merge* join. This is mainly to show how Hyperspace indexes would be used at scale for accelerating join queries.\n",
							"\n",
							"The output of running the cell below shows a reference to the successfully created Spark session and prints out '-1' as the value for the modified join config which indicates that broadcast join is successfully disabled."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import random\n",
							"\n",
							"session_id = random.randint(0,1000000)\n",
							"data_path = \"/hyperspace/data-{0}\".format(session_id)\n",
							"index_location = \"/hyperspace/indexes-{0}\".format(session_id)\n",
							"\n",
							"# Please note that you DO NOT need to change this configuration in production.\n",
							"# We store all indexes in the system folder within Synapse.\n",
							"spark.conf.set(\"spark.hyperspace.system.path\", index_location)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Start your Spark session\n",
							"spark\n",
							"\n",
							"# Disable BroadcastHashJoin, so Spark will use standard SortMergeJoin. Currently Hyperspace indexes utilize SortMergeJoin to speed up query.\n",
							"spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
							"\n",
							"# Verify that BroadcastHashJoin is set correctly \n",
							"print(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))\n",
							"spark.conf.set(\"spark.hyperspace.explain.displayMode\", \"html\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data Preparation\n",
							"\n",
							"To prepare our environment, we will create sample data records and save them as parquet data files. While we use Parquet for illustration, you can use other formats such as CSV. In the subsequent cells, we will also demonstrate how you can create several Hyperspace indexes on this sample dataset and how one can make Spark use them when running queries. \n",
							"\n",
							"Our example records correspond to two datasets: *department* and *employee*. You should configure \"empLocation\" and \"deptLocation\" paths so that on the storage account they point to your desired location to save generated data files. \n",
							"\n",
							"The output of running below cell shows contents of our datasets as lists of triplets followed by references to dataFrames created to save the content of each dataset in our preferred location."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
							"\n",
							"# Sample department records\n",
							"departments = [(10, \"Accounting\", \"New York\"), (20, \"Research\", \"Dallas\"), (30, \"Sales\", \"Chicago\"), (40, \"Operations\", \"Boston\")]\n",
							"\n",
							"# Sample employee records\n",
							"employees = [(7369, \"SMITH\", 20), (7499, \"ALLEN\", 30), (7521, \"WARD\", 30), (7566, \"JONES\", 20), (7698, \"BLAKE\", 30)]\n",
							"\n",
							"# Create a schema for the dataframe\n",
							"dept_schema = StructType([StructField('deptId', IntegerType(), True), StructField('deptName', StringType(), True), StructField('location', StringType(), True)])\n",
							"emp_schema = StructType([StructField('empId', IntegerType(), True), StructField('empName', StringType(), True), StructField('deptId', IntegerType(), True)])\n",
							"\n",
							"departments_df = spark.createDataFrame(departments, dept_schema)\n",
							"employees_df = spark.createDataFrame(employees, emp_schema)\n",
							"\n",
							"emp_Location = data_path + \"/employees.parquet\"\n",
							"dept_Location = data_path + \"/departments.parquet\"\n",
							"\n",
							"employees_df.write.mode(\"overwrite\").parquet(emp_Location)\n",
							"departments_df.write.mode(\"overwrite\").parquet(dept_Location)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Let's verify the contents of parquet files we created above to make sure they contain expected records in correct format. We later use these data files to create Hyperspace indexes and run sample queries.\n",
							"\n",
							"Running below cell, the output displays the rows in employee and department dataframes in a tabular form. There should be 14 employees and 4 departments, each matching with one of triplets we created in the previous cell."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# emp_Location and dept_Location are the user defined locations above to save parquet files\n",
							"emp_DF = spark.read.parquet(emp_Location)\n",
							"dept_DF = spark.read.parquet(dept_Location)\n",
							"\n",
							"# Verify the data is available and correct\n",
							"emp_DF.show()\n",
							"dept_DF.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Hello Hyperspace Index!\n",
							"Hyperspace lets users create indexes on records scanned from persisted data files. Once successfully created, an entry corresponding to the index is added to the Hyperspace's metadata. This metadata is later used by Apache Spark's Hyperspace-enabled optimizer during query processing to find and use proper indexes. \n",
							"\n",
							"Once indexes are created, users can perform several actions:\n",
							"  - **Refresh** If the underlying data changes, users can refresh an existing index to capture that. \n",
							"  - **Delete** If the index is not needed, users can perform a soft-delete i.e., index is not physically deleted but is marked as 'deleted' so it is no longer used in your workloads.\n",
							"  - **Vacuum** If an index is no longer required, users can vacuum it which forces a physical deletion of the index contents and associated metadata completely from Hyperspace's metadata.\n",
							"\n",
							"Below sections show how such index management operations can be done in Hyperspace.\n",
							"\n",
							"First, we need to import the required libraries and create an instance of Hyperspace. We later use this instance to invoke different Hyperspace APIs to create indexes on our sample data and modify those indexes.\n",
							"\n",
							"Output of running below cell shows a reference to the created instance of Hyperspace."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from hyperspace import *\n",
							"\n",
							"# Create an instance of Hyperspace\n",
							"hyperspace = Hyperspace(spark)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create Indexes\n",
							"To create a Hyperspace index, the user needs to provide 2 pieces of information:\n",
							"* An Apache Spark DataFrame which references the data to be indexed.\n",
							"* An index configuration object: IndexConfig, which specifies the *index name*, *indexed* and *included* columns of the index. \n",
							"\n",
							"As you might have noticed, in this notebook, we illustrate indexing using the [Covering Index](https://www.red-gate.com/simple-talk/sql/learn-sql-server/using-covering-indexes-to-improve-query-performance/), which are the default index in Hyperspace. In the future, we plan on adding support for other index types. \n",
							"\n",
							"We start by creating three Hyperspace indexes on our sample data: two indexes on the department dataset named \"deptIndex1\" and \"deptIndex2\", and one index on the employee dataset named 'empIndex'. \n",
							"For each index, we need a corresponding IndexConfig to capture the name along with columns lists for the indexed and included columns. Running below cell creates these indexConfigs and its output lists them.\n",
							"\n",
							"**Note**: An *index column* is a column that appears in your filters or join conditions. An *included column* is a column that appears in your select/project.\n",
							"\n",
							"For instance, in the following query:\n",
							"```sql\n",
							"SELECT X\n",
							"FROM Table\n",
							"WHERE Y = 2\n",
							"```\n",
							"X can be an *index column* and Y can be an *included column*."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Create index configurations\n",
							"\n",
							"emp_IndexConfig = IndexConfig(\"empIndex1\", [\"deptId\"], [\"empName\"])\n",
							"dept_IndexConfig1 = IndexConfig(\"deptIndex1\", [\"deptId\"], [\"deptName\"])\n",
							"dept_IndexConfig2 = IndexConfig(\"deptIndex2\", [\"location\"], [\"deptName\"])"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Now, we create three indexes using our index configurations. For this purpose, we invoke \"createIndex\" command on our Hyperspace instance. This command requires an index configuration and the dataFrame containing rows to be indexed.\n",
							"Running below cell creates three indexes.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Create indexes from configurations\n",
							"\n",
							"hyperspace.createIndex(emp_DF, emp_IndexConfig)\n",
							"hyperspace.createIndex(dept_DF, dept_IndexConfig1)\n",
							"hyperspace.createIndex(dept_DF, dept_IndexConfig2)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"### List Indexes\n",
							"\n",
							"Below code shows how a user can list all available indexes in a Hyperspace instance. It uses the `indexes` API which returns information about existing indexes as a Spark's DataFrame so you can perform additional operations. For instance, you can invoke valid operations on this DataFrame for checking its content or analyzing it further (for example filtering specific indexes or grouping them according to some desired property). \n",
							"\n",
							"Below cell uses DataFrame's `show` action to fully print the rows and show details of our indexes in a tabular form. For each index, we can see all the information Hyperspace has stored about it in its metadata. \n",
							"\n",
							"You will immediately notice the following:\n",
							"  - `config.indexName`, `config.indexedColumns`, `config.includedColumns` are the fields that a user normally provides during index creation.\n",
							"  - `status.status` indicates if the index is being actively used by the Spark's optimizer.\n",
							"  - `dfSignature` is automatically generated by Hyperspace and is unique for each index. Hyperspace uses this signature internally to maintain the index and exploit it at query time. \n",
							"  \n",
							"In the output below, all three indexes should have \"ACTIVE\" as status and their name, indexed columns, and included columns should match with what we defined in index configurations above."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.indexes().show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Delete Indexes\n",
							"A user can drop an existing index by using the `deleteIndex` API and providing the index name. \n",
							"\n",
							"Index deletion is a **soft-delete** operation i.e., only the index's status in the Hyperspace metadata from is changed from \"ACTIVE\" to \"DELETED\". This will exclude the deleted index from any future query optimization and Hyperspace no longer picks that index for any query. However, index files for a deleted index still remain available (since it is a soft-delete), so if you accidentally deleted the index, you could still restore it.\n",
							"\n",
							"The cell below deletes index with name \"deptIndex2\" and lists Hyperspace metadata after that. The output should be similar to above cell for \"List Indexes\" except for \"deptIndex2\" which now should have its status changed into \"DELETED\"."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.deleteIndex(\"deptIndex2\")\n",
							"\n",
							"hyperspace.indexes().show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Restore Indexes\n",
							"A user can use the `restoreIndex` API to restore a deleted index. This will bring back the latest version of index into ACTIVE status and makes it usable again for queries. \n",
							"\n",
							"The cell below shows an example of `restoreIndex` API. We delete \"deptIndex1\" and restore it. The output shows \"deptIndex1\" first went into the \"DELETED\" status after invoking \"deleteIndex\" command and came back to the \"ACTIVE\" status after calling \"restoreIndex\"."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.deleteIndex(\"deptIndex1\")\n",
							"\n",
							"hyperspace.indexes().show()\n",
							"\n",
							"hyperspace.restoreIndex(\"deptIndex1\")\n",
							"\n",
							"hyperspace.indexes().show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Vacuum Indexes\n",
							"The user can perform a **hard-delete** i.e., fully remove files and the metadata entry for a deleted index using the `vacuumIndex` API. Once done, this action is **irreversible** as it physically deletes all the index files associated with the index.\n",
							"\n",
							"The cell below vacuums the \"deptIndex2\" index and shows Hyperspace metadata after vaccuming. You should see metadata entries for two indexes \"deptIndex1\" and \"empIndex\" both with \"ACTIVE\" status and no entry for \"deptIndex2\"."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.vacuumIndex(\"deptIndex2\")\n",
							"hyperspace.indexes().show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Enable/Disable Hyperspace\n",
							"\n",
							"Hyperspace provides APIs to enable or disable index usage with Spark.\n",
							"\n",
							"  - By using `enableHyperspace` API, Hyperspace optimization rules become visible to the Apache Spark optimizer and it will exploit existing Hyperspace indexes to optimize user queries.\n",
							"  - By using `disableHyperspace` command, Hyperspace rules no longer apply during query optimization. You should note that disabling Hyperspace has no impact on created indexes as they remain intact.\n",
							"\n",
							"Below cell shows how you can use these commands to enable or disable hyperspace. The output simply shows a reference to the existing Spark session whose configuration is updated."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Enable Hyperspace\n",
							"Hyperspace.enable(spark)\n",
							"\n",
							"# Disable Hyperspace\n",
							"Hyperspace.disable(spark)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Index Usage\n",
							"In order to make Spark use Hyperspace indexes during query processing, the user needs to make sure that Hyperspace is enabled. \n",
							"\n",
							"The cell below enables Hyperspace and creates two DataFrames containing our sample data records which we use for running example queries. For each DataFrame, a few sample rows are printed."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Enable Hyperspace\n",
							"Hyperspace.enable(spark)\n",
							"\n",
							"emp_DF = spark.read.parquet(emp_Location)\n",
							"dept_DF = spark.read.parquet(dept_Location)\n",
							"\n",
							"emp_DF.show(5)\n",
							"dept_DF.show(5)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Hyperspace's Index Types\n",
							"\n",
							"Currently, Hyperspace can exploit indexes for two groups of queries: \n",
							"* Selection queries with lookup or range selection filtering predicates.\n",
							"* Join queries with an equality join predicate (i.e. Equi-joins)."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Indexes for Accelerating Filters\n",
							"\n",
							"Our first example query does a lookup on department records (see below cell). In SQL, this query looks as follows:\n",
							"\n",
							"```sql\n",
							"SELECT deptName \n",
							"FROM departments\n",
							"WHERE deptId = 20\n",
							"```\n",
							"\n",
							"The output of running the cell below shows: \n",
							"- query result, which is a single department name.\n",
							"- query plan that Spark used to run the query. \n",
							"\n",
							"In the query plan, the \"FileScan\" operator at the bottom of the plan shows the datasource where the records were read from. The location of this file indicates the path to the latest version of the \"deptIndex1\" index. This shows  that according to the query and using Hyperspace optimization rules, Spark decided to exploit the proper index at runtime.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Filter with equality predicate\n",
							"\n",
							"eqFilter = dept_DF.filter(\"\"\"deptId = 20\"\"\").select(\"\"\"deptName\"\"\")\n",
							"eqFilter.show()\n",
							"\n",
							"hyperspace.explain(eqFilter, True, displayHTML)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Our second example is a range selection query on department records. In SQL, this query looks as follows:\n",
							"\n",
							"```sql\n",
							"SELECT deptName \n",
							"FROM departments\n",
							"WHERE deptId > 20\n",
							"```\n",
							"Similar to our first example, the output of the cell below shows the query results (names of two departments) and the query plan. The location of data file in the FileScan operator shows that 'deptIndex1\" was used to run the query.   \n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Filter with range selection predicate\n",
							"\n",
							"rangeFilter = dept_DF.filter(\"\"\"deptId > 20\"\"\").select(\"deptName\")\n",
							"rangeFilter.show()\n",
							"\n",
							"hyperspace.explain(rangeFilter, True, displayHTML)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Our third example is a query joining department and employee records on the department id. The equivalent SQL statement is shown below:\n",
							"\n",
							"```sql\n",
							"SELECT employees.deptId, empName, departments.deptId, deptName\n",
							"FROM   employees, departments \n",
							"WHERE  employees.deptId = departments.deptId\n",
							"```\n",
							"\n",
							"The output of running the cell below shows the query results which are the names of 14 employees and the name of department each employee works in. The query plan is also included in the output. Notice how the file locations for two FileScan operators shows that Spark used \"empIndex\" and \"deptIndex1\" indexes to run the query.   \n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Join\n",
							"\n",
							"eqJoin = emp_DF.join(dept_DF, emp_DF.deptId == dept_DF.deptId).select(emp_DF.empName, dept_DF.deptName)\n",
							"\n",
							"eqJoin.show()\n",
							"\n",
							"hyperspace.explain(eqJoin, True, displayHTML)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Support for SQL Semantics\n",
							"\n",
							"The index usage is transparent to whether the user uses DataFrame API or Spark SQL. The following example shows the same join example as before but using Spark SQL, showing the use of indexes if applicable."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\n",
							"\n",
							"emp_DF.createOrReplaceTempView(\"EMP\")\n",
							"dept_DF.createOrReplaceTempView(\"DEPT\")\n",
							"\n",
							"joinQuery = spark.sql(\"SELECT EMP.empName, DEPT.deptName FROM EMP, DEPT WHERE EMP.deptId = DEPT.deptId\")\n",
							"\n",
							"joinQuery.show()\n",
							"hyperspace.explain(joinQuery, True, displayHTML)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Explain API\n",
							"\n",
							"So far, you might have observed we have been using the explain API provided by Hyperspace. The `explain` API from Hyperspace is very similar to Spark's `df.explain` API but allows users to compare their original plan vs the updated index-dependent plan before running their query. You have an option to choose from html/plaintext/console mode to display the command output. \n",
							"\n",
							"The following cell shows an example with HTML. The highlighted section represents the difference between original and updated plans along with the indexes being used."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"eqJoin = emp_DF.join(dept_DF, emp_DF.deptId == dept_DF.deptId).select(emp_DF.empName, dept_DF.deptName)\n",
							"\n",
							"spark.conf.set(\"spark.hyperspace.explain.displayMode\", \"html\")\n",
							"hyperspace.explain(eqJoin, True, displayHTML)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Refresh Indexes\n",
							"If the original data on which an index was created changes, then the index will no longer capture the latest state of data and hence will not be used by Hyperspace to provide any acceleration. The user can refresh such a stale index using the `refreshIndex` API. This causes the index to be fully rebuilt and updates it according to the latest data records.\n",
							"    \n",
							"    Spoiler alert: if you are worried about fully rebuilding your index every time your data changes, don't worry! We will show you how to *incrementally refresh* your index in subsequent cells below.\n",
							"\n",
							"The two cells below show an example for this scenario:\n",
							"- First cell adds two more departments to the original departments data. It reads and prints list of departments to verify new departments are added correctly. The output shows 6 departments in total: four old ones and two new. Invoking \"refreshIndex\" updates \"deptIndex1\" so index captures new departments.\n",
							"- Second cell runs our range selection query example. The results should now contain four departments: two are the ones, seen before when we ran the query above, and two are the new departments we just added."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"extra_Departments = [(50, \"Inovation\", \"Seattle\"), (60, \"Human Resources\", \"San Francisco\")]\n",
							"\n",
							"extra_departments_df = spark.createDataFrame(extra_Departments, dept_schema)\n",
							"extra_departments_df.write.mode(\"Append\").parquet(dept_Location)\n",
							"\n",
							"\n",
							"dept_DFrame_Updated = spark.read.parquet(dept_Location)\n",
							"\n",
							"dept_DFrame_Updated.show(10)\n",
							"\n",
							"hyperspace.refreshIndex(\"deptIndex1\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"newRangeFilter = dept_DFrame_Updated.filter(\"deptId > 20\").select(\"deptName\")\n",
							"newRangeFilter.show()\n",
							"\n",
							"hyperspace.explain(newRangeFilter, True, displayHTML)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.indexes().show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Clean-up the remaining indexes\n",
							"hyperspace.deleteIndex(\"empIndex1\")\n",
							"hyperspace.deleteIndex(\"deptIndex1\")\n",
							"\n",
							"hyperspace.vacuumIndex(\"empIndex1\")\n",
							"hyperspace.vacuumIndex(\"deptIndex1\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Hybrid Scan for Mutable Datasets\n",
							"\n",
							"Often times, if your underlying source data had some new files appended or existing files deleted, your index will get stale and Hyperspace decides not to use it. However, there are times where you just want to use the index without having to refresh it everytime. There could be multiple reasons for doing so:\n",
							"\n",
							"  1. You do not want to continuosly refresh your index but instead want to do it periodically since you understand your workloads the best.  \n",
							"  2. You added/removed only a few files and do not want to wait for yet another refresh job to finish. \n",
							"\n",
							"To allow you to still use a stale index, Hyperspace introduces **Hybrid Scan**, a novel technique that allows users to utilize outdated or stale indexes (e.g., the underlying source data had some new files appended or existing files deleted), without refreshing indexes. \n",
							"\n",
							"To achieve this, when you set the appropriate configuration to enable Hybrid Scan, Hyperspace modifies the query plan to leverage the changes as following:\n",
							"- Appended files can be merged to index data by using **`Union` or `BucketUnion` (for join)**. Shuffling appended data can also be applied before merging, if needed.\n",
							"- Deleted files can be handled by injecting `Filter-NOT-IN` condition on **lineage column** of index data, so that the indexed rows from the deleted files can be excluded at query time. \n",
							"\n",
							"You can check the transformation of the query plan in below examples.\n",
							"\n",
							"    Note: Hybrid scan is only supported for non-partitioned data. Support for partitioned data is currently being worked upon."
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Hybrid Scan for appended files - non-partitioned data\n",
							"\n",
							"Non-partitioned data is used in below example. In this example, we expect Join index can be used for the query and `BucketUnion` is introduced for appended files."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# GENERATE TEST DATA\n",
							"\n",
							"testdata = [\n",
							"    (\"orange\", 3, \"2020-10-01\"),\n",
							"    (\"banana\", 1, \"2020-10-01\"),\n",
							"    (\"carrot\", 5, \"2020-10-02\"),\n",
							"    (\"beetroot\", 12, \"2020-10-02\"),\n",
							"    (\"orange\", 2, \"2020-10-03\"),\n",
							"    (\"banana\", 11, \"2020-10-03\"),\n",
							"    (\"carrot\", 3, \"2020-10-03\"),\n",
							"    (\"beetroot\", 2, \"2020-10-04\"),\n",
							"    (\"cucumber\", 7, \"2020-10-05\"),\n",
							"    (\"pepper\", 20, \"2020-10-06\")\n",
							"]\n",
							"\n",
							"testdata_location = data_path + \"/productTable\"\n",
							"from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
							"testdata_schema = StructType([\n",
							"    StructField('name', StringType(), True),\n",
							"    StructField('qty', IntegerType(), True),\n",
							"    StructField('date', StringType(), True)])\n",
							"\n",
							"test_df = spark.createDataFrame(testdata, testdata_schema)\n",
							"test_df.write.mode(\"overwrite\").parquet(testdata_location)\n",
							"test_df = spark.read.parquet(testdata_location)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# CREATE INDEX\n",
							"hyperspace.createIndex(test_df, IndexConfig(\"productIndex2\", [\"name\"], [\"date\", \"qty\"]))\n",
							"\n",
							"spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
							"filter1 = test_df.filter(\"name = 'banana'\")\n",
							"filter2 = test_df.filter(\"qty > 10\")\n",
							"query = filter1.join(filter2, \"name\")\n",
							"\n",
							"# Check Join index rule is applied properly.\n",
							"hyperspace.explain(query, True, displayHTML)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Append new files.\r\n",
							"append_data = [\r\n",
							"    (\"orange\", 13, \"2020-11-01\"),\r\n",
							"    (\"banana\", 5, \"2020-11-01\")\r\n",
							"]\r\n",
							"append_df = spark.createDataFrame(append_data, testdata_schema)\r\n",
							"append_df.write.mode(\"append\").parquet(testdata_location)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Hybrid scan is disabled by default. Therefore, you will see that since we appended new data, Hyperspace will decide NOT to use the index.\n",
							"\n",
							"In the output, you will see no plan differences (hence no highlighting)."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Hybrid Scan configs are false by default.\n",
							"spark.conf.set(\"spark.hyperspace.index.hybridscan.enabled\", \"false\")\n",
							"spark.conf.set(\"spark.hyperspace.index.hybridscan.delete.enabled\", \"false\")\n",
							"\n",
							"test_df_with_append = spark.read.parquet(testdata_location)\n",
							"filter1 = test_df_with_append.filter(\"name = 'banana'\")\n",
							"filter2 = test_df_with_append.filter(\"qty > 10\")\n",
							"query = filter1.join(filter2, \"name\")\n",
							"hyperspace.explain(query, True, displayHTML)\n",
							"query.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Enable Hybrid Scan\r\n",
							"\r\n",
							"In plan with indexes, you can see\r\n",
							"`Exchange hashpartitioning` required only for appended files so that we could still utilize the \"shuffled\" index data with appended files. `BucketUnion` is used to merge \"shuffled\" appended files with the index data."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Enable Hybrid Scan config. \"delete\" config is not necessary.\n",
							"spark.conf.set(\"spark.hyperspace.index.hybridscan.enabled\", \"true\")\n",
							"# spark.conf.set(\"spark.hyperspace.index.hybridscan.delete.enabled\", \"true\")\n",
							"\n",
							"# Need to redefine query to recalculate the query plan.\n",
							"query = filter1.join(filter2, \"name\")\n",
							"hyperspace.explain(query, True, displayHTML)\n",
							"query.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Incremental Index Refresh\n",
							"When you ready to update your indexes but do not want to rebuild your entire index, Hyperspace supports updating indexes in an incremental manner using `hs.refreshIndex(\"name\", \"incremental\")` API. This will allow eliminate the need for a full rebuild of index from scratch, utilizing previously created index files as well as updating indexes on only the newly added data.\n",
							"\n",
							"Of course, please be sure to use the complementary `optimizeIndex` API (shown below) periodically to make sure you do not see performance regressions. We recommend calling `optimize` at least once for every 10 times you call `refreshIndex(..., \"incremental\")`, assuming the data you added/removed is < 10% of the original dataset. For instance, if your original dataset is 100 GB, and you've added/removed data in increments/decrements of 1 GB, you can call `refreshIndex` 10 times before calling `optimizeIndex`. Please note that this example is simply used for illustration and you have to adapt this for your workloads.\n",
							"\n",
							"In the example below, notice the addition of a `Sort` node in the query plan when indexes are used. This is because partial indexes are created on the appended data files, causing Spark to introduce a `Sort`. Please also note that `Shuffle` i.e. `Exchange` is still eliminated from the plan, giving you the appropriate acceleration."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"def query():\n",
							"    test_df_with_append = spark.read.parquet(testdata_location)\n",
							"    filter1 = test_df_with_append.filter(\"name = 'banana'\")\n",
							"    filter2 = test_df_with_append.filter(\"qty > 10\")\n",
							"    return filter1.join(filter2, \"name\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.refreshIndex(\"productIndex2\", \"incremental\")\n",
							"\n",
							"hyperspace.explain(query(), True, displayHTML)\n",
							"query().show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Optimize Index layout\n",
							"After calling incremental refreshes multiple times on newly appended data (e.g. if the user writes to data in small batches or in case of streaming scenarios), the number of index files tend to become large affecting the performance of the index (large number of small files problem). Hyperspace provides `hyperspace.optimizeIndex(\"indexName\")` API to optimize the index layout and reduce the large files problem.\n",
							"\n",
							"In the plan below, notice that Hyperspace has removed the additional `Sort` node in the query plan. Optimize can help avoiding sorting for any index bucket which contains only one file. However, this will only be true if ALL the index buckets have at most 1 file per bucket, after `optimizeIndex`."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Append some more data and call refresh again.\n",
							"append_data = [\n",
							"    (\"orange\", 13, \"2020-11-01\"),\n",
							"    (\"banana\", 5, \"2020-11-01\")\n",
							"]\n",
							"append_df = spark.createDataFrame(append_data, testdata_schema)\n",
							"append_df.write.mode(\"append\").parquet(testdata_location)\n",
							"\n",
							"hyperspace.refreshIndex(\"productIndex2\", \"incremental\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Call optimize. Ensure that Sort is removed after optimization (This is possible here because after optimize, in this case, every bucket contains only 1 file.).\n",
							"hyperspace.optimizeIndex(\"productIndex2\")\n",
							"\n",
							"hyperspace.explain(query(), True, displayHTML)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Optimize modes\n",
							"\n",
							"The default mode for optimization is \"quick\" mode where files smaller than a predefined threshold are picked for optmization. To maximize the effect of optimization, Hyperspace allows another optimize mode \"full\" as shown below. This mode picks ALL index files for optimization irrespective of their file size and creates the best possible layout of the index. This is also slower than the default optimize mode because more data is being processed here.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.optimizeIndex(\"productIndex2\", \"full\")\n",
							"\n",
							"hyperspace.explain(query(), True, displayHTML)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Clean Up\n",
							"To make this notebook self-contained and not leave any dangling data, we have some small clean-up code below. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.fs.rm(data_path, True)\n",
							"mssparkutils.fs.rm(index_location, True)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Getting Started with Hyperspace Indexing_ifb')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7774b11e-cf31-4482-a61e-5ed825477010"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparkdotnet",
						"display_name": "Synapse SparkDotNet"
					},
					"language_info": {
						"name": "csharp"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Hitchhiker's Guide to Hyperspace (.NET for Spark C#)\n",
							"## An Indexing Subsystem for Apache Spark\n",
							"\n",
							"<img src=\"https://raw.githubusercontent.com/rapoth/hyperspace/master/docs/assets/images/hyperspace-small-banner.png\" alt=\"Hyperspace Indexing Sub-System Logo\" width=\"1000\"/>\n",
							"\n",
							"[Hyperspace](https://github.com/microsoft/hyperspace) introduces the ability for Apache Spark users to create indexes on their datasets (e.g., CSV, JSON, Parquet etc.) and leverage them for potential query and workload acceleration.\n",
							"\n",
							"In this notebook, we highlight the basics of Hyperspace, emphasizing on its simplicity and show how it can be used by just anyone.\n",
							"\n",
							"**Disclaimer**: Hyperspace helps accelerate your workloads/queries under two circumstances:\n",
							"\n",
							"  1. Queries contain filters on predicates with high selectivity (e.g., you want to select 100 matching rows from a million candidate rows)\n",
							"  2. Queries contain a join that requires heavy-shuffles (e.g., you want to join a 100 GB dataset with a 10 GB dataset)\n",
							"\n",
							"You may want to carefully monitor your workloads and determine whether indexing is helping you on a case-by-case basis."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Setup\n",
							"To begin with, let's start a new Spark session. Since this notebook is a tutorial merely to illustrate what Hyperspace can offer, we will make a configuration change that allow us to highlight what Hyperspace is doing on small datasets. By default, Spark uses *broadcast join* to optimize join queries when the data size for one side of join is small (which is the case for the sample data we use in this tutorial). Therefore, we disable broadcast joins so that later when we run join queries, Spark uses *sort-merge* join. This is mainly to show how Hyperspace indexes would be used at scale for accelerating join queries.\n",
							"\n",
							"The output of running the cell below shows a reference to the successfully created Spark session and prints out '-1' as the value for the modified join config which indicates that broadcast join is successfully disabled."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"var sessionId = (new Random()).Next(10000000);\n",
							"var dataPath = $\"/hyperspace/data-{sessionId}\";\n",
							"var indexLocation = $\"/hyperspace/indexes-{sessionId}\";\n",
							"\n",
							"// Please note that you DO NOT need to change this configuration in production.\n",
							"// We store all indexes in the system folder within Synapse.\n",
							"spark.Conf().Set(\"spark.hyperspace.system.path\", indexLocation);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"// Disable BroadcastHashJoin, so Spark will use standard SortMergeJoin. Currently hyperspace indexes utilize SortMergeJoin to speed up query.\n",
							"spark.Conf().Set(\"spark.sql.autoBroadcastJoinThreshold\", -1);\n",
							"\n",
							"// Verify that BroadcastHashJoin is set correctly \n",
							"Console.WriteLine(spark.Conf().Get(\"spark.sql.autoBroadcastJoinThreshold\"));\n",
							"\n",
							"spark.Conf().Set(\"spark.hyperspace.explain.displayMode\", \"html\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data Preparation\n",
							"\n",
							"To prepare our environment, we will create sample data records and save them as parquet data files. While we use Parquet for illustration, you can use other formats such as CSV. In the subsequent cells, we will also demonstrate how you can create several Hyperspace indexes on this sample dataset and how one can make Spark use them when running queries. \n",
							"\n",
							"Our example records correspond to two datasets: *department* and *employee*. You should configure \"empLocation\" and \"deptLocation\" paths so that on the storage account they point to your desired location to save generated data files. \n",
							"\n",
							"The output of running below cell shows contents of our datasets as lists of triplets followed by references to dataFrames created to save the content of each dataset in our preferred location."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"using Microsoft.Spark.Sql.Types;\n",
							"\n",
							"// Sample department records\n",
							"var departments = new List<GenericRow>()\n",
							"{\n",
							"    new GenericRow(new object[] {10, \"Accounting\", \"New York\"}),\n",
							"    new GenericRow(new object[] {20, \"Research\", \"Dallas\"}),\n",
							"    new GenericRow(new object[] {30, \"Sales\", \"Chicago\"}),\n",
							"    new GenericRow(new object[] {40, \"Operations\", \"Boston\"})\n",
							"};\n",
							"\n",
							"// Sample employee records\n",
							"var employees = new List<GenericRow>() {\n",
							"      new GenericRow(new object[] {7369, \"SMITH\", 20}),\n",
							"      new GenericRow(new object[] {7499, \"ALLEN\", 30}),\n",
							"      new GenericRow(new object[] {7521, \"WARD\", 30}),\n",
							"      new GenericRow(new object[] {7566, \"JONES\", 20}),\n",
							"      new GenericRow(new object[] {7698, \"BLAKE\", 30}),\n",
							"      new GenericRow(new object[] {7782, \"CLARK\", 10}),\n",
							"      new GenericRow(new object[] {7788, \"SCOTT\", 20}),\n",
							"      new GenericRow(new object[] {7839, \"KING\", 10}),\n",
							"      new GenericRow(new object[] {7844, \"TURNER\", 30}),\n",
							"      new GenericRow(new object[] {7876, \"ADAMS\", 20}),\n",
							"      new GenericRow(new object[] {7900, \"JAMES\", 30}),\n",
							"      new GenericRow(new object[] {7934, \"MILLER\", 10}),\n",
							"      new GenericRow(new object[] {7902, \"FORD\", 20}),\n",
							"      new GenericRow(new object[] {7654, \"MARTIN\", 30})\n",
							"};\n",
							"\n",
							"// Save sample data in the Parquet format\n",
							"var departmentSchema = new StructType(new List<StructField>()\n",
							"{\n",
							"    new StructField(\"deptId\", new IntegerType()),\n",
							"    new StructField(\"deptName\", new StringType()),\n",
							"    new StructField(\"location\", new StringType())\n",
							"});\n",
							"var employeeSchema = new StructType(new List<StructField>()\n",
							"{\n",
							"    new StructField(\"empId\", new IntegerType()),\n",
							"    new StructField(\"empName\", new StringType()),\n",
							"    new StructField(\"deptId\", new IntegerType())\n",
							"});\n",
							"\n",
							"DataFrame empData = spark.CreateDataFrame(employees, employeeSchema); \n",
							"DataFrame deptData = spark.CreateDataFrame(departments, departmentSchema); \n",
							"\n",
							"string empLocation = $\"{dataPath}/employees.parquet\";\n",
							"string deptLocation = $\"{dataPath}/departments.parquet\";\n",
							"empData.Write().Mode(\"overwrite\").Parquet(empLocation);\n",
							"deptData.Write().Mode(\"overwrite\").Parquet(deptLocation);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"Let's verify the contents of parquet files we created above to make sure they contain expected records in correct format. We later use these data files to create Hyperspace indexes and run sample queries.\n",
							"\n",
							"Running below cell, the output displays the rows in employee and department dataframes in a tabular form. There should be 14 employees and 4 departments, each matching with one of triplets we created in the previous cell."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// empLocation and deptLocation are the user defined locations above to save parquet files\n",
							"DataFrame empDF = spark.Read().Parquet(empLocation);\n",
							"DataFrame deptDF = spark.Read().Parquet(deptLocation);\n",
							"\n",
							"// Verify the data is available and correct\n",
							"empDF.Show();\n",
							"deptDF.Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Hello Hyperspace Index!\n",
							"Hyperspace lets users create indexes on records scanned from persisted data files. Once successfully created, an entry corresponding to the index is added to the Hyperspace's metadata. This metadata is later used by Apache Spark's Hyperspace-enabled optimizer during query processing to find and use proper indexes. \n",
							"\n",
							"Once indexes are created, users can perform several actions:\n",
							"  - **Refresh** If the underlying data changes, users can refresh an existing index to capture that. \n",
							"  - **Delete** If the index is not needed, users can perform a soft-delete i.e., index is not physically deleted but is marked as 'deleted' so it is no longer used in your workloads.\n",
							"  - **Vacuum** If an index is no longer required, users can vacuum it which forces a physical deletion of the index contents and associated metadata completely from Hyperspace's metadata.\n",
							"\n",
							"Below sections show how such index management operations can be done in Hyperspace.\n",
							"\n",
							"First, we need to import the required libraries and create an instance of Hyperspace. We later use this instance to invoke different Hyperspace APIs to create indexes on our sample data and modify those indexes.\n",
							"\n",
							"Output of running below cell shows a reference to the created instance of Hyperspace."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Create an instance of Hyperspace\n",
							"using Microsoft.Spark.Extensions.Hyperspace;\n",
							"\n",
							"Hyperspace hyperspace = new Hyperspace(spark);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create Indexes\n",
							"To create a Hyperspace index, the user needs to provide 2 pieces of information:\n",
							"* An Apache Spark DataFrame which references the data to be indexed.\n",
							"* An index configuration object: IndexConfig, which specifies the *index name*, *indexed* and *included* columns of the index. \n",
							"\n",
							"As you might have noticed, in this notebook, we illustrate indexing using the [Covering Index](https://www.red-gate.com/simple-talk/sql/learn-sql-server/using-covering-indexes-to-improve-query-performance/), which are the default index in Hyperspace. In the future, we plan on adding support for other index types. \n",
							"\n",
							"We start by creating three Hyperspace indexes on our sample data: two indexes on the department dataset named \"deptIndex1\" and \"deptIndex2\", and one index on the employee dataset named 'empIndex'. \n",
							"For each index, we need a corresponding IndexConfig to capture the name along with columns lists for the indexed and included columns. Running below cell creates these indexConfigs and its output lists them.\n",
							"\n",
							"**Note**: An *index column* is a column that appears in your filters or join conditions. An *included column* is a column that appears in your select/project.\n",
							"\n",
							"For instance, in the following query:\n",
							"```sql\n",
							"SELECT X\n",
							"FROM Table\n",
							"WHERE Y = 2\n",
							"```\n",
							"X can be an *index column* and Y can be an *included column*."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Create index configurations\n",
							"using Microsoft.Spark.Extensions.Hyperspace.Index;\n",
							"\n",
							"var empIndexConfig = new IndexConfig(\"empIndex\", new string[] {\"deptId\"}, new string[] {\"empName\"});\n",
							"var deptIndexConfig1 = new IndexConfig(\"deptIndex1\", new string[] {\"deptId\"}, new string[] {\"deptName\"});\n",
							"var deptIndexConfig2 = new IndexConfig(\"deptIndex2\", new string[] {\"location\"}, new string[] {\"deptName\"});"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"Now, we create three indexes using our index configurations. For this purpose, we invoke \"createIndex\" command on our Hyperspace instance. This command requires an index configuration and the dataFrame containing rows to be indexed.\n",
							"Running below cell creates three indexes.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Create indexes from configurations\n",
							"hyperspace.CreateIndex(empDF, empIndexConfig);\n",
							"hyperspace.CreateIndex(deptDF, deptIndexConfig1);\n",
							"hyperspace.CreateIndex(deptDF, deptIndexConfig2);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"### List Indexes\n",
							"\n",
							"Below code shows how a user can list all available indexes in a Hyperspace instance. It uses the `indexes` API which returns information about existing indexes as a Spark's DataFrame so you can perform additional operations. For instance, you can invoke valid operations on this DataFrame for checking its content or analyzing it further (for example filtering specific indexes or grouping them according to some desired property). \n",
							"\n",
							"Below cell uses DataFrame's `show` action to fully print the rows and show details of our indexes in a tabular form. For each index, we can see all the information Hyperspace has stored about it in its metadata. \n",
							"\n",
							"You will immediately notice the following:\n",
							"  - `config.indexName`, `config.indexedColumns`, `config.includedColumns` are the fields that a user normally provides during index creation.\n",
							"  - `status.status` indicates if the index is being actively used by the Spark's optimizer.\n",
							"  - `dfSignature` is automatically generated by Hyperspace and is unique for each index. Hyperspace uses this signature internally to maintain the index and exploit it at query time. \n",
							"  \n",
							"In the output below, all three indexes should have \"ACTIVE\" as status and their name, indexed columns, and included columns should match with what we defined in index configurations above."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.Indexes().Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Delete Indexes\n",
							"A user can drop an existing index by using the `deleteIndex` API and providing the index name. \n",
							"\n",
							"Index deletion is a **soft-delete** operation i.e., only the index's status in the Hyperspace metadata from is changed from \"ACTIVE\" to \"DELETED\". This will exclude the deleted index from any future query optimization and Hyperspace no longer picks that index for any query. However, index files for a deleted index still remain available (since it is a soft-delete), so if you accidentally deleted the index, you could still restore it.\n",
							"\n",
							"The cell below deletes index with name \"deptIndex2\" and lists Hyperspace metadata after that. The output should be similar to above cell for \"List Indexes\" except for \"deptIndex2\" which now should have its status changed into \"DELETED\"."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.DeleteIndex(\"deptIndex2\");\n",
							"\n",
							"hyperspace.Indexes().Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Restore Indexes\n",
							"A user can use the `restoreIndex` API to restore a deleted index. This will bring back the latest version of index into ACTIVE status and makes it usable again for queries. \n",
							"\n",
							"The cell below shows an example of `restoreIndex` API. We delete \"deptIndex1\" and restore it. The output shows \"deptIndex1\" first went into the \"DELETED\" status after invoking \"deleteIndex\" command and came back to the \"ACTIVE\" status after calling \"restoreIndex\"."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.DeleteIndex(\"deptIndex1\");\n",
							"\n",
							"hyperspace.Indexes().Show();\n",
							"\n",
							"hyperspace.RestoreIndex(\"deptIndex1\");\n",
							"\n",
							"hyperspace.Indexes().Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Vacuum Indexes\n",
							"The user can perform a **hard-delete** i.e., fully remove files and the metadata entry for a deleted index using the `vacuumIndex` API. Once done, this action is **irreversible** as it physically deletes all the index files associated with the index.\n",
							"\n",
							"The cell below vacuums the \"deptIndex2\" index and shows Hyperspace metadata after vaccuming. You should see metadata entries for two indexes \"deptIndex1\" and \"empIndex\" both with \"ACTIVE\" status and no entry for \"deptIndex2\"."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.VacuumIndex(\"deptIndex2\");\n",
							"\n",
							"hyperspace.Indexes().Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Enable/Disable Hyperspace\n",
							"\n",
							"Hyperspace provides APIs to enable or disable index usage with Spark.\n",
							"\n",
							"  - By using `enableHyperspace` API, Hyperspace optimization rules become visible to the Apache Spark optimizer and it will exploit existing Hyperspace indexes to optimize user queries.\n",
							"  - By using `disableHyperspace` command, Hyperspace rules no longer apply during query optimization. You should note that disabling Hyperspace has no impact on created indexes as they remain intact.\n",
							"\n",
							"Below cell shows how you can use these commands to enable or disable hyperspace. The output simply shows a reference to the existing Spark session whose configuration is updated."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Enable Hyperspace\n",
							"spark.EnableHyperspace();\n",
							"\n",
							"// Disable Hyperspace\n",
							"spark.DisableHyperspace();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Index Usage\n",
							"In order to make Spark use Hyperspace indexes during query processing, the user needs to make sure that Hyperspace is enabled. \n",
							"\n",
							"The cell below enables Hyperspace and creates two DataFrames containing our sample data records which we use for running example queries. For each DataFrame, a few sample rows are printed."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Enable Hyperspace\n",
							"spark.EnableHyperspace();\n",
							"\n",
							"DataFrame empDFrame = spark.Read().Parquet(empLocation);\n",
							"DataFrame deptDFrame = spark.Read().Parquet(deptLocation);\n",
							"\n",
							"empDFrame.Show(5);\n",
							"deptDFrame.Show(5);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Hyperspace's Index Types\n",
							"\n",
							"Currently, Hyperspace can exploit indexes for two groups of queries: \n",
							"* Selection queries with lookup or range selection filtering predicates.\n",
							"* Join queries with an equality join predicate (i.e. Equi-joins)."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Indexes for Accelerating Filters\n",
							"\n",
							"Our first example query does a lookup on department records (see below cell). In SQL, this query looks as follows:\n",
							"\n",
							"```sql\n",
							"SELECT deptName \n",
							"FROM departments\n",
							"WHERE deptId = 20\n",
							"```\n",
							"\n",
							"The output of running the cell below shows: \n",
							"- query result, which is a single department name.\n",
							"- query plan that Spark used to run the query. \n",
							"\n",
							"In the query plan, the \"FileScan\" operator at the bottom of the plan shows the datasource where the records were read from. The location of this file indicates the path to the latest version of the \"deptIndex1\" index. This shows  that according to the query and using Hyperspace optimization rules, Spark decided to exploit the proper index at runtime.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Filter with equality predicate\n",
							"DataFrame eqFilter = deptDFrame.Filter(\"deptId = 20\").Select(\"deptName\");\n",
							"eqFilter.Show();\n",
							"\n",
							"hyperspace.Explain(eqFilter, true, input => DisplayHTML(input));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"source": [
							"Our second example is a range selection query on department records. In SQL, this query looks as follows:\n",
							"\n",
							"```sql\n",
							"SELECT deptName \n",
							"FROM departments\n",
							"WHERE deptId > 20\n",
							"```\n",
							"Similar to our first example, the output of the cell below shows the query results (names of two departments) and the query plan. The location of data file in the FileScan operator shows that 'deptIndex1\" was used to run the query.   \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Filter with range selection predicate\n",
							"DataFrame rangeFilter = deptDFrame.Filter(\"deptId > 20\").Select(\"deptName\");\n",
							"rangeFilter.Show();\n",
							"\n",
							"hyperspace.Explain(rangeFilter, true, input => DisplayHTML(input));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"source": [
							"Our third example is a query joining department and employee records on the department id. The equivalent SQL statement is shown below:\n",
							"\n",
							"```sql\n",
							"SELECT employees.deptId, empName, departments.deptId, deptName\n",
							"FROM   employees, departments \n",
							"WHERE  employees.deptId = departments.deptId\n",
							"```\n",
							"\n",
							"The output of running the cell below shows the query results which are the names of 14 employees and the name of department each employee works in. The query plan is also included in the output. Notice how the file locations for two FileScan operators shows that Spark used \"empIndex\" and \"deptIndex1\" indexes to run the query.   \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Join\n",
							"DataFrame eqJoin =\n",
							"      empDFrame\n",
							"      .Join(deptDFrame, empDFrame.Col(\"deptId\") == deptDFrame.Col(\"deptId\"))\n",
							"      .Select(empDFrame.Col(\"empName\"), deptDFrame.Col(\"deptName\"));\n",
							"\n",
							"eqJoin.Show();\n",
							"\n",
							"hyperspace.Explain(eqJoin, true, input => DisplayHTML(input));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Support for SQL Semantics\n",
							"\n",
							"The index usage is transparent to whether the user uses DataFrame API or Spark SQL. The following example shows the same join example as before but using Spark SQL, showing the use of indexes if applicable."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"empDFrame.CreateOrReplaceTempView(\"EMP\");\n",
							"deptDFrame.CreateOrReplaceTempView(\"DEPT\");\n",
							"\n",
							"var joinQuery = spark.Sql(\"SELECT EMP.empName, DEPT.deptName FROM EMP, DEPT WHERE EMP.deptId = DEPT.deptId\");\n",
							"\n",
							"joinQuery.Show();\n",
							"hyperspace.Explain(joinQuery, true, input => DisplayHTML(input));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Explain API\n",
							"\n",
							"So far, you might have observed we have been using the explain API provided by Hyperspace. The `explain` API from Hyperspace is very similar to Spark's `df.explain` API but allows users to compare their original plan vs the updated index-dependent plan before running their query. You have an option to choose from html/plaintext/console mode to display the command output. \n",
							"\n",
							"The following cell shows an example with HTML. The highlighted section represents the difference between original and updated plans along with the indexes being used."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.Conf().Set(\"spark.hyperspace.explain.displayMode\", \"html\");\n",
							"spark.Conf().Set(\"spark.hyperspace.explain.displayMode.highlight.beginTag\", \"<b style=\\\"background:LightGreen\\\">\");\n",
							"spark.Conf().Set(\"spark.hyperspace.explain.displayMode.highlight.endTag\", \"</b>\");\n",
							"\n",
							"hyperspace.Explain(eqJoin, true, input => DisplayHTML(input));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Refresh Indexes\n",
							"If the original data on which an index was created changes, then the index will no longer capture the latest state of data and hence will not be used by Hyperspace to provide any acceleration. The user can refresh such a stale index using the `refreshIndex` API. This causes the index to be fully rebuilt and updates it according to the latest data records.\n",
							"    \n",
							"    Spoiler alert: if you are worried about fully rebuilding your index every time your data changes, don't worry! We will show you how to *incrementally refresh* your index in subsequent cells below.\n",
							"\n",
							"The two cells below show an example for this scenario:\n",
							"- First cell adds two more departments to the original departments data. It reads and prints list of departments to verify new departments are added correctly. The output shows 6 departments in total: four old ones and two new. Invoking \"refreshIndex\" updates \"deptIndex1\" so index captures new departments.\n",
							"- Second cell runs our range selection query example. The results should now contain four departments: two are the ones, seen before when we ran the query above, and two are the new departments we just added."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"var extraDepartments = new List<GenericRow>()\n",
							"{\n",
							"    new GenericRow(new object[] {50, \"Inovation\", \"Seattle\"}),\n",
							"    new GenericRow(new object[] {60, \"Human Resources\", \"San Francisco\"})\n",
							"};\n",
							"\t  \n",
							"DataFrame extraDeptData = spark.CreateDataFrame(extraDepartments, departmentSchema);\n",
							"extraDeptData.Write().Mode(\"Append\").Parquet(deptLocation);\n",
							"\n",
							"DataFrame deptDFrameUpdated = spark.Read().Parquet(deptLocation);\n",
							"\n",
							"deptDFrameUpdated.Show(10);\n",
							"\n",
							"hyperspace.RefreshIndex(\"deptIndex1\");"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"source": [
							"DataFrame newRangeFilter = deptDFrameUpdated.Filter(\"deptId > 20\").Select(\"deptName\");\n",
							"newRangeFilter.Show();\n",
							"\n",
							"hyperspace.Explain(newRangeFilter, true, input => DisplayHTML(input));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Hybrid Scan for Mutable Datasets\n",
							"\n",
							"Often times, if your underlying source data had some new files appended or existing files deleted, your index will get stale and Hyperspace decides not to use it. However, there are times where you just want to use the index without having to refresh it everytime. There could be multiple reasons for doing so:\n",
							"\n",
							"  1. You do not want to continuosly refresh your index but instead want to do it periodically since you understand your workloads the best.  \n",
							"  2. You added/removed only a few files and do not want to wait for yet another refresh job to finish. \n",
							"\n",
							"To allow you to still use a stale index, Hyperspace introduces **Hybrid Scan**, a novel technique that allows users to utilize outdated or stale indexes (e.g., the underlying source data had some new files appended or existing files deleted), without refreshing indexes. \n",
							"\n",
							"To achieve this, when you set the appropriate configuration to enable Hybrid Scan, Hyperspace modifies the query plan to leverage the changes as following:\n",
							"- Appended files can be merged to index data by using **`Union` or `BucketUnion` (for join)**. Shuffling appended data can also be applied before merging, if needed.\n",
							"- Deleted files can be handled by injecting `Filter-NOT-IN` condition on **lineage column** of index data, so that the indexed rows from the deleted files can be excluded at query time. \n",
							"\n",
							"You can check the transformation of the query plan in below examples.\n",
							"\n",
							"    Note: Hybrid scan is only supported for non-partitioned data. Support for partitioned data is currently being worked upon."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Hybrid Scan for appended files - non-partitioned data\n",
							"\n",
							"Non-partitioned data is used in below example. In this example, we expect Join index can be used for the query and `BucketUnion` is introduced for appended files."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// GENERATE TEST DATA\n",
							"using Microsoft.Spark.Sql.Types;\n",
							"\n",
							"var products = new List<GenericRow>() {\n",
							"    new GenericRow(new object[] {\"orange\", 3, \"2020-10-01\"}),\n",
							"    new GenericRow(new object[] {\"banana\", 1, \"2020-10-01\"}),\n",
							"    new GenericRow(new object[] {\"carrot\", 5, \"2020-10-02\"}),\n",
							"    new GenericRow(new object[] {\"beetroot\", 12, \"2020-10-02\"}),\n",
							"    new GenericRow(new object[] {\"orange\", 2, \"2020-10-03\"}),\n",
							"    new GenericRow(new object[] {\"banana\", 11, \"2020-10-03\"}),\n",
							"    new GenericRow(new object[] {\"carrot\", 3, \"2020-10-03\"}),\n",
							"    new GenericRow(new object[] {\"beetroot\", 2, \"2020-10-04\"}),\n",
							"    new GenericRow(new object[] {\"cucumber\", 7, \"2020-10-05\"}),\n",
							"    new GenericRow(new object[] {\"pepper\", 20, \"2020-10-06\"})\n",
							"};\n",
							"var productsSchema = new StructType(new List<StructField>()\n",
							"{\n",
							"    new StructField(\"name\", new StringType()),\n",
							"    new StructField(\"qty\", new IntegerType()),\n",
							"    new StructField(\"date\", new StringType())\n",
							"});\n",
							"\n",
							"DataFrame testData = spark.CreateDataFrame(products, productsSchema); \n",
							"string testDataLocation = $\"{dataPath}/productTable\";\n",
							"testData.Write().Mode(\"overwrite\").Parquet(testDataLocation);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"source": [
							"// CREATE INDEX\n",
							"DataFrame testDF = spark.Read().Parquet(testDataLocation);\n",
							"var productIndex2Config = new IndexConfig(\"productIndex\", new string[] {\"name\"}, new string[] {\"date\", \"qty\"});\n",
							"hyperspace.CreateIndex(testDF, productIndex2Config);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"source": [
							"DataFrame filter1 = testDF.Filter(\"name = 'banana'\");\n",
							"DataFrame filter2 = testDF.Filter(\"qty > 10\");\n",
							"DataFrame query = filter1.Join(filter2, filter1.Col(\"name\") == filter2.Col(\"name\"));\n",
							"\n",
							"query.Show();\n",
							"\n",
							"hyperspace.Explain(query, true, input => DisplayHTML(input));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"source": [
							"// Append new files.\n",
							"var appendProducts = new List<GenericRow>()\n",
							"{\n",
							"    new GenericRow(new object[] {\"orange\", 13, \"2020-11-01\"}),\n",
							"    new GenericRow(new object[] {\"banana\", 5, \"2020-11-01\"})\n",
							"};\n",
							"    \n",
							"DataFrame appendData = spark.CreateDataFrame(appendProducts, productsSchema);\n",
							"appendData.Write().Mode(\"Append\").Parquet(testDataLocation);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "markdown",
						"source": [
							"Hybrid scan is disabled by default. Therefore, you will see that since we appended new data, Hyperspace will decide NOT to use the index.\n",
							"\n",
							"In the output, you will see no plan differences (hence no highlighting)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Hybrid Scan configs are false by default.\n",
							"spark.Conf().Set(\"spark.hyperspace.index.hybridscan.enabled\", \"false\");\n",
							"spark.Conf().Set(\"spark.hyperspace.index.hybridscan.delete.enabled\", \"false\");\n",
							"\n",
							"DataFrame testDFWithAppend = spark.Read().Parquet(testDataLocation);\n",
							"DataFrame filter1 = testDFWithAppend.Filter(\"name = 'banana'\");\n",
							"DataFrame filter2 = testDFWithAppend.Filter(\"qty > 10\");\n",
							"DataFrame query = filter1.Join(filter2, filter1.Col(\"name\") == filter2.Col(\"name\"));\n",
							"\n",
							"query.Show();\n",
							"\n",
							"hyperspace.Explain(query, true, input => DisplayHTML(input));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Enable Hybrid Scan\n",
							"\n",
							"In plan with indexes, you can see\n",
							"`Exchange hashpartitioning` required only for appended files so that we could still utilize the \"shuffled\" index data with appended files. `BucketUnion` is used to merge \"shuffled\" appended files with the index data."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Enable Hybrid Scan config. \"delete\" config is not necessary.\n",
							"spark.Conf().Set(\"spark.hyperspace.index.hybridscan.enabled\", \"true\");\n",
							"// spark.Conf().Set(\"spark.hyperspace.index.hybridscan.delete.enabled\", \"true\");\n",
							"spark.EnableHyperspace();\n",
							"// Need to redefine query to recalculate the query plan.\n",
							"DataFrame query = filter1.Join(filter2, filter1.Col(\"name\") == filter2.Col(\"name\"));\n",
							"\n",
							"query.Show();\n",
							"\n",
							"hyperspace.Explain(query, true, input => DisplayHTML(input));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Cleanup\n",
							"To make this notebook self-contained and not leave any dangling data, we have some small clean-up code below. "
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"using Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Notebook.MSSparkUtils;\n",
							"\n",
							"FS.Rm(dataPath, true);\n",
							"FS.Rm(indexLocation, true);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 27
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Getting Started with Hyperspace Indexing_kci')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9224513e-7eb3-4f61-96b8-cf76cf67c6cd"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Hitchhiker's Guide to Hyperspace (Python)\n",
							"## An Indexing Subsystem for Apache Spark\n",
							"\n",
							"<img src=\"https://raw.githubusercontent.com/rapoth/hyperspace/master/docs/assets/images/hyperspace-small-banner.png\" alt=\"Hyperspace Indexing Sub-System Logo\" width=\"1000\"/>\n",
							"\n",
							"[Hyperspace](https://github.com/microsoft/hyperspace) introduces the ability for Apache Spark users to create indexes on their datasets (e.g., CSV, JSON, Parquet etc.) and leverage them for potential query and workload acceleration.\n",
							"\n",
							"In this notebook, we highlight the basics of Hyperspace, emphasizing on its simplicity and show how it can be used by just anyone.\n",
							"\n",
							"**Disclaimer**: Hyperspace helps accelerate your workloads/queries under two circumstances:\n",
							"\n",
							"  1. Queries contain filters on predicates with high selectivity (e.g., you want to select 100 matching rows from a million candidate rows)\n",
							"  2. Queries contain a join that requires heavy-shuffles (e.g., you want to join a 100 GB dataset with a 10 GB dataset)\n",
							"\n",
							"You may want to carefully monitor your workloads and determine whether indexing is helping you on a case-by-case basis."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Setup\n",
							"To begin with, let's start a new Spark session. Since this notebook is a tutorial merely to illustrate what Hyperspace can offer, we will make a configuration change that allow us to highlight what Hyperspace is doing on small datasets. By default, Spark uses *broadcast join* to optimize join queries when the data size for one side of join is small (which is the case for the sample data we use in this tutorial). Therefore, we disable broadcast joins so that later when we run join queries, Spark uses *sort-merge* join. This is mainly to show how Hyperspace indexes would be used at scale for accelerating join queries.\n",
							"\n",
							"The output of running the cell below shows a reference to the successfully created Spark session and prints out '-1' as the value for the modified join config which indicates that broadcast join is successfully disabled."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"import random\n",
							"\n",
							"session_id = random.randint(0,1000000)\n",
							"data_path = \"/hyperspace/data-{0}\".format(session_id)\n",
							"index_location = \"/hyperspace/indexes-{0}\".format(session_id)\n",
							"\n",
							"# Please note that you DO NOT need to change this configuration in production.\n",
							"# We store all indexes in the system folder within Synapse.\n",
							"spark.conf.set(\"spark.hyperspace.system.path\", index_location)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"# Start your Spark session\n",
							"spark\n",
							"\n",
							"# Disable BroadcastHashJoin, so Spark will use standard SortMergeJoin. Currently Hyperspace indexes utilize SortMergeJoin to speed up query.\n",
							"spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
							"\n",
							"# Verify that BroadcastHashJoin is set correctly \n",
							"print(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))\n",
							"spark.conf.set(\"spark.hyperspace.explain.displayMode\", \"html\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data Preparation\n",
							"\n",
							"To prepare our environment, we will create sample data records and save them as parquet data files. While we use Parquet for illustration, you can use other formats such as CSV. In the subsequent cells, we will also demonstrate how you can create several Hyperspace indexes on this sample dataset and how one can make Spark use them when running queries. \n",
							"\n",
							"Our example records correspond to two datasets: *department* and *employee*. You should configure \"empLocation\" and \"deptLocation\" paths so that on the storage account they point to your desired location to save generated data files. \n",
							"\n",
							"The output of running below cell shows contents of our datasets as lists of triplets followed by references to dataFrames created to save the content of each dataset in our preferred location."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
							"\n",
							"# Sample department records\n",
							"departments = [(10, \"Accounting\", \"New York\"), (20, \"Research\", \"Dallas\"), (30, \"Sales\", \"Chicago\"), (40, \"Operations\", \"Boston\")]\n",
							"\n",
							"# Sample employee records\n",
							"employees = [(7369, \"SMITH\", 20), (7499, \"ALLEN\", 30), (7521, \"WARD\", 30), (7566, \"JONES\", 20), (7698, \"BLAKE\", 30)]\n",
							"\n",
							"# Create a schema for the dataframe\n",
							"dept_schema = StructType([StructField('deptId', IntegerType(), True), StructField('deptName', StringType(), True), StructField('location', StringType(), True)])\n",
							"emp_schema = StructType([StructField('empId', IntegerType(), True), StructField('empName', StringType(), True), StructField('deptId', IntegerType(), True)])\n",
							"\n",
							"departments_df = spark.createDataFrame(departments, dept_schema)\n",
							"employees_df = spark.createDataFrame(employees, emp_schema)\n",
							"\n",
							"emp_Location = data_path + \"/employees.parquet\"\n",
							"dept_Location = data_path + \"/departments.parquet\"\n",
							"\n",
							"employees_df.write.mode(\"overwrite\").parquet(emp_Location)\n",
							"departments_df.write.mode(\"overwrite\").parquet(dept_Location)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"Let's verify the contents of parquet files we created above to make sure they contain expected records in correct format. We later use these data files to create Hyperspace indexes and run sample queries.\n",
							"\n",
							"Running below cell, the output displays the rows in employee and department dataframes in a tabular form. There should be 14 employees and 4 departments, each matching with one of triplets we created in the previous cell."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# emp_Location and dept_Location are the user defined locations above to save parquet files\n",
							"emp_DF = spark.read.parquet(emp_Location)\n",
							"dept_DF = spark.read.parquet(dept_Location)\n",
							"\n",
							"# Verify the data is available and correct\n",
							"emp_DF.show()\n",
							"dept_DF.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Hello Hyperspace Index!\n",
							"Hyperspace lets users create indexes on records scanned from persisted data files. Once successfully created, an entry corresponding to the index is added to the Hyperspace's metadata. This metadata is later used by Apache Spark's Hyperspace-enabled optimizer during query processing to find and use proper indexes. \n",
							"\n",
							"Once indexes are created, users can perform several actions:\n",
							"  - **Refresh** If the underlying data changes, users can refresh an existing index to capture that. \n",
							"  - **Delete** If the index is not needed, users can perform a soft-delete i.e., index is not physically deleted but is marked as 'deleted' so it is no longer used in your workloads.\n",
							"  - **Vacuum** If an index is no longer required, users can vacuum it which forces a physical deletion of the index contents and associated metadata completely from Hyperspace's metadata.\n",
							"\n",
							"Below sections show how such index management operations can be done in Hyperspace.\n",
							"\n",
							"First, we need to import the required libraries and create an instance of Hyperspace. We later use this instance to invoke different Hyperspace APIs to create indexes on our sample data and modify those indexes.\n",
							"\n",
							"Output of running below cell shows a reference to the created instance of Hyperspace."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from hyperspace import *\n",
							"\n",
							"# Create an instance of Hyperspace\n",
							"hyperspace = Hyperspace(spark)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create Indexes\n",
							"To create a Hyperspace index, the user needs to provide 2 pieces of information:\n",
							"* An Apache Spark DataFrame which references the data to be indexed.\n",
							"* An index configuration object: IndexConfig, which specifies the *index name*, *indexed* and *included* columns of the index. \n",
							"\n",
							"As you might have noticed, in this notebook, we illustrate indexing using the [Covering Index](https://www.red-gate.com/simple-talk/sql/learn-sql-server/using-covering-indexes-to-improve-query-performance/), which are the default index in Hyperspace. In the future, we plan on adding support for other index types. \n",
							"\n",
							"We start by creating three Hyperspace indexes on our sample data: two indexes on the department dataset named \"deptIndex1\" and \"deptIndex2\", and one index on the employee dataset named 'empIndex'. \n",
							"For each index, we need a corresponding IndexConfig to capture the name along with columns lists for the indexed and included columns. Running below cell creates these indexConfigs and its output lists them.\n",
							"\n",
							"**Note**: An *index column* is a column that appears in your filters or join conditions. An *included column* is a column that appears in your select/project.\n",
							"\n",
							"For instance, in the following query:\n",
							"```sql\n",
							"SELECT X\n",
							"FROM Table\n",
							"WHERE Y = 2\n",
							"```\n",
							"X can be an *index column* and Y can be an *included column*."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Create index configurations\n",
							"\n",
							"emp_IndexConfig = IndexConfig(\"empIndex1\", [\"deptId\"], [\"empName\"])\n",
							"dept_IndexConfig1 = IndexConfig(\"deptIndex1\", [\"deptId\"], [\"deptName\"])\n",
							"dept_IndexConfig2 = IndexConfig(\"deptIndex2\", [\"location\"], [\"deptName\"])"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"Now, we create three indexes using our index configurations. For this purpose, we invoke \"createIndex\" command on our Hyperspace instance. This command requires an index configuration and the dataFrame containing rows to be indexed.\n",
							"Running below cell creates three indexes.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Create indexes from configurations\n",
							"\n",
							"hyperspace.createIndex(emp_DF, emp_IndexConfig)\n",
							"hyperspace.createIndex(dept_DF, dept_IndexConfig1)\n",
							"hyperspace.createIndex(dept_DF, dept_IndexConfig2)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"### List Indexes\n",
							"\n",
							"Below code shows how a user can list all available indexes in a Hyperspace instance. It uses the `indexes` API which returns information about existing indexes as a Spark's DataFrame so you can perform additional operations. For instance, you can invoke valid operations on this DataFrame for checking its content or analyzing it further (for example filtering specific indexes or grouping them according to some desired property). \n",
							"\n",
							"Below cell uses DataFrame's `show` action to fully print the rows and show details of our indexes in a tabular form. For each index, we can see all the information Hyperspace has stored about it in its metadata. \n",
							"\n",
							"You will immediately notice the following:\n",
							"  - `config.indexName`, `config.indexedColumns`, `config.includedColumns` are the fields that a user normally provides during index creation.\n",
							"  - `status.status` indicates if the index is being actively used by the Spark's optimizer.\n",
							"  - `dfSignature` is automatically generated by Hyperspace and is unique for each index. Hyperspace uses this signature internally to maintain the index and exploit it at query time. \n",
							"  \n",
							"In the output below, all three indexes should have \"ACTIVE\" as status and their name, indexed columns, and included columns should match with what we defined in index configurations above."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.indexes().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Delete Indexes\n",
							"A user can drop an existing index by using the `deleteIndex` API and providing the index name. \n",
							"\n",
							"Index deletion is a **soft-delete** operation i.e., only the index's status in the Hyperspace metadata from is changed from \"ACTIVE\" to \"DELETED\". This will exclude the deleted index from any future query optimization and Hyperspace no longer picks that index for any query. However, index files for a deleted index still remain available (since it is a soft-delete), so if you accidentally deleted the index, you could still restore it.\n",
							"\n",
							"The cell below deletes index with name \"deptIndex2\" and lists Hyperspace metadata after that. The output should be similar to above cell for \"List Indexes\" except for \"deptIndex2\" which now should have its status changed into \"DELETED\"."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.deleteIndex(\"deptIndex2\")\n",
							"\n",
							"hyperspace.indexes().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Restore Indexes\n",
							"A user can use the `restoreIndex` API to restore a deleted index. This will bring back the latest version of index into ACTIVE status and makes it usable again for queries. \n",
							"\n",
							"The cell below shows an example of `restoreIndex` API. We delete \"deptIndex1\" and restore it. The output shows \"deptIndex1\" first went into the \"DELETED\" status after invoking \"deleteIndex\" command and came back to the \"ACTIVE\" status after calling \"restoreIndex\"."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.deleteIndex(\"deptIndex1\")\n",
							"\n",
							"hyperspace.indexes().show()\n",
							"\n",
							"hyperspace.restoreIndex(\"deptIndex1\")\n",
							"\n",
							"hyperspace.indexes().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Vacuum Indexes\n",
							"The user can perform a **hard-delete** i.e., fully remove files and the metadata entry for a deleted index using the `vacuumIndex` API. Once done, this action is **irreversible** as it physically deletes all the index files associated with the index.\n",
							"\n",
							"The cell below vacuums the \"deptIndex2\" index and shows Hyperspace metadata after vaccuming. You should see metadata entries for two indexes \"deptIndex1\" and \"empIndex\" both with \"ACTIVE\" status and no entry for \"deptIndex2\"."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.vacuumIndex(\"deptIndex2\")\n",
							"hyperspace.indexes().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Enable/Disable Hyperspace\n",
							"\n",
							"Hyperspace provides APIs to enable or disable index usage with Spark.\n",
							"\n",
							"  - By using `enableHyperspace` API, Hyperspace optimization rules become visible to the Apache Spark optimizer and it will exploit existing Hyperspace indexes to optimize user queries.\n",
							"  - By using `disableHyperspace` command, Hyperspace rules no longer apply during query optimization. You should note that disabling Hyperspace has no impact on created indexes as they remain intact.\n",
							"\n",
							"Below cell shows how you can use these commands to enable or disable hyperspace. The output simply shows a reference to the existing Spark session whose configuration is updated."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Enable Hyperspace\n",
							"Hyperspace.enable(spark)\n",
							"\n",
							"# Disable Hyperspace\n",
							"Hyperspace.disable(spark)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Index Usage\n",
							"In order to make Spark use Hyperspace indexes during query processing, the user needs to make sure that Hyperspace is enabled. \n",
							"\n",
							"The cell below enables Hyperspace and creates two DataFrames containing our sample data records which we use for running example queries. For each DataFrame, a few sample rows are printed."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Enable Hyperspace\n",
							"Hyperspace.enable(spark)\n",
							"\n",
							"emp_DF = spark.read.parquet(emp_Location)\n",
							"dept_DF = spark.read.parquet(dept_Location)\n",
							"\n",
							"emp_DF.show(5)\n",
							"dept_DF.show(5)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Hyperspace's Index Types\n",
							"\n",
							"Currently, Hyperspace can exploit indexes for two groups of queries: \n",
							"* Selection queries with lookup or range selection filtering predicates.\n",
							"* Join queries with an equality join predicate (i.e. Equi-joins)."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Indexes for Accelerating Filters\n",
							"\n",
							"Our first example query does a lookup on department records (see below cell). In SQL, this query looks as follows:\n",
							"\n",
							"```sql\n",
							"SELECT deptName \n",
							"FROM departments\n",
							"WHERE deptId = 20\n",
							"```\n",
							"\n",
							"The output of running the cell below shows: \n",
							"- query result, which is a single department name.\n",
							"- query plan that Spark used to run the query. \n",
							"\n",
							"In the query plan, the \"FileScan\" operator at the bottom of the plan shows the datasource where the records were read from. The location of this file indicates the path to the latest version of the \"deptIndex1\" index. This shows  that according to the query and using Hyperspace optimization rules, Spark decided to exploit the proper index at runtime.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Filter with equality predicate\n",
							"\n",
							"eqFilter = dept_DF.filter(\"\"\"deptId = 20\"\"\").select(\"\"\"deptName\"\"\")\n",
							"eqFilter.show()\n",
							"\n",
							"hyperspace.explain(eqFilter, True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"source": [
							"Our second example is a range selection query on department records. In SQL, this query looks as follows:\n",
							"\n",
							"```sql\n",
							"SELECT deptName \n",
							"FROM departments\n",
							"WHERE deptId > 20\n",
							"```\n",
							"Similar to our first example, the output of the cell below shows the query results (names of two departments) and the query plan. The location of data file in the FileScan operator shows that 'deptIndex1\" was used to run the query.   \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Filter with range selection predicate\n",
							"\n",
							"rangeFilter = dept_DF.filter(\"\"\"deptId > 20\"\"\").select(\"deptName\")\n",
							"rangeFilter.show()\n",
							"\n",
							"hyperspace.explain(rangeFilter, True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"source": [
							"Our third example is a query joining department and employee records on the department id. The equivalent SQL statement is shown below:\n",
							"\n",
							"```sql\n",
							"SELECT employees.deptId, empName, departments.deptId, deptName\n",
							"FROM   employees, departments \n",
							"WHERE  employees.deptId = departments.deptId\n",
							"```\n",
							"\n",
							"The output of running the cell below shows the query results which are the names of 14 employees and the name of department each employee works in. The query plan is also included in the output. Notice how the file locations for two FileScan operators shows that Spark used \"empIndex\" and \"deptIndex1\" indexes to run the query.   \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Join\n",
							"\n",
							"eqJoin = emp_DF.join(dept_DF, emp_DF.deptId == dept_DF.deptId).select(emp_DF.empName, dept_DF.deptName)\n",
							"\n",
							"eqJoin.show()\n",
							"\n",
							"hyperspace.explain(eqJoin, True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Support for SQL Semantics\n",
							"\n",
							"The index usage is transparent to whether the user uses DataFrame API or Spark SQL. The following example shows the same join example as before but using Spark SQL, showing the use of indexes if applicable."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\n",
							"\n",
							"emp_DF.createOrReplaceTempView(\"EMP\")\n",
							"dept_DF.createOrReplaceTempView(\"DEPT\")\n",
							"\n",
							"joinQuery = spark.sql(\"SELECT EMP.empName, DEPT.deptName FROM EMP, DEPT WHERE EMP.deptId = DEPT.deptId\")\n",
							"\n",
							"joinQuery.show()\n",
							"hyperspace.explain(joinQuery, True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Explain API\n",
							"\n",
							"So far, you might have observed we have been using the explain API provided by Hyperspace. The `explain` API from Hyperspace is very similar to Spark's `df.explain` API but allows users to compare their original plan vs the updated index-dependent plan before running their query. You have an option to choose from html/plaintext/console mode to display the command output. \n",
							"\n",
							"The following cell shows an example with HTML. The highlighted section represents the difference between original and updated plans along with the indexes being used."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"eqJoin = emp_DF.join(dept_DF, emp_DF.deptId == dept_DF.deptId).select(emp_DF.empName, dept_DF.deptName)\n",
							"\n",
							"spark.conf.set(\"spark.hyperspace.explain.displayMode\", \"html\")\n",
							"hyperspace.explain(eqJoin, True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Refresh Indexes\n",
							"If the original data on which an index was created changes, then the index will no longer capture the latest state of data and hence will not be used by Hyperspace to provide any acceleration. The user can refresh such a stale index using the `refreshIndex` API. This causes the index to be fully rebuilt and updates it according to the latest data records.\n",
							"    \n",
							"    Spoiler alert: if you are worried about fully rebuilding your index every time your data changes, don't worry! We will show you how to *incrementally refresh* your index in subsequent cells below.\n",
							"\n",
							"The two cells below show an example for this scenario:\n",
							"- First cell adds two more departments to the original departments data. It reads and prints list of departments to verify new departments are added correctly. The output shows 6 departments in total: four old ones and two new. Invoking \"refreshIndex\" updates \"deptIndex1\" so index captures new departments.\n",
							"- Second cell runs our range selection query example. The results should now contain four departments: two are the ones, seen before when we ran the query above, and two are the new departments we just added."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"extra_Departments = [(50, \"Inovation\", \"Seattle\"), (60, \"Human Resources\", \"San Francisco\")]\n",
							"\n",
							"extra_departments_df = spark.createDataFrame(extra_Departments, dept_schema)\n",
							"extra_departments_df.write.mode(\"Append\").parquet(dept_Location)\n",
							"\n",
							"\n",
							"dept_DFrame_Updated = spark.read.parquet(dept_Location)\n",
							"\n",
							"dept_DFrame_Updated.show(10)\n",
							"\n",
							"hyperspace.refreshIndex(\"deptIndex1\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"source": [
							"newRangeFilter = dept_DFrame_Updated.filter(\"deptId > 20\").select(\"deptName\")\n",
							"newRangeFilter.show()\n",
							"\n",
							"hyperspace.explain(newRangeFilter, True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.indexes().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"source": [
							"# Clean-up the remaining indexes\n",
							"hyperspace.deleteIndex(\"empIndex1\")\n",
							"hyperspace.deleteIndex(\"deptIndex1\")\n",
							"\n",
							"hyperspace.vacuumIndex(\"empIndex1\")\n",
							"hyperspace.vacuumIndex(\"deptIndex1\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Hybrid Scan for Mutable Datasets\n",
							"\n",
							"Often times, if your underlying source data had some new files appended or existing files deleted, your index will get stale and Hyperspace decides not to use it. However, there are times where you just want to use the index without having to refresh it everytime. There could be multiple reasons for doing so:\n",
							"\n",
							"  1. You do not want to continuosly refresh your index but instead want to do it periodically since you understand your workloads the best.  \n",
							"  2. You added/removed only a few files and do not want to wait for yet another refresh job to finish. \n",
							"\n",
							"To allow you to still use a stale index, Hyperspace introduces **Hybrid Scan**, a novel technique that allows users to utilize outdated or stale indexes (e.g., the underlying source data had some new files appended or existing files deleted), without refreshing indexes. \n",
							"\n",
							"To achieve this, when you set the appropriate configuration to enable Hybrid Scan, Hyperspace modifies the query plan to leverage the changes as following:\n",
							"- Appended files can be merged to index data by using **`Union` or `BucketUnion` (for join)**. Shuffling appended data can also be applied before merging, if needed.\n",
							"- Deleted files can be handled by injecting `Filter-NOT-IN` condition on **lineage column** of index data, so that the indexed rows from the deleted files can be excluded at query time. \n",
							"\n",
							"You can check the transformation of the query plan in below examples.\n",
							"\n",
							"    Note: Hybrid scan is only supported for non-partitioned data. Support for partitioned data is currently being worked upon."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Hybrid Scan for appended files - non-partitioned data\n",
							"\n",
							"Non-partitioned data is used in below example. In this example, we expect Join index can be used for the query and `BucketUnion` is introduced for appended files."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# GENERATE TEST DATA\n",
							"\n",
							"testdata = [\n",
							"    (\"orange\", 3, \"2020-10-01\"),\n",
							"    (\"banana\", 1, \"2020-10-01\"),\n",
							"    (\"carrot\", 5, \"2020-10-02\"),\n",
							"    (\"beetroot\", 12, \"2020-10-02\"),\n",
							"    (\"orange\", 2, \"2020-10-03\"),\n",
							"    (\"banana\", 11, \"2020-10-03\"),\n",
							"    (\"carrot\", 3, \"2020-10-03\"),\n",
							"    (\"beetroot\", 2, \"2020-10-04\"),\n",
							"    (\"cucumber\", 7, \"2020-10-05\"),\n",
							"    (\"pepper\", 20, \"2020-10-06\")\n",
							"]\n",
							"\n",
							"testdata_location = data_path + \"/productTable\"\n",
							"from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
							"testdata_schema = StructType([\n",
							"    StructField('name', StringType(), True),\n",
							"    StructField('qty', IntegerType(), True),\n",
							"    StructField('date', StringType(), True)])\n",
							"\n",
							"test_df = spark.createDataFrame(testdata, testdata_schema)\n",
							"test_df.write.mode(\"overwrite\").parquet(testdata_location)\n",
							"test_df = spark.read.parquet(testdata_location)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# CREATE INDEX\n",
							"hyperspace.createIndex(test_df, IndexConfig(\"productIndex2\", [\"name\"], [\"date\", \"qty\"]))\n",
							"\n",
							"spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
							"filter1 = test_df.filter(\"name = 'banana'\")\n",
							"filter2 = test_df.filter(\"qty > 10\")\n",
							"query = filter1.join(filter2, \"name\")\n",
							"\n",
							"# Check Join index rule is applied properly.\n",
							"hyperspace.explain(query, True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Append new files.\r\n",
							"append_data = [\r\n",
							"    (\"orange\", 13, \"2020-11-01\"),\r\n",
							"    (\"banana\", 5, \"2020-11-01\")\r\n",
							"]\r\n",
							"append_df = spark.createDataFrame(append_data, testdata_schema)\r\n",
							"append_df.write.mode(\"append\").parquet(testdata_location)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "markdown",
						"source": [
							"Hybrid scan is disabled by default. Therefore, you will see that since we appended new data, Hyperspace will decide NOT to use the index.\n",
							"\n",
							"In the output, you will see no plan differences (hence no highlighting)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Hybrid Scan configs are false by default.\n",
							"spark.conf.set(\"spark.hyperspace.index.hybridscan.enabled\", \"false\")\n",
							"spark.conf.set(\"spark.hyperspace.index.hybridscan.delete.enabled\", \"false\")\n",
							"\n",
							"test_df_with_append = spark.read.parquet(testdata_location)\n",
							"filter1 = test_df_with_append.filter(\"name = 'banana'\")\n",
							"filter2 = test_df_with_append.filter(\"qty > 10\")\n",
							"query = filter1.join(filter2, \"name\")\n",
							"hyperspace.explain(query, True, displayHTML)\n",
							"query.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Enable Hybrid Scan\r\n",
							"\r\n",
							"In plan with indexes, you can see\r\n",
							"`Exchange hashpartitioning` required only for appended files so that we could still utilize the \"shuffled\" index data with appended files. `BucketUnion` is used to merge \"shuffled\" appended files with the index data."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Enable Hybrid Scan config. \"delete\" config is not necessary.\n",
							"spark.conf.set(\"spark.hyperspace.index.hybridscan.enabled\", \"true\")\n",
							"# spark.conf.set(\"spark.hyperspace.index.hybridscan.delete.enabled\", \"true\")\n",
							"\n",
							"# Need to redefine query to recalculate the query plan.\n",
							"query = filter1.join(filter2, \"name\")\n",
							"hyperspace.explain(query, True, displayHTML)\n",
							"query.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Incremental Index Refresh\n",
							"When you ready to update your indexes but do not want to rebuild your entire index, Hyperspace supports updating indexes in an incremental manner using `hs.refreshIndex(\"name\", \"incremental\")` API. This will allow eliminate the need for a full rebuild of index from scratch, utilizing previously created index files as well as updating indexes on only the newly added data.\n",
							"\n",
							"Of course, please be sure to use the complementary `optimizeIndex` API (shown below) periodically to make sure you do not see performance regressions. We recommend calling `optimize` at least once for every 10 times you call `refreshIndex(..., \"incremental\")`, assuming the data you added/removed is < 10% of the original dataset. For instance, if your original dataset is 100 GB, and you've added/removed data in increments/decrements of 1 GB, you can call `refreshIndex` 10 times before calling `optimizeIndex`. Please note that this example is simply used for illustration and you have to adapt this for your workloads.\n",
							"\n",
							"In the example below, notice the addition of a `Sort` node in the query plan when indexes are used. This is because partial indexes are created on the appended data files, causing Spark to introduce a `Sort`. Please also note that `Shuffle` i.e. `Exchange` is still eliminated from the plan, giving you the appropriate acceleration."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"def query():\n",
							"    test_df_with_append = spark.read.parquet(testdata_location)\n",
							"    filter1 = test_df_with_append.filter(\"name = 'banana'\")\n",
							"    filter2 = test_df_with_append.filter(\"qty > 10\")\n",
							"    return filter1.join(filter2, \"name\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.refreshIndex(\"productIndex2\", \"incremental\")\n",
							"\n",
							"hyperspace.explain(query(), True, displayHTML)\n",
							"query().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Optimize Index layout\n",
							"After calling incremental refreshes multiple times on newly appended data (e.g. if the user writes to data in small batches or in case of streaming scenarios), the number of index files tend to become large affecting the performance of the index (large number of small files problem). Hyperspace provides `hyperspace.optimizeIndex(\"indexName\")` API to optimize the index layout and reduce the large files problem.\n",
							"\n",
							"In the plan below, notice that Hyperspace has removed the additional `Sort` node in the query plan. Optimize can help avoiding sorting for any index bucket which contains only one file. However, this will only be true if ALL the index buckets have at most 1 file per bucket, after `optimizeIndex`."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Append some more data and call refresh again.\n",
							"append_data = [\n",
							"    (\"orange\", 13, \"2020-11-01\"),\n",
							"    (\"banana\", 5, \"2020-11-01\")\n",
							"]\n",
							"append_df = spark.createDataFrame(append_data, testdata_schema)\n",
							"append_df.write.mode(\"append\").parquet(testdata_location)\n",
							"\n",
							"hyperspace.refreshIndex(\"productIndex2\", \"incremental\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"source": [
							"# Call optimize. Ensure that Sort is removed after optimization (This is possible here because after optimize, in this case, every bucket contains only 1 file.).\n",
							"hyperspace.optimizeIndex(\"productIndex2\")\n",
							"\n",
							"hyperspace.explain(query(), True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Optimize modes\n",
							"\n",
							"The default mode for optimization is \"quick\" mode where files smaller than a predefined threshold are picked for optmization. To maximize the effect of optimization, Hyperspace allows another optimize mode \"full\" as shown below. This mode picks ALL index files for optimization irrespective of their file size and creates the best possible layout of the index. This is also slower than the default optimize mode because more data is being processed here.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.optimizeIndex(\"productIndex2\", \"full\")\n",
							"\n",
							"hyperspace.explain(query(), True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Clean Up\n",
							"To make this notebook self-contained and not leave any dangling data, we have some small clean-up code below. "
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.fs.rm(data_path, True)\n",
							"mssparkutils.fs.rm(index_location, True)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 33
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Hitchhiker Guide to Delta Lake_Csharp')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder/sample"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e10f48c3-f885-449e-b55b-0e2545b6955a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparkdotnet",
						"display_name": "csharp"
					},
					"language_info": {
						"name": "csharp"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"var sessionId = (new Random()).Next(10000000);\n",
							"var deltaTablePath = $\"/delta/delta-table-{sessionId}\";\n",
							"\n",
							"deltaTablePath"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"var data = spark.Range(0,5);\n",
							"data.Show();\n",
							"data.Write().Format(\"delta\").Save(deltaTablePath);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"using System.Linq;\n",
							"spark.Read().Text($\"{deltaTablePath}/_delta_log/\").Collect().ToList().ForEach(x => Console.WriteLine(x.GetAs<string>(\"value\")));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"var df = spark.Read().Format(\"delta\").Load(deltaTablePath);\n",
							"df.Show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"var data = spark.Range(5,10);\n",
							"data.Write().Format(\"delta\").Mode(\"overwrite\").Save(deltaTablePath);\n",
							"df.Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"spark.Read().Text($\"{deltaTablePath}/_delta_log/\").Collect().ToList().ForEach(x => Console.WriteLine(x.GetAs<string>(\"value\")));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"// Write data to a new managed catalog table.\n",
							"data.Write().Format(\"delta\").SaveAsTable(\"ManagedDeltaTable\");"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"// Define an external catalog table that points to the existing Delta Lake data in storage.\n",
							"spark.Sql($\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '{deltaTablePath}'\");"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"// List the 2 new tables.\n",
							"spark.Sql(\"SHOW TABLES\").Show();\n",
							"\n",
							"// Explore their properties.\n",
							"spark.Sql(\"DESCRIBE EXTENDED ManagedDeltaTable\").Show(truncate: 0);\n",
							"spark.Sql(\"DESCRIBE EXTENDED ExternalDeltaTable\").Show(truncate: 0);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							"using Microsoft.Spark.Extensions.Delta;\n",
							"using Microsoft.Spark.Extensions.Delta.Tables;\n",
							"using Microsoft.Spark.Sql;\n",
							"using static Microsoft.Spark.Sql.Functions;\n",
							"\n",
							"var deltaTable = DeltaTable.ForPath(deltaTablePath);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"// Update every even value by adding 100 to it\n",
							"deltaTable.Update(\n",
							"  condition: Expr(\"id % 2 == 0\"),\n",
							"  set: new Dictionary<string, Column>(){{ \"id\", Expr(\"id + 100\") }});\n",
							"deltaTable.ToDF().Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.Delete(condition: Expr(\"id % 2 == 0\"));\n",
							"deltaTable.ToDF().Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"// Upsert (merge) new data\n",
							"var newData = spark.Range(20).As(\"newData\");\n",
							"\n",
							"deltaTable\n",
							"    .As(\"oldData\")\n",
							"    .Merge(newData, \"oldData.id = newData.id\")\n",
							"    .WhenMatched()\n",
							"        .Update(new Dictionary<string, Column>() {{\"id\", Lit(\"-1\")}})\n",
							"    .WhenNotMatched()\n",
							"        .Insert(new Dictionary<string, Column>() {{\"id\", Col(\"newData.id\")}})\n",
							"    .Execute();\n",
							"\n",
							"deltaTable.ToDF().Show(100);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.History().Show(20, 1000, false);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"var df = spark.Read().Format(\"delta\").Option(\"versionAsOf\", 0).Load(deltaTablePath);\n",
							"df.Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"var streamingDf = spark.ReadStream().Format(\"rate\").Load();\n",
							"var stream = streamingDf.SelectExpr(\"value as id\").WriteStream().Format(\"delta\").Option(\"checkpointLocation\", $\"/tmp/checkpoint-{sessionId}\").Start(deltaTablePath);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.ToDF().Sort(Col(\"id\").Desc()).Show(100);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.History().Drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").Show(20, 1000, false);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"source": [
							"stream.Stop();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.History().Drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").Show(100, 1000, false);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"int partitionCount = 2;\r\n",
							"\r\n",
							"spark.Read()\r\n",
							"    .Format(\"delta\")\r\n",
							"    .Load(deltaTablePath)\r\n",
							"    .Repartition(partitionCount)\r\n",
							"    .Write()\r\n",
							"    .Option(\"dataChange\", \"false\")\r\n",
							"    .Format(\"delta\")\r\n",
							"    .Mode(\"overwrite\")\r\n",
							"    .Save(deltaTablePath);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"var parquetPath = $\"/parquet/parquet-table-{sessionId}\";\n",
							"\n",
							"var data = spark.Range(0,5);\n",
							"data.Write().Parquet(parquetPath);\n",
							"\n",
							"// Confirm that the data isn't in the Delta format\n",
							"DeltaTable.IsDeltaTable(parquetPath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"source": [
							"DeltaTable.ConvertToDelta(spark, $\"parquet.`{parquetPath}`\");\n",
							"\n",
							"//Confirm that the converted data is now in the Delta format\n",
							"DeltaTable.IsDeltaTable(parquetPath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"source": [
							"spark.Sql($\"DESCRIBE HISTORY delta.`{deltaTablePath}`\").Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"source": [
							"spark.Sql($\"VACUUM delta.`{deltaTablePath}`\").Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"source": [
							"var parquetId =  (new Random()).Next(10000000);\n",
							"var parquetPath = $\"/parquet/parquet-table-{sessionId}-{parquetId}\";\n",
							"\n",
							"var data = spark.Range(0,5);\n",
							"data.Write().Parquet(parquetPath);\n",
							"\n",
							"// Confirm that the data isn't in the Delta format\n",
							"DeltaTable.IsDeltaTable(parquetPath);\n",
							"\n",
							"// Use SQL to convert the parquet table to Delta\n",
							"spark.Sql($\"CONVERT TO DELTA parquet.`{parquetPath}`\");\n",
							"\n",
							"DeltaTable.IsDeltaTable(parquetPath);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 26
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Hitchhiker Guide to Hyperspace_csharp')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder/sample"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "049d5e79-5c2a-4266-8977-a253c2051dd6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparkdotnet",
						"display_name": "csharp"
					},
					"language_info": {
						"name": "csharp"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"var sessionId = (new Random()).Next(10000000);\r\n",
							"var dataPath = $\"/hyperspace/data-{sessionId}\";\r\n",
							"var indexLocation = $\"/hyperspace/indexes-{sessionId}\";\r\n",
							"\r\n",
							"// Please note that you DO NOT need to change this configuration in production.\r\n",
							"// We store all indexes in the system folder within Synapse.\r\n",
							"spark.Conf().Set(\"spark.hyperspace.system.path\", indexLocation)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Disable BroadcastHashJoin, so Spark will use standard SortMergeJoin. Currently hyperspace indexes utilize SortMergeJoin to speed up query.\r\n",
							"spark.Conf().Set(\"spark.sql.autoBroadcastJoinThreshold\", -1);\r\n",
							"\r\n",
							"// Verify that BroadcastHashJoin is set correctly \r\n",
							"Console.WriteLine(spark.Conf().Get(\"spark.sql.autoBroadcastJoinThreshold\"));\r\n",
							"\r\n",
							"spark.Conf().Set(\"spark.hyperspace.explain.displayMode\", \"html\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"using Microsoft.Spark.Sql.Types;\r\n",
							"\r\n",
							"// Sample department records\r\n",
							"var departments = new List<GenericRow>()\r\n",
							"{\r\n",
							"    new GenericRow(new object[] {10, \"Accounting\", \"New York\"}),\r\n",
							"    new GenericRow(new object[] {20, \"Research\", \"Dallas\"}),\r\n",
							"    new GenericRow(new object[] {30, \"Sales\", \"Chicago\"}),\r\n",
							"    new GenericRow(new object[] {40, \"Operations\", \"Boston\"})\r\n",
							"};\r\n",
							"\r\n",
							"// Sample employee records\r\n",
							"var employees = new List<GenericRow>() {\r\n",
							"      new GenericRow(new object[] {7369, \"SMITH\", 20}),\r\n",
							"      new GenericRow(new object[] {7499, \"ALLEN\", 30}),\r\n",
							"      new GenericRow(new object[] {7521, \"WARD\", 30}),\r\n",
							"      new GenericRow(new object[] {7566, \"JONES\", 20}),\r\n",
							"      new GenericRow(new object[] {7698, \"BLAKE\", 30}),\r\n",
							"      new GenericRow(new object[] {7782, \"CLARK\", 10}),\r\n",
							"      new GenericRow(new object[] {7788, \"SCOTT\", 20}),\r\n",
							"      new GenericRow(new object[] {7839, \"KING\", 10}),\r\n",
							"      new GenericRow(new object[] {7844, \"TURNER\", 30}),\r\n",
							"      new GenericRow(new object[] {7876, \"ADAMS\", 20}),\r\n",
							"      new GenericRow(new object[] {7900, \"JAMES\", 30}),\r\n",
							"      new GenericRow(new object[] {7934, \"MILLER\", 10}),\r\n",
							"      new GenericRow(new object[] {7902, \"FORD\", 20}),\r\n",
							"      new GenericRow(new object[] {7654, \"MARTIN\", 30})\r\n",
							"};\r\n",
							"\r\n",
							"// Save sample data in the Parquet format\r\n",
							"var departmentSchema = new StructType(new List<StructField>()\r\n",
							"{\r\n",
							"    new StructField(\"deptId\", new IntegerType()),\r\n",
							"    new StructField(\"deptName\", new StringType()),\r\n",
							"    new StructField(\"location\", new StringType())\r\n",
							"});\r\n",
							"var employeeSchema = new StructType(new List<StructField>()\r\n",
							"{\r\n",
							"    new StructField(\"empId\", new IntegerType()),\r\n",
							"    new StructField(\"empName\", new StringType()),\r\n",
							"    new StructField(\"deptId\", new IntegerType())\r\n",
							"});\r\n",
							"\r\n",
							"DataFrame empData = spark.CreateDataFrame(employees, employeeSchema); \r\n",
							"DataFrame deptData = spark.CreateDataFrame(departments, departmentSchema); \r\n",
							"\r\n",
							"string empLocation = $\"{dataPath}/employees.parquet\";\r\n",
							"string deptLocation = $\"{dataPath}/departments.parquet\";\r\n",
							"empData.Write().Mode(\"overwrite\").Parquet(empLocation);\r\n",
							"deptData.Write().Mode(\"overwrite\").Parquet(deptLocation);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// empLocation and deptLocation are the user defined locations above to save parquet files\r\n",
							"DataFrame empDF = spark.Read().Parquet(empLocation);\r\n",
							"DataFrame deptDF = spark.Read().Parquet(deptLocation);\r\n",
							"\r\n",
							"// Verify the data is available and correct\r\n",
							"empDF.Show();\r\n",
							"deptDF.Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"using Microsoft.Spark.Extensions.Hyperspace;\r\n",
							"\r\n",
							"Hyperspace hyperspace = new Hyperspace(spark);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Create index configurations\r\n",
							"using Microsoft.Spark.Extensions.Hyperspace.Index;\r\n",
							"\r\n",
							"var empIndexConfig = new IndexConfig(\"empIndex\", new string[] {\"deptId\"}, new string[] {\"empName\"});\r\n",
							"var deptIndexConfig1 = new IndexConfig(\"deptIndex1\", new string[] {\"deptId\"}, new string[] {\"deptName\"});\r\n",
							"var deptIndexConfig2 = new IndexConfig(\"deptIndex2\", new string[] {\"location\"}, new string[] {\"deptName\"});"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Create indexes from configurations\r\n",
							"hyperspace.CreateIndex(empDF, empIndexConfig);\r\n",
							"hyperspace.CreateIndex(deptDF, deptIndexConfig1);\r\n",
							"hyperspace.CreateIndex(deptDF, deptIndexConfig2);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"hyperspace.Indexes().Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"hyperspace.DeleteIndex(\"deptIndex2\");\r\n",
							"\r\n",
							"hyperspace.Indexes().Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"hyperspace.DeleteIndex(\"deptIndex1\");\r\n",
							"\r\n",
							"hyperspace.Indexes().Show();\r\n",
							"\r\n",
							"hyperspace.RestoreIndex(\"deptIndex1\");\r\n",
							"\r\n",
							"hyperspace.Indexes().Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"hyperspace.VacuumIndex(\"deptIndex2\");\r\n",
							"\r\n",
							"hyperspace.Indexes().Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Enable Hyperspace\r\n",
							"spark.EnableHyperspace();\r\n",
							"\r\n",
							"// Disable Hyperspace\r\n",
							"spark.DisableHyperspace();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Enable Hyperspace\r\n",
							"spark.EnableHyperspace();\r\n",
							"\r\n",
							"DataFrame empDFrame = spark.Read().Parquet(empLocation);\r\n",
							"DataFrame deptDFrame = spark.Read().Parquet(deptLocation);\r\n",
							"\r\n",
							"empDFrame.Show(5);\r\n",
							"deptDFrame.Show(5);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Filter with equality predicate\r\n",
							"DataFrame eqFilter = deptDFrame.Filter(\"deptId = 20\").Select(\"deptName\");\r\n",
							"eqFilter.Show();\r\n",
							"\r\n",
							"hyperspace.Explain(eqFilter, true, input => DisplayHTML(input));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Filter with range selection predicate\r\n",
							"DataFrame rangeFilter = deptDFrame.Filter(\"deptId > 20\").Select(\"deptName\");\r\n",
							"rangeFilter.Show();\r\n",
							"\r\n",
							"hyperspace.Explain(rangeFilter, true, input => DisplayHTML(input));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Join\r\n",
							"DataFrame eqJoin =\r\n",
							"      empDFrame\r\n",
							"      .Join(deptDFrame, empDFrame.Col(\"deptId\") == deptDFrame.Col(\"deptId\"))\r\n",
							"      .Select(empDFrame.Col(\"empName\"), deptDFrame.Col(\"deptName\"));\r\n",
							"\r\n",
							"eqJoin.Show();\r\n",
							"\r\n",
							"hyperspace.Explain(eqJoin, true, input => DisplayHTML(input));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"empDFrame.CreateOrReplaceTempView(\"EMP\");\r\n",
							"deptDFrame.CreateOrReplaceTempView(\"DEPT\");\r\n",
							"\r\n",
							"var joinQuery = spark.Sql(\"SELECT EMP.empName, DEPT.deptName FROM EMP, DEPT WHERE EMP.deptId = DEPT.deptId\");\r\n",
							"\r\n",
							"joinQuery.Show();\r\n",
							"hyperspace.Explain(joinQuery, true, input => DisplayHTML(input));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"spark.Conf().Set(\"spark.hyperspace.explain.displayMode\", \"html\");\r\n",
							"spark.Conf().Set(\"spark.hyperspace.explain.displayMode.highlight.beginTag\", \"<b style=\\\"background:LightGreen\\\">\");\r\n",
							"spark.Conf().Set(\"spark.hyperspace.explain.displayMode.highlight.endTag\", \"</b>\");\r\n",
							"\r\n",
							"hyperspace.Explain(eqJoin, true, input => DisplayHTML(input));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"var extraDepartments = new List<GenericRow>()\r\n",
							"{\r\n",
							"    new GenericRow(new object[] {50, \"Inovation\", \"Seattle\"}),\r\n",
							"    new GenericRow(new object[] {60, \"Human Resources\", \"San Francisco\"})\r\n",
							"};\r\n",
							"\t  \r\n",
							"DataFrame extraDeptData = spark.CreateDataFrame(extraDepartments, departmentSchema);\r\n",
							"extraDeptData.Write().Mode(\"Append\").Parquet(deptLocation);\r\n",
							"\r\n",
							"DataFrame deptDFrameUpdated = spark.Read().Parquet(deptLocation);\r\n",
							"\r\n",
							"deptDFrameUpdated.Show(10);\r\n",
							"\r\n",
							"hyperspace.RefreshIndex(\"deptIndex1\");"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"DataFrame newRangeFilter = deptDFrameUpdated.Filter(\"deptId > 20\").Select(\"deptName\");\r\n",
							"newRangeFilter.Show();\r\n",
							"\r\n",
							"hyperspace.Explain(newRangeFilter, true, input => DisplayHTML(input));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// GENERATE TEST DATA\r\n",
							"using Microsoft.Spark.Sql.Types;\r\n",
							"\r\n",
							"var products = new List<GenericRow>() {\r\n",
							"    new GenericRow(new object[] {\"orange\", 3, \"2020-10-01\"}),\r\n",
							"    new GenericRow(new object[] {\"banana\", 1, \"2020-10-01\"}),\r\n",
							"    new GenericRow(new object[] {\"carrot\", 5, \"2020-10-02\"}),\r\n",
							"    new GenericRow(new object[] {\"beetroot\", 12, \"2020-10-02\"}),\r\n",
							"    new GenericRow(new object[] {\"orange\", 2, \"2020-10-03\"}),\r\n",
							"    new GenericRow(new object[] {\"banana\", 11, \"2020-10-03\"}),\r\n",
							"    new GenericRow(new object[] {\"carrot\", 3, \"2020-10-03\"}),\r\n",
							"    new GenericRow(new object[] {\"beetroot\", 2, \"2020-10-04\"}),\r\n",
							"    new GenericRow(new object[] {\"cucumber\", 7, \"2020-10-05\"}),\r\n",
							"    new GenericRow(new object[] {\"pepper\", 20, \"2020-10-06\"})\r\n",
							"};\r\n",
							"var productsSchema = new StructType(new List<StructField>()\r\n",
							"{\r\n",
							"    new StructField(\"name\", new StringType()),\r\n",
							"    new StructField(\"qty\", new IntegerType()),\r\n",
							"    new StructField(\"date\", new StringType())\r\n",
							"});\r\n",
							"\r\n",
							"DataFrame testData = spark.CreateDataFrame(products, productsSchema); \r\n",
							"string testDataLocation = $\"{dataPath}/productTable\";\r\n",
							"testData.Write().Mode(\"overwrite\").Parquet(testDataLocation);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// CREATE INDEX\r\n",
							"DataFrame testDF = spark.Read().Parquet(testDataLocation);\r\n",
							"var productIndex2Config = new IndexConfig(\"productIndex1120\", new string[] {\"name\"}, new string[] {\"date\", \"qty\"});\r\n",
							"hyperspace.CreateIndex(testDF, productIndex2Config);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"DataFrame filter1 = testDF.Filter(\"name = 'banana'\");\r\n",
							"DataFrame filter2 = testDF.Filter(\"qty > 10\");\r\n",
							"DataFrame query = filter1.Join(filter2, filter1.Col(\"name\") == filter2.Col(\"name\"));\r\n",
							"\r\n",
							"query.Show();\r\n",
							"\r\n",
							"hyperspace.Explain(query, true, input => DisplayHTML(input));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Append new files.\r\n",
							"var appendProducts = new List<GenericRow>()\r\n",
							"{\r\n",
							"    new GenericRow(new object[] {\"orange\", 13, \"2020-11-01\"}),\r\n",
							"    new GenericRow(new object[] {\"banana\", 5, \"2020-11-01\"})\r\n",
							"};\r\n",
							"    \r\n",
							"DataFrame appendData = spark.CreateDataFrame(appendProducts, productsSchema);\r\n",
							"appendData.Write().Mode(\"Append\").Parquet(testDataLocation);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Hybrid Scan configs are false by default.\r\n",
							"spark.Conf().Set(\"spark.hyperspace.index.hybridscan.enabled\", \"false\");\r\n",
							"spark.Conf().Set(\"spark.hyperspace.index.hybridscan.delete.enabled\", \"false\");\r\n",
							"\r\n",
							"DataFrame testDFWithAppend = spark.Read().Parquet(testDataLocation);\r\n",
							"DataFrame filter1 = testDFWithAppend.Filter(\"name = 'banana'\");\r\n",
							"DataFrame filter2 = testDFWithAppend.Filter(\"qty > 10\");\r\n",
							"DataFrame query = filter1.Join(filter2, filter1.Col(\"name\") == filter2.Col(\"name\"));\r\n",
							"\r\n",
							"query.Show();\r\n",
							"\r\n",
							"hyperspace.Explain(query, true, input => DisplayHTML(input));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Enable Hybrid Scan config. \"delete\" config is not necessary.\r\n",
							"spark.Conf().Set(\"spark.hyperspace.index.hybridscan.enabled\", \"true\");\r\n",
							"// spark.Conf().Set(\"spark.hyperspace.index.hybridscan.delete.enabled\", \"true\");\r\n",
							"spark.EnableHyperspace();\r\n",
							"// Need to redefine query to recalculate the query plan.\r\n",
							"DataFrame query = filter1.Join(filter2, filter1.Col(\"name\") == filter2.Col(\"name\"));\r\n",
							"\r\n",
							"query.Show();\r\n",
							"\r\n",
							"hyperspace.Explain(query, true, input => DisplayHTML(input));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"using Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Notebook.MSSparkUtils;\r\n",
							"\r\n",
							"FS.Rm(dataPath, true);\r\n",
							"FS.Rm(indexLocation, true);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 28
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/HitchhikerGuide to Delta Lake_Scala')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder/sample"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "46c20f98-bb79-4f39-b7df-eea5ea9bc2af"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "scala"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"val sessionId = scala.util.Random.nextInt(1000000)\n",
							"val deltaTablePath = s\"/delta/delta-table-$sessionId\";"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"val data = spark.range(0, 5)\n",
							"data.show\n",
							"data.write.format(\"delta\").save(deltaTablePath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"spark.read.text(s\"$deltaTablePath/_delta_log/\").collect.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"val df = spark.read.format(\"delta\").load(deltaTablePath)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"val data = spark.range(5, 10)\n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(deltaTablePath)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"spark.read.text(s\"$deltaTablePath/_delta_log/\").collect.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"// Write data to a new managed catalog table.\n",
							"data.write.format(\"delta\").saveAsTable(\"ManagedDeltaTable\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"// Define an external catalog table that points to the existing Delta Lake data in storage.\n",
							"spark.sql(s\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '$deltaTablePath'\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"// List the 2 new tables.\n",
							"spark.sql(\"SHOW TABLES\").show\n",
							"\n",
							"// Explore their properties.\n",
							"spark.sql(\"DESCRIBE EXTENDED ManagedDeltaTable\").show(truncate=false)\n",
							"spark.sql(\"DESCRIBE EXTENDED ExternalDeltaTable\").show(truncate=false)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							"import io.delta.tables._\n",
							"import org.apache.spark.sql.functions._\n",
							"\n",
							"val deltaTable = DeltaTable.forPath(deltaTablePath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"// Update every even value by adding 100 to it\n",
							"deltaTable.update(\n",
							"  condition = expr(\"id % 2 == 0\"),\n",
							"  set = Map(\"id\" -> expr(\"id + 100\")))\n",
							"deltaTable.toDF.show"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"// Delete every even value\n",
							"deltaTable.delete(condition = expr(\"id % 2 == 0\"))\n",
							"deltaTable.toDF.show"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"// Upsert (merge) new data\n",
							"val newData = spark.range(0, 20).toDF\n",
							"\n",
							"deltaTable.as(\"oldData\").\n",
							"  merge(\n",
							"    newData.as(\"newData\"),\n",
							"    \"oldData.id = newData.id\").\n",
							"  whenMatched.\n",
							"  update(Map(\"id\" -> lit(-1))).\n",
							"  whenNotMatched.\n",
							"  insert(Map(\"id\" -> col(\"newData.id\"))).\n",
							"  execute()\n",
							"\n",
							"deltaTable.toDF.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.history.show(false)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"val df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(deltaTablePath)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"val streamingDf = spark.readStream.format(\"rate\").load()\n",
							"val stream = streamingDf.select($\"value\" as \"id\").writeStream.format(\"delta\").option(\"checkpointLocation\", s\"/tmp/checkpoint-$sessionId\").start(deltaTablePath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.toDF.sort($\"id\".desc).show"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.history.show"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"source": [
							"stream.stop"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.history.show"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"source": [
							"val partitionCount = 2\n",
							"\n",
							"spark.\n",
							"    read.\n",
							"    format(\"delta\").\n",
							"    load(deltaTablePath).\n",
							"    repartition(partitionCount).\n",
							"    write.\n",
							"    option(\"dataChange\", \"false\").\n",
							"    format(\"delta\").\n",
							"    mode(\"overwrite\").\n",
							"    save(deltaTablePath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"val parquetPath = s\"/parquet/parquet-table-$sessionId\"\n",
							"\n",
							"val data = spark.range(0,5)\n",
							"data.write.parquet(parquetPath)\n",
							"\n",
							"// Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(parquetPath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"source": [
							"DeltaTable.convertToDelta(spark, s\"parquet.`$parquetPath`\")\n",
							"\n",
							"// Confirm that the converted data is now in the Delta format\n",
							"DeltaTable.isDeltaTable(parquetPath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(s\"DESCRIBE HISTORY delta.`$deltaTablePath`\").show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(s\"VACUUM delta.`$deltaTablePath`\").show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"source": [
							"val parquetId = scala.util.Random.nextInt(1000)\n",
							"val parquetPath = s\"/parquet/parquet-table-$sessionId-$parquetId\"\n",
							"\n",
							"val data = spark.range(0,5)\n",
							"data.write.parquet(parquetPath)\n",
							"\n",
							"// Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(parquetPath)\n",
							"\n",
							"// Use SQL to convert the parquet table to Delta\n",
							"spark.sql(s\"CONVERT TO DELTA parquet.`$parquetPath`\")\n",
							"\n",
							"DeltaTable.isDeltaTable(parquetPath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 25
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Hitchhikers Guide to DotNET for Apache Spark csharp')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder/sample"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fc61ae91-2e41-4fcf-8027-c86a93ca6f68"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparkdotnet",
						"display_name": "csharp"
					},
					"language_info": {
						"name": "csharp"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"// Simple assignments should just work \n",
							"var x = 1 + 25;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"// You can either use traditional approach to printing a variable...\n",
							"Console.WriteLine(x);\n",
							"\n",
							"// ... or just type it and execute a cell\n",
							"256"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"// You can even play with built-in libraries/functions\n",
							"Enumerable.Range(1, 5)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"// And now for some C# 8.0 features. If you haven't read it,\n",
							"// here's the link: \n",
							"// https://docs.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-8\n",
							"1..4"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"// We can even do pattern matching!\n",
							"public static string RockPaperScissors(string first, string second)\n",
							"    => (first, second) switch\n",
							"    {\n",
							"        (\"rock\", \"paper\") => \"rock is covered by paper. Paper wins.\", // <-- Next cell prints this out\n",
							"        (\"rock\", \"scissors\") => \"rock breaks scissors. Rock wins.\",\n",
							"        (\"paper\", \"rock\") => \"paper covers rock. Paper wins.\",\n",
							"        (\"paper\", \"scissors\") => \"paper is cut by scissors. Scissors wins.\",\n",
							"        (\"scissors\", \"rock\") => \"scissors is broken by rock. Rock wins.\",\n",
							"        (\"scissors\", \"paper\") => \"scissors cuts paper. Scissors wins.\",\n",
							"        (_, _) => \"tie\"\n",
							"    };"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"RockPaperScissors(\"rock\", \"paper\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"// Now, for the fun part! You can render HTML\n",
							"display(\n",
							"    div(\n",
							"        h1(\"Our Incredibly Declarative Example\"),\n",
							"        p(\"Can you believe we wrote this \", b(\"in C#\"), \"?\"),\n",
							"        img[src:\"https://media.giphy.com/media/xUPGcguWZHRC2HyBRS/giphy.gif\"],\n",
							"        p(\"What will \", b(\"you\"), \" create next?\")\n",
							"    )\n",
							");"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"// Let us use some sample data. In this cell, we create this data \n",
							"// from *scratch* but you can also load it from your storage container. \n",
							"// For instance, \n",
							"// var df = spark.Read().Json(\"wasbs://<account>@<container>.blob.core.windows.net/people.json\");\n",
							"\n",
							"using Microsoft.Spark.Sql;\n",
							"using Microsoft.Spark.Sql.Types;\n",
							"using static Microsoft.Spark.Sql.Functions;\n",
							"\n",
							"var schema = new StructType(new List<StructField>()\n",
							"    {\n",
							"        new StructField(\"id\", new IntegerType()),\n",
							"        new StructField(\"name\", new StringType())\n",
							"    });\n",
							"\n",
							"var data = new List<GenericRow>();\n",
							"data.Add(new GenericRow(new object[] { 0,  \"Michael\" }));\n",
							"data.Add(new GenericRow(new object[] { 1,  \"Elva\"    }));\n",
							"data.Add(new GenericRow(new object[] { 2,  \"Terry\"   }));\n",
							"data.Add(new GenericRow(new object[] { 3,  \"Steve\"   }));\n",
							"data.Add(new GenericRow(new object[] { 4,  \"Brigit\"  }));\n",
							"data.Add(new GenericRow(new object[] { 5,  \"Niharika\"}));\n",
							"data.Add(new GenericRow(new object[] { 6,  \"Rahul\"   }));\n",
							"data.Add(new GenericRow(new object[] { 7,  \"Tomas\"   }));\n",
							"data.Add(new GenericRow(new object[] { 8,  \"Euan\"   }));\n",
							"data.Add(new GenericRow(new object[] { 9,  \"Lev\"   }));\n",
							"data.Add(new GenericRow(new object[] { 10, \"Saveen\"   }));\n",
							"\n",
							"var df = spark.CreateDataFrame(data, schema);\n",
							"df.Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"// What we're doing here is to define a specific formatter that is tied to \n",
							"// Microsoft.Spark.Sql.DataFrame and registering it. When we then invoke\n",
							"// display() and pass a DataFrame, the formatter is invoked, which then\n",
							"// generates the necessary HTML\n",
							"\n",
							"Microsoft.DotNet.Interactive.Formatting.Formatter<Microsoft.Spark.Sql.DataFrame>.Register((df, writer) =>\n",
							"{\n",
							"    var headers = new List<dynamic>();\n",
							"    var columnNames = df.Columns();\n",
							"    headers.Add(th(i(\"index\")));\n",
							"    headers.AddRange(columnNames.Select(c => th(c)));\n",
							"\n",
							"    var rows = new List<List<dynamic>>();\n",
							"    var currentRow = 0;\n",
							"    var dfRows = df.Take(Math.Min(20, (int)df.Count()));\n",
							"    foreach (Row dfRow in dfRows)\n",
							"    {\n",
							"        var cells = new List<dynamic>();\n",
							"        cells.Add(td(currentRow));\n",
							"\n",
							"        foreach (string columnName in columnNames)\n",
							"        {\n",
							"            cells.Add(td(dfRow.Get(columnName)));\n",
							"        }\n",
							"\n",
							"        rows.Add(cells);\n",
							"        ++currentRow;\n",
							"    }\n",
							"\n",
							"    var t = table[@border: \"0.1\"](\n",
							"        thead[@style: \"background-color: blue; color: white\"](headers),\n",
							"        tbody[@style: \"color: red\"](rows.Select(r => tr(r))));\n",
							"\n",
							"    writer.Write(t);\n",
							"}, \"text/html\");"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							"// Now, let's try rendering the Spark's DataFrame in two ways...\n",
							"\n",
							"// ... a regular way ...\n",
							"df.Show();\n",
							"\n",
							"// ... and just typing df (so it invokes the formatter we just defined)\n",
							"display(df);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"// ... and just typing df (equivalent to \"display(df);\")\n",
							"df"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"\n",
							"// Let us now try something more advanced like, defining C# classes on-the-fly...\n",
							"public static class A {\n",
							"    public static readonly string s = \"The person named \";\n",
							"}"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"// ... and just for illustration, let's define one more simple class\n",
							"public static class B {\n",
							"    private static Random _r = new Random();\n",
							"    private static List<string> _moods = new List<string>{ \"happy\",\"funny\",\"awesome\",\"cool\"};\n",
							"\n",
							"    public static string GetMood() {\n",
							"        return _moods[_r.Next(_moods.Count)];\n",
							"    }\n",
							"}"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							"// Let us now define a Spark User-defined Function (UDF) that utilizes\n",
							"// the classes we just defined above. If you do not recognize the syntax\n",
							"// below, here's some relevant documentation:\n",
							"// https://docs.microsoft.com/en-us/dotnet/api/system.func-2?view=netframework-4.8\n",
							"// https://github.com/dotnet/spark/blob/master/examples/Microsoft.Spark.CSharp.Examples/Sql/Basic.cs\n",
							"//\n",
							"// Note: If you change the class definition above, and execute the cell,\n",
							"// you should re-execute this cell (i.e., the cell that defines the UDF)\n",
							"var udf = Udf<string, string>(str => $\"{A.s} - {str} - is {B.GetMood()}!\");"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"// Let's use the UDF on our Spark DataFrame\n",
							"display(\n",
							"    df\n",
							"    .Select(\n",
							"        udf((Microsoft.Spark.Sql.Column)df[\"name\"])));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"// Tables are not that interesting, right? :) Let's do some visualizations now.\n",
							"// Let us start with something simple to illustrate the idea. We highly encourage\n",
							"// you to look at https://fslab.org/XPlot/ to understand how you can use XPlot's\n",
							"// full capabilities. While the examples are in F#, it is fairly straightforward\n",
							"// to rewrite in C#.\n",
							"\n",
							"using XPlot.Plotly;\n",
							"\n",
							"var lineChart = Chart.Line(new List<int> { 1, 2, 3, 4, 5, 6, 10, 44 });\n",
							"lineChart.WithTitle(\"My awesome chart\");\n",
							"lineChart.WithXTitle(\"X axis\");\n",
							"lineChart.WithYTitle(\"Y axis\");\n",
							"display(lineChart);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"source": [
							"// Good! Now let us try to visualize the Spark DataFrame we have.\n",
							"// Now is a good time to refresh your concept of a Spark DataFrame\n",
							"// https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
							"// Remember that a Spark DataFrame is a distributed representation \n",
							"// of your dataset (yes, even if your data is a few KB). Since we\n",
							"// are using a visualization library, we need to first 'collect'\n",
							"// (notice how we are using df.Collect().ToArray() below)\n",
							"// all the data that is distributed on your cluster, and shape it\n",
							"// appropriately for XPlot.\n",
							"//\n",
							"// Note: Visualizations are good for smaller datasets (typically, \n",
							"// a few 10s of thousands of data points coming to KBs), so if you are\n",
							"// trying to visualize GBs of data, it is usually a good idea to\n",
							"// summarize your data appropriately using Spark.NET's APIs. For\n",
							"// a list of summarization APIs, see here:\n",
							"// https://docs.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.functions?view=spark-dotnet\n",
							"\n",
							"var names = new List<string>();\n",
							"var ids = new List<int>();\n",
							"\n",
							"foreach (Row row in df.Collect().ToArray())\n",
							"{\n",
							" names.Add(row.GetAs<string>(\"name\"));\n",
							" int? id = row.GetAs<int?>(\"id\");\n",
							" ids.Add( id ?? 0);\n",
							"}\n",
							"var bar = new Graph.Bar\n",
							"{\n",
							" name = \"bar chart\",\n",
							" x = names,\n",
							" y = ids\n",
							"};\n",
							"\n",
							"var chart = Chart.Plot(new[] {bar});\n",
							"display(chart);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"// As a final step, let us now plot a histogram of a random dataset\n",
							"\n",
							"using XPlot.Plotly;\n",
							"\n",
							"var schema = new StructType(new List<StructField>()\n",
							"    {\n",
							"        new StructField(\"number\", new DoubleType())\n",
							"    });\n",
							"\n",
							"Random random = new Random();\n",
							"\n",
							"var data = new List<GenericRow>();\n",
							"for(int i = 0; i <=100; i++) {\n",
							"    data.Add(new GenericRow(new object[] { random.NextDouble() }));\n",
							"}\n",
							"\n",
							"var histogramDf = spark.CreateDataFrame(data, schema);\n",
							"histogramDf.Show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"source": [
							"// Time to use LINQ (or Language Integrated Query) :)\n",
							"// For those that are not familiar with LINQ, you can read more about it\n",
							"// here: https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/\n",
							"\n",
							"using System.Linq;\n",
							"\n",
							"// Let us take the histogramDf we loaded through Spark and sample some data points\n",
							"// for the histogram. We will then use LINQ to shape the data for our next \n",
							"// steps (visualization!)\n",
							"var sample1 = \n",
							"        histogramDf.Sample(0.5, true).Collect().ToArray() // <---- Spark APIs\n",
							"        .Select(x => x.GetAs<double>(\"number\")); // <---- LINQ APIs\n",
							"        \n",
							"// Let us create two more sample sets we can use for plotting\n",
							"var sample2 = histogramDf.Sample(0.3, false).Collect().ToArray().Select(x => x.GetAs<double>(\"number\"));\n",
							"var sample3 = histogramDf.Sample(0.6, true).Collect().ToArray().Select(x => x.GetAs<double>(\"number\"));"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"source": [
							"// Let us plot the histograms now!\n",
							"var hist1 = new Graph.Histogram{x = sample1, opacity = 0.75};\n",
							"var hist2 = new Graph.Histogram{x = sample2, opacity = 0.75};\n",
							"var hist3 = new Graph.Histogram{x = sample3, opacity = 0.75};"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"source": [
							"Chart.Plot(new[] {hist1})"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"source": [
							"Chart.Plot(new[] {hist2})"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"source": [
							"Chart.Plot(new[] {hist3})"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"source": [
							"// but wait, that's three different graphs and it's impossible to read them\n",
							"// altogether! Let's try an overlay histogram, shall we?\n",
							"var layout = new XPlot.Plotly.Layout.Layout{barmode=\"overlay\", title=\"Overlaid Histogram\"};\n",
							"var histogram = Chart.Plot(new[] {hist1, hist2, hist3});\n",
							"histogram.WithLayout(layout);\n",
							"histogram"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"source": [
							"// And for the final touches\n",
							"using static XPlot.Plotly.Graph;\n",
							"\n",
							"layout.title = \"Overlaid Histogram with cool colors!\";\n",
							"hist1.marker = new Marker {color = \"#D65108)\"};\n",
							"hist2.marker = new Marker {color = \"#ffff00\"}; \n",
							"hist3.marker = new Marker {color = \"#462255\"};\n",
							"\n",
							"histogram"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"source": [
							"// Let's construct a VectorUdf by directly using Arrow.\n",
							"using Apache.Arrow;\n",
							"using static Microsoft.Spark.Sql.ArrowFunctions;\n",
							"using Column = Microsoft.Spark.Sql.Column;\n",
							"\n",
							"// Helper method to construct an ArrowArray from a string[].\n",
							"public static IArrowArray ToStringArrowArray(string[] array)\n",
							"{\n",
							"    var valueOffsets = new ArrowBuffer.Builder<int>();\n",
							"    var valueBuffer = new ArrowBuffer.Builder<byte>();\n",
							"    int offset = 0;\n",
							"\n",
							"    foreach (string str in array)\n",
							"    {\n",
							"        byte[] bytes = Encoding.UTF8.GetBytes(str);\n",
							"        valueOffsets.Append(offset);\n",
							"        valueBuffer.Append(bytes);\n",
							"        offset += bytes.Length;\n",
							"    }\n",
							"\n",
							"    valueOffsets.Append(offset);\n",
							"    return new StringArray(\n",
							"        new ArrayData(\n",
							"            Apache.Arrow.Types.StringType.Default,\n",
							"            valueOffsets.Length - 1,\n",
							"            0,\n",
							"            0,\n",
							"            new[] { ArrowBuffer.Empty, valueOffsets.Build(), valueBuffer.Build() }));\n",
							"}\n",
							"\n",
							"Func<Int32Array, StringArray, StringArray> arrowUdf =\n",
							"    (ids, names) => (StringArray)ToStringArrowArray(\n",
							"        Enumerable.Range(0, names.Length)\n",
							"            .Select(i => $\"id: {ids.GetValue(i)}, name: {names.GetString(i)}\")\n",
							"            .ToArray());\n",
							"\n",
							"Func<Column, Column, Column> vectorUdf1 = VectorUdf(arrowUdf);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"source": [
							"df.Select(vectorUdf1(df[\"id\"], df[\"name\"]))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"source": [
							"// Now let's construct a VectorUdf by using Microsoft Dataframe\n",
							"using Microsoft.Data.Analysis;\n",
							"using static Microsoft.Spark.Sql.DataFrameFunctions;\n",
							"\n",
							"Func<Int32DataFrameColumn, ArrowStringDataFrameColumn, ArrowStringDataFrameColumn> msftDfFunc =\n",
							"    (ids, names) =>\n",
							"    {\n",
							"        long i = 0;\n",
							"        return names.Apply(cur => $\"id: {ids[i++]}, name: {cur}\");\n",
							"    };\n",
							"\n",
							"Func<Column, Column, Column> vectorUdf2 = VectorUdf(msftDfFunc);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"source": [
							"df.Select(vectorUdf2(df[\"id\"], df[\"name\"]))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"source": [
							"// Use #r to install new packages into the current session\n",
							"\n",
							"// Installs latest version\n",
							"#r \"nuget: MathNet.Numerics\"\n",
							"\n",
							"// Installs specified version\n",
							"#r \"nuget: NumSharp,0.20.5\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"source": [
							"using MathNet.Numerics.LinearAlgebra;\n",
							"using MathNet.Numerics.LinearAlgebra.Double;\n",
							"using NumSharp;\n",
							"\n",
							"var mathNetUdf = Udf<string, string>(str => {\n",
							"    Matrix<double> matrix = DenseMatrix.OfArray(new double[,] {\n",
							"        {1,1,1,1},\n",
							"        {1,2,3,4},\n",
							"        {4,3,2,1}});\n",
							"\n",
							"    return $\"{matrix[0, 0]} - {str} - {matrix[1, 1]}!\";\n",
							"});\n",
							"\n",
							"var numSharpUdf = Udf<string, string>(str => {\n",
							"    var nd = np.arange(12);\n",
							"\n",
							"    return $\"{nd[1].ToString()} - {str} - {nd[5].ToString()}!\";\n",
							"});"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"source": [
							"\n",
							"// UDFs are run on the Microsoft.Spark.Worker process. The package assemblies\n",
							"// defined as a Udf depedency are shipped to the Worker so they are available\n",
							"// at the time of execution.\n",
							"df.Select(mathNetUdf(df[\"name\"])).Show();\n",
							"\n",
							"df.Select(numSharpUdf(df[\"name\"])).Show();\n",
							"\n",
							"// We can also chain udfs.\n",
							"df.Select(mathNetUdf(numSharpUdf(df[\"name\"])))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"source": [
							"// Utility for obtaining credentials (tokens and keys) for Synapse resources.\n",
							"// Credentials methods https://dev.azure.com/dnceng/internal/_git/dotnet-spark-extensions?path=%2Fsrc%2FMicrosoft.Spark.Extensions.Azure.Synapse.Analytics%2FNotebook%2FMSSparkUtils%2FCredentials.cs\n",
							"using Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Notebook.MSSparkUtils;\n",
							"\n",
							"// Note that the help message is the help message returned by the Scala implementation. This needs to be updated and addressed in a future version.\n",
							"Console.WriteLine($\"Help:\\n{Credentials.Help()}\");"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"source": [
							"// Utility for obtaining environment metadata for Synapse.\n",
							"// Env methods https://dev.azure.com/dnceng/internal/_git/dotnet-spark-extensions?path=%2Fsrc%2FMicrosoft.Spark.Extensions.Azure.Synapse.Analytics%2FNotebook%2FMSSparkUtils%2FEnv.cs\n",
							"Console.WriteLine($\"UserName: {Env.GetUserName()}\");\n",
							"Console.WriteLine($\"UserId: {Env.GetUserId()}\");\n",
							"Console.WriteLine($\"WorkspaceName: {Env.GetWorkspaceName()}\");\n",
							"Console.WriteLine($\"PoolName: {Env.GetPoolName()}\");\n",
							"Console.WriteLine($\"ClusterId: {Env.GetClusterId()}\");\n",
							"\n",
							"// Note that the help message is the help message returned by the Scala implementation. This needs to be updated and addressed in a future version.\n",
							"Console.WriteLine($\"Help:\\n{Env.Help()}\");"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"source": [
							"// Utility for filesystem operations in Synapse notebook\n",
							"// FS methods https://dev.azure.com/dnceng/internal/_git/dotnet-spark-extensions?path=%2Fsrc%2FMicrosoft.Spark.Extensions.Azure.Synapse.Analytics%2FNotebook%2FMSSparkUtils%2FFS.cs\n",
							"// FileInfo methods https://dev.azure.com/dnceng/internal/_git/dotnet-spark-extensions?path=%2Fsrc%2FMicrosoft.Spark.Extensions.Azure.Synapse.Analytics%2FNotebook%2FMSSparkUtils%2FFileInfo.cs\n",
							"\n",
							"// Note that the help message is the help message returned by the Scala implementation. This needs to be updated and addressed in a future version.\n",
							"FS.Help(\"\");"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"source": [
							"// Utility for notebook operations (e.g, chaining Synapse notebooks together)\n",
							"// Notebook methods https://dev.azure.com/dnceng/internal/_git/dotnet-spark-extensions?path=%2Fsrc%2FMicrosoft.Spark.Extensions.Azure.Synapse.Analytics%2FNotebook%2FMSSparkUtils%2FNotebook.cs\n",
							"\n",
							"// Note that the help message is the help message returned by the Scala implementation. This needs to be updated and addressed in a future version.\n",
							"Notebook.Help(\"\");"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"source": [
							"using Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Notebook.Visualization;\n",
							"// Construct an specific html fragment to synapse notebook front-end for rendering\n",
							"// based on user-input html content.\n",
							"DisplayHTML(\"<h1>Hello World</h1>\");"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"source": [
							"using Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Utils;\n",
							"\n",
							"// Note that the help message is the help message returned by the Scala implementation. This needs to be updated and addressed in a future version.\n",
							"// TODO: Methodname needs to be uppercase.\n",
							"Console.WriteLine($\"Help:\\n{TokenLibrary.help()}\");"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"source": [
							"// Curious about the version of Spark .NET currently installed?\n",
							"// Let's use the following method to find out!\n",
							"using Microsoft.Spark.Experimental.Sql;\n",
							"spark.GetAssemblyInfo()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"source": [
							"// Current version of the dotnet-interactive REPL.\n",
							"#!about"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"source": [
							"// We can even run powershell core commands\n",
							"#!pwsh\n",
							"cat /etc/hosts"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"source": [
							"// We can also run F# code\n",
							"#!fsharp\n",
							"open System\n",
							"printfn \"Hello World from F#!\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"source": [
							"// Whatever code is deemed invalid by the C# Compiler, is invalid here too \n",
							"var z = 12345"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"source": [
							"// You could write code that throws exceptions and they bubble up to the notebook\n",
							"throw new Exception(\"watzzz\");"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 45
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Hitchhikers Guide to Hyperspace - Python')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder/python"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "dcd6d311-c777-4371-ac74-79f6e80c8c4d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Hitchhiker's Guide to Hyperspace (Python)\n",
							"## An Indexing Subsystem for Apache Spark\n",
							"\n",
							"<img src=\"https://raw.githubusercontent.com/rapoth/hyperspace/master/docs/assets/images/hyperspace-small-banner.png\" alt=\"Hyperspace Indexing Sub-System Logo\" width=\"1000\"/>\n",
							"\n",
							"[Hyperspace](https://github.com/microsoft/hyperspace) introduces the ability for Apache Spark users to create indexes on their datasets (e.g., CSV, JSON, Parquet etc.) and leverage them for potential query and workload acceleration.\n",
							"\n",
							"In this notebook, we highlight the basics of Hyperspace, emphasizing on its simplicity and show how it can be used by just anyone.\n",
							"\n",
							"**Disclaimer**: Hyperspace helps accelerate your workloads/queries under two circumstances:\n",
							"\n",
							"  1. Queries contain filters on predicates with high selectivity (e.g., you want to select 100 matching rows from a million candidate rows)\n",
							"  2. Queries contain a join that requires heavy-shuffles (e.g., you want to join a 100 GB dataset with a 10 GB dataset)\n",
							"\n",
							"You may want to carefully monitor your workloads and determine whether indexing is helping you on a case-by-case basis."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Setup\n",
							"To begin with, let's start a new Spark session. Since this notebook is a tutorial merely to illustrate what Hyperspace can offer, we will make a configuration change that allow us to highlight what Hyperspace is doing on small datasets. By default, Spark uses *broadcast join* to optimize join queries when the data size for one side of join is small (which is the case for the sample data we use in this tutorial). Therefore, we disable broadcast joins so that later when we run join queries, Spark uses *sort-merge* join. This is mainly to show how Hyperspace indexes would be used at scale for accelerating join queries.\n",
							"\n",
							"The output of running the cell below shows a reference to the successfully created Spark session and prints out '-1' as the value for the modified join config which indicates that broadcast join is successfully disabled."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"import random\n",
							"\n",
							"session_id = random.randint(0,1000000)\n",
							"data_path = \"/hyperspace/data-{0}\".format(session_id)\n",
							"index_location = \"/hyperspace/indexes-{0}\".format(session_id)\n",
							"\n",
							"# Please note that you DO NOT need to change this configuration in production.\n",
							"# We store all indexes in the system folder within Synapse.\n",
							"spark.conf.set(\"spark.hyperspace.system.path\", index_location)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Start your Spark session\n",
							"spark\n",
							"\n",
							"# Disable BroadcastHashJoin, so Spark will use standard SortMergeJoin. Currently Hyperspace indexes utilize SortMergeJoin to speed up query.\n",
							"spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
							"\n",
							"# Verify that BroadcastHashJoin is set correctly \n",
							"print(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))\n",
							"spark.conf.set(\"spark.hyperspace.explain.displayMode\", \"html\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data Preparation\n",
							"\n",
							"To prepare our environment, we will create sample data records and save them as parquet data files. While we use Parquet for illustration, you can use other formats such as CSV. In the subsequent cells, we will also demonstrate how you can create several Hyperspace indexes on this sample dataset and how one can make Spark use them when running queries. \n",
							"\n",
							"Our example records correspond to two datasets: *department* and *employee*. You should configure \"empLocation\" and \"deptLocation\" paths so that on the storage account they point to your desired location to save generated data files. \n",
							"\n",
							"The output of running below cell shows contents of our datasets as lists of triplets followed by references to dataFrames created to save the content of each dataset in our preferred location."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
							"\n",
							"# Sample department records\n",
							"departments = [(10, \"Accounting\", \"New York\"), (20, \"Research\", \"Dallas\"), (30, \"Sales\", \"Chicago\"), (40, \"Operations\", \"Boston\")]\n",
							"\n",
							"# Sample employee records\n",
							"employees = [(7369, \"SMITH\", 20), (7499, \"ALLEN\", 30), (7521, \"WARD\", 30), (7566, \"JONES\", 20), (7698, \"BLAKE\", 30)]\n",
							"\n",
							"# Create a schema for the dataframe\n",
							"dept_schema = StructType([StructField('deptId', IntegerType(), True), StructField('deptName', StringType(), True), StructField('location', StringType(), True)])\n",
							"emp_schema = StructType([StructField('empId', IntegerType(), True), StructField('empName', StringType(), True), StructField('deptId', IntegerType(), True)])\n",
							"\n",
							"departments_df = spark.createDataFrame(departments, dept_schema)\n",
							"employees_df = spark.createDataFrame(employees, emp_schema)\n",
							"\n",
							"emp_Location = data_path + \"/employees.parquet\"\n",
							"dept_Location = data_path + \"/departments.parquet\"\n",
							"\n",
							"employees_df.write.mode(\"overwrite\").parquet(emp_Location)\n",
							"departments_df.write.mode(\"overwrite\").parquet(dept_Location)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Let's verify the contents of parquet files we created above to make sure they contain expected records in correct format. We later use these data files to create Hyperspace indexes and run sample queries.\n",
							"\n",
							"Running below cell, the output displays the rows in employee and department dataframes in a tabular form. There should be 14 employees and 4 departments, each matching with one of triplets we created in the previous cell."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# emp_Location and dept_Location are the user defined locations above to save parquet files\n",
							"emp_DF = spark.read.parquet(emp_Location)\n",
							"dept_DF = spark.read.parquet(dept_Location)\n",
							"\n",
							"# Verify the data is available and correct\n",
							"emp_DF.show()\n",
							"dept_DF.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Hello Hyperspace Index!\n",
							"Hyperspace lets users create indexes on records scanned from persisted data files. Once successfully created, an entry corresponding to the index is added to the Hyperspace's metadata. This metadata is later used by Apache Spark's Hyperspace-enabled optimizer during query processing to find and use proper indexes. \n",
							"\n",
							"Once indexes are created, users can perform several actions:\n",
							"  - **Refresh** If the underlying data changes, users can refresh an existing index to capture that. \n",
							"  - **Delete** If the index is not needed, users can perform a soft-delete i.e., index is not physically deleted but is marked as 'deleted' so it is no longer used in your workloads.\n",
							"  - **Vacuum** If an index is no longer required, users can vacuum it which forces a physical deletion of the index contents and associated metadata completely from Hyperspace's metadata.\n",
							"\n",
							"Below sections show how such index management operations can be done in Hyperspace.\n",
							"\n",
							"First, we need to import the required libraries and create an instance of Hyperspace. We later use this instance to invoke different Hyperspace APIs to create indexes on our sample data and modify those indexes.\n",
							"\n",
							"Output of running below cell shows a reference to the created instance of Hyperspace."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from hyperspace import *\n",
							"\n",
							"# Create an instance of Hyperspace\n",
							"hyperspace = Hyperspace(spark)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create Indexes\n",
							"To create a Hyperspace index, the user needs to provide 2 pieces of information:\n",
							"* An Apache Spark DataFrame which references the data to be indexed.\n",
							"* An index configuration object: IndexConfig, which specifies the *index name*, *indexed* and *included* columns of the index. \n",
							"\n",
							"As you might have noticed, in this notebook, we illustrate indexing using the [Covering Index](https://www.red-gate.com/simple-talk/sql/learn-sql-server/using-covering-indexes-to-improve-query-performance/), which are the default index in Hyperspace. In the future, we plan on adding support for other index types. \n",
							"\n",
							"We start by creating three Hyperspace indexes on our sample data: two indexes on the department dataset named \"deptIndex1\" and \"deptIndex2\", and one index on the employee dataset named 'empIndex'. \n",
							"For each index, we need a corresponding IndexConfig to capture the name along with columns lists for the indexed and included columns. Running below cell creates these indexConfigs and its output lists them.\n",
							"\n",
							"**Note**: An *index column* is a column that appears in your filters or join conditions. An *included column* is a column that appears in your select/project.\n",
							"\n",
							"For instance, in the following query:\n",
							"```sql\n",
							"SELECT X\n",
							"FROM Table\n",
							"WHERE Y = 2\n",
							"```\n",
							"X can be an *index column* and Y can be an *included column*."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Create index configurations\n",
							"\n",
							"emp_IndexConfig = IndexConfig(\"empIndex1\", [\"deptId\"], [\"empName\"])\n",
							"dept_IndexConfig1 = IndexConfig(\"deptIndex1\", [\"deptId\"], [\"deptName\"])\n",
							"dept_IndexConfig2 = IndexConfig(\"deptIndex2\", [\"location\"], [\"deptName\"])"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Now, we create three indexes using our index configurations. For this purpose, we invoke \"createIndex\" command on our Hyperspace instance. This command requires an index configuration and the dataFrame containing rows to be indexed.\n",
							"Running below cell creates three indexes.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Create indexes from configurations\n",
							"\n",
							"hyperspace.createIndex(emp_DF, emp_IndexConfig)\n",
							"hyperspace.createIndex(dept_DF, dept_IndexConfig1)\n",
							"hyperspace.createIndex(dept_DF, dept_IndexConfig2)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"### List Indexes\n",
							"\n",
							"Below code shows how a user can list all available indexes in a Hyperspace instance. It uses the `indexes` API which returns information about existing indexes as a Spark's DataFrame so you can perform additional operations. For instance, you can invoke valid operations on this DataFrame for checking its content or analyzing it further (for example filtering specific indexes or grouping them according to some desired property). \n",
							"\n",
							"Below cell uses DataFrame's `show` action to fully print the rows and show details of our indexes in a tabular form. For each index, we can see all the information Hyperspace has stored about it in its metadata. \n",
							"\n",
							"You will immediately notice the following:\n",
							"  - `config.indexName`, `config.indexedColumns`, `config.includedColumns` are the fields that a user normally provides during index creation.\n",
							"  - `status.status` indicates if the index is being actively used by the Spark's optimizer.\n",
							"  - `dfSignature` is automatically generated by Hyperspace and is unique for each index. Hyperspace uses this signature internally to maintain the index and exploit it at query time. \n",
							"  \n",
							"In the output below, all three indexes should have \"ACTIVE\" as status and their name, indexed columns, and included columns should match with what we defined in index configurations above."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.indexes().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Delete Indexes\n",
							"A user can drop an existing index by using the `deleteIndex` API and providing the index name. \n",
							"\n",
							"Index deletion is a **soft-delete** operation i.e., only the index's status in the Hyperspace metadata from is changed from \"ACTIVE\" to \"DELETED\". This will exclude the deleted index from any future query optimization and Hyperspace no longer picks that index for any query. However, index files for a deleted index still remain available (since it is a soft-delete), so if you accidentally deleted the index, you could still restore it.\n",
							"\n",
							"The cell below deletes index with name \"deptIndex2\" and lists Hyperspace metadata after that. The output should be similar to above cell for \"List Indexes\" except for \"deptIndex2\" which now should have its status changed into \"DELETED\"."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.deleteIndex(\"deptIndex2\")\n",
							"\n",
							"hyperspace.indexes().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Restore Indexes\n",
							"A user can use the `restoreIndex` API to restore a deleted index. This will bring back the latest version of index into ACTIVE status and makes it usable again for queries. \n",
							"\n",
							"The cell below shows an example of `restoreIndex` API. We delete \"deptIndex1\" and restore it. The output shows \"deptIndex1\" first went into the \"DELETED\" status after invoking \"deleteIndex\" command and came back to the \"ACTIVE\" status after calling \"restoreIndex\"."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.deleteIndex(\"deptIndex1\")\n",
							"\n",
							"hyperspace.indexes().show()\n",
							"\n",
							"hyperspace.restoreIndex(\"deptIndex1\")\n",
							"\n",
							"hyperspace.indexes().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Vacuum Indexes\n",
							"The user can perform a **hard-delete** i.e., fully remove files and the metadata entry for a deleted index using the `vacuumIndex` API. Once done, this action is **irreversible** as it physically deletes all the index files associated with the index.\n",
							"\n",
							"The cell below vacuums the \"deptIndex2\" index and shows Hyperspace metadata after vaccuming. You should see metadata entries for two indexes \"deptIndex1\" and \"empIndex\" both with \"ACTIVE\" status and no entry for \"deptIndex2\"."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.vacuumIndex(\"deptIndex2\")\n",
							"hyperspace.indexes().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Enable/Disable Hyperspace\n",
							"\n",
							"Hyperspace provides APIs to enable or disable index usage with Spark.\n",
							"\n",
							"  - By using `enableHyperspace` API, Hyperspace optimization rules become visible to the Apache Spark optimizer and it will exploit existing Hyperspace indexes to optimize user queries.\n",
							"  - By using `disableHyperspace` command, Hyperspace rules no longer apply during query optimization. You should note that disabling Hyperspace has no impact on created indexes as they remain intact.\n",
							"\n",
							"Below cell shows how you can use these commands to enable or disable hyperspace. The output simply shows a reference to the existing Spark session whose configuration is updated."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Enable Hyperspace\n",
							"Hyperspace.enable(spark)\n",
							"\n",
							"# Disable Hyperspace\n",
							"Hyperspace.disable(spark)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Index Usage\n",
							"In order to make Spark use Hyperspace indexes during query processing, the user needs to make sure that Hyperspace is enabled. \n",
							"\n",
							"The cell below enables Hyperspace and creates two DataFrames containing our sample data records which we use for running example queries. For each DataFrame, a few sample rows are printed."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Enable Hyperspace\n",
							"Hyperspace.enable(spark)\n",
							"\n",
							"emp_DF = spark.read.parquet(emp_Location)\n",
							"dept_DF = spark.read.parquet(dept_Location)\n",
							"\n",
							"emp_DF.show(5)\n",
							"dept_DF.show(5)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Hyperspace's Index Types\n",
							"\n",
							"Currently, Hyperspace can exploit indexes for two groups of queries: \n",
							"* Selection queries with lookup or range selection filtering predicates.\n",
							"* Join queries with an equality join predicate (i.e. Equi-joins)."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Indexes for Accelerating Filters\n",
							"\n",
							"Our first example query does a lookup on department records (see below cell). In SQL, this query looks as follows:\n",
							"\n",
							"```sql\n",
							"SELECT deptName \n",
							"FROM departments\n",
							"WHERE deptId = 20\n",
							"```\n",
							"\n",
							"The output of running the cell below shows: \n",
							"- query result, which is a single department name.\n",
							"- query plan that Spark used to run the query. \n",
							"\n",
							"In the query plan, the \"FileScan\" operator at the bottom of the plan shows the datasource where the records were read from. The location of this file indicates the path to the latest version of the \"deptIndex1\" index. This shows  that according to the query and using Hyperspace optimization rules, Spark decided to exploit the proper index at runtime.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Filter with equality predicate\n",
							"\n",
							"eqFilter = dept_DF.filter(\"\"\"deptId = 20\"\"\").select(\"\"\"deptName\"\"\")\n",
							"eqFilter.show()\n",
							"\n",
							"hyperspace.explain(eqFilter, True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Our second example is a range selection query on department records. In SQL, this query looks as follows:\n",
							"\n",
							"```sql\n",
							"SELECT deptName \n",
							"FROM departments\n",
							"WHERE deptId > 20\n",
							"```\n",
							"Similar to our first example, the output of the cell below shows the query results (names of two departments) and the query plan. The location of data file in the FileScan operator shows that 'deptIndex1\" was used to run the query.   \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Filter with range selection predicate\n",
							"\n",
							"rangeFilter = dept_DF.filter(\"\"\"deptId > 20\"\"\").select(\"deptName\")\n",
							"rangeFilter.show()\n",
							"\n",
							"hyperspace.explain(rangeFilter, True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Our third example is a query joining department and employee records on the department id. The equivalent SQL statement is shown below:\n",
							"\n",
							"```sql\n",
							"SELECT employees.deptId, empName, departments.deptId, deptName\n",
							"FROM   employees, departments \n",
							"WHERE  employees.deptId = departments.deptId\n",
							"```\n",
							"\n",
							"The output of running the cell below shows the query results which are the names of 14 employees and the name of department each employee works in. The query plan is also included in the output. Notice how the file locations for two FileScan operators shows that Spark used \"empIndex\" and \"deptIndex1\" indexes to run the query.   \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Join\n",
							"\n",
							"eqJoin = emp_DF.join(dept_DF, emp_DF.deptId == dept_DF.deptId).select(emp_DF.empName, dept_DF.deptName)\n",
							"\n",
							"eqJoin.show()\n",
							"\n",
							"hyperspace.explain(eqJoin, True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Support for SQL Semantics\n",
							"\n",
							"The index usage is transparent to whether the user uses DataFrame API or Spark SQL. The following example shows the same join example as before but using Spark SQL, showing the use of indexes if applicable."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\n",
							"\n",
							"emp_DF.createOrReplaceTempView(\"EMP\")\n",
							"dept_DF.createOrReplaceTempView(\"DEPT\")\n",
							"\n",
							"joinQuery = spark.sql(\"SELECT EMP.empName, DEPT.deptName FROM EMP, DEPT WHERE EMP.deptId = DEPT.deptId\")\n",
							"\n",
							"joinQuery.show()\n",
							"hyperspace.explain(joinQuery, True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Explain API\n",
							"\n",
							"So far, you might have observed we have been using the explain API provided by Hyperspace. The `explain` API from Hyperspace is very similar to Spark's `df.explain` API but allows users to compare their original plan vs the updated index-dependent plan before running their query. You have an option to choose from html/plaintext/console mode to display the command output. \n",
							"\n",
							"The following cell shows an example with HTML. The highlighted section represents the difference between original and updated plans along with the indexes being used."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"eqJoin = emp_DF.join(dept_DF, emp_DF.deptId == dept_DF.deptId).select(emp_DF.empName, dept_DF.deptName)\n",
							"\n",
							"spark.conf.set(\"spark.hyperspace.explain.displayMode\", \"html\")\n",
							"hyperspace.explain(eqJoin, True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Refresh Indexes\n",
							"If the original data on which an index was created changes, then the index will no longer capture the latest state of data and hence will not be used by Hyperspace to provide any acceleration. The user can refresh such a stale index using the `refreshIndex` API. This causes the index to be fully rebuilt and updates it according to the latest data records.\n",
							"    \n",
							"    Spoiler alert: if you are worried about fully rebuilding your index every time your data changes, don't worry! We will show you how to *incrementally refresh* your index in subsequent cells below.\n",
							"\n",
							"The two cells below show an example for this scenario:\n",
							"- First cell adds two more departments to the original departments data. It reads and prints list of departments to verify new departments are added correctly. The output shows 6 departments in total: four old ones and two new. Invoking \"refreshIndex\" updates \"deptIndex1\" so index captures new departments.\n",
							"- Second cell runs our range selection query example. The results should now contain four departments: two are the ones, seen before when we ran the query above, and two are the new departments we just added."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"extra_Departments = [(50, \"Inovation\", \"Seattle\"), (60, \"Human Resources\", \"San Francisco\")]\n",
							"\n",
							"extra_departments_df = spark.createDataFrame(extra_Departments, dept_schema)\n",
							"extra_departments_df.write.mode(\"Append\").parquet(dept_Location)\n",
							"\n",
							"\n",
							"dept_DFrame_Updated = spark.read.parquet(dept_Location)\n",
							"\n",
							"dept_DFrame_Updated.show(10)\n",
							"\n",
							"hyperspace.refreshIndex(\"deptIndex1\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"newRangeFilter = dept_DFrame_Updated.filter(\"deptId > 20\").select(\"deptName\")\n",
							"newRangeFilter.show()\n",
							"\n",
							"hyperspace.explain(newRangeFilter, True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.indexes().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Clean-up the remaining indexes\n",
							"hyperspace.deleteIndex(\"empIndex1\")\n",
							"hyperspace.deleteIndex(\"deptIndex1\")\n",
							"\n",
							"hyperspace.vacuumIndex(\"empIndex1\")\n",
							"hyperspace.vacuumIndex(\"deptIndex1\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Hybrid Scan for Mutable Datasets\n",
							"\n",
							"Often times, if your underlying source data had some new files appended or existing files deleted, your index will get stale and Hyperspace decides not to use it. However, there are times where you just want to use the index without having to refresh it everytime. There could be multiple reasons for doing so:\n",
							"\n",
							"  1. You do not want to continuosly refresh your index but instead want to do it periodically since you understand your workloads the best.  \n",
							"  2. You added/removed only a few files and do not want to wait for yet another refresh job to finish. \n",
							"\n",
							"To allow you to still use a stale index, Hyperspace introduces **Hybrid Scan**, a novel technique that allows users to utilize outdated or stale indexes (e.g., the underlying source data had some new files appended or existing files deleted), without refreshing indexes. \n",
							"\n",
							"To achieve this, when you set the appropriate configuration to enable Hybrid Scan, Hyperspace modifies the query plan to leverage the changes as following:\n",
							"- Appended files can be merged to index data by using **`Union` or `BucketUnion` (for join)**. Shuffling appended data can also be applied before merging, if needed.\n",
							"- Deleted files can be handled by injecting `Filter-NOT-IN` condition on **lineage column** of index data, so that the indexed rows from the deleted files can be excluded at query time. \n",
							"\n",
							"You can check the transformation of the query plan in below examples.\n",
							"\n",
							"    Note: Hybrid scan is only supported for non-partitioned data. Support for partitioned data is currently being worked upon."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Hybrid Scan for appended files - non-partitioned data\n",
							"\n",
							"Non-partitioned data is used in below example. In this example, we expect Join index can be used for the query and `BucketUnion` is introduced for appended files."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# GENERATE TEST DATA\n",
							"\n",
							"testdata = [\n",
							"    (\"orange\", 3, \"2020-10-01\"),\n",
							"    (\"banana\", 1, \"2020-10-01\"),\n",
							"    (\"carrot\", 5, \"2020-10-02\"),\n",
							"    (\"beetroot\", 12, \"2020-10-02\"),\n",
							"    (\"orange\", 2, \"2020-10-03\"),\n",
							"    (\"banana\", 11, \"2020-10-03\"),\n",
							"    (\"carrot\", 3, \"2020-10-03\"),\n",
							"    (\"beetroot\", 2, \"2020-10-04\"),\n",
							"    (\"cucumber\", 7, \"2020-10-05\"),\n",
							"    (\"pepper\", 20, \"2020-10-06\")\n",
							"]\n",
							"\n",
							"testdata_location = data_path + \"/productTable\"\n",
							"from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
							"testdata_schema = StructType([\n",
							"    StructField('name', StringType(), True),\n",
							"    StructField('qty', IntegerType(), True),\n",
							"    StructField('date', StringType(), True)])\n",
							"\n",
							"test_df = spark.createDataFrame(testdata, testdata_schema)\n",
							"test_df.write.mode(\"overwrite\").parquet(testdata_location)\n",
							"test_df = spark.read.parquet(testdata_location)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# CREATE INDEX\n",
							"hyperspace.createIndex(test_df, IndexConfig(\"productIndex2\", [\"name\"], [\"date\", \"qty\"]))\n",
							"\n",
							"spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
							"filter1 = test_df.filter(\"name = 'banana'\")\n",
							"filter2 = test_df.filter(\"qty > 10\")\n",
							"query = filter1.join(filter2, \"name\")\n",
							"\n",
							"# Check Join index rule is applied properly.\n",
							"hyperspace.explain(query, True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Append new files.\r\n",
							"append_data = [\r\n",
							"    (\"orange\", 13, \"2020-11-01\"),\r\n",
							"    (\"banana\", 5, \"2020-11-01\")\r\n",
							"]\r\n",
							"append_df = spark.createDataFrame(append_data, testdata_schema)\r\n",
							"append_df.write.mode(\"append\").parquet(testdata_location)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Hybrid scan is disabled by default. Therefore, you will see that since we appended new data, Hyperspace will decide NOT to use the index.\n",
							"\n",
							"In the output, you will see no plan differences (hence no highlighting)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Hybrid Scan configs are false by default.\n",
							"spark.conf.set(\"spark.hyperspace.index.hybridscan.enabled\", \"false\")\n",
							"spark.conf.set(\"spark.hyperspace.index.hybridscan.delete.enabled\", \"false\")\n",
							"\n",
							"test_df_with_append = spark.read.parquet(testdata_location)\n",
							"filter1 = test_df_with_append.filter(\"name = 'banana'\")\n",
							"filter2 = test_df_with_append.filter(\"qty > 10\")\n",
							"query = filter1.join(filter2, \"name\")\n",
							"hyperspace.explain(query, True, displayHTML)\n",
							"query.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Enable Hybrid Scan\r\n",
							"\r\n",
							"In plan with indexes, you can see\r\n",
							"`Exchange hashpartitioning` required only for appended files so that we could still utilize the \"shuffled\" index data with appended files. `BucketUnion` is used to merge \"shuffled\" appended files with the index data."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Enable Hybrid Scan config. \"delete\" config is not necessary.\n",
							"spark.conf.set(\"spark.hyperspace.index.hybridscan.enabled\", \"true\")\n",
							"# spark.conf.set(\"spark.hyperspace.index.hybridscan.delete.enabled\", \"true\")\n",
							"\n",
							"# Need to redefine query to recalculate the query plan.\n",
							"query = filter1.join(filter2, \"name\")\n",
							"hyperspace.explain(query, True, displayHTML)\n",
							"query.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Incremental Index Refresh\n",
							"When you ready to update your indexes but do not want to rebuild your entire index, Hyperspace supports updating indexes in an incremental manner using `hs.refreshIndex(\"name\", \"incremental\")` API. This will allow eliminate the need for a full rebuild of index from scratch, utilizing previously created index files as well as updating indexes on only the newly added data.\n",
							"\n",
							"Of course, please be sure to use the complementary `optimizeIndex` API (shown below) periodically to make sure you do not see performance regressions. We recommend calling `optimize` at least once for every 10 times you call `refreshIndex(..., \"incremental\")`, assuming the data you added/removed is < 10% of the original dataset. For instance, if your original dataset is 100 GB, and you've added/removed data in increments/decrements of 1 GB, you can call `refreshIndex` 10 times before calling `optimizeIndex`. Please note that this example is simply used for illustration and you have to adapt this for your workloads.\n",
							"\n",
							"In the example below, notice the addition of a `Sort` node in the query plan when indexes are used. This is because partial indexes are created on the appended data files, causing Spark to introduce a `Sort`. Please also note that `Shuffle` i.e. `Exchange` is still eliminated from the plan, giving you the appropriate acceleration."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"def query():\n",
							"    test_df_with_append = spark.read.parquet(testdata_location)\n",
							"    filter1 = test_df_with_append.filter(\"name = 'banana'\")\n",
							"    filter2 = test_df_with_append.filter(\"qty > 10\")\n",
							"    return filter1.join(filter2, \"name\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.refreshIndex(\"productIndex2\", \"incremental\")\n",
							"\n",
							"hyperspace.explain(query(), True, displayHTML)\n",
							"query().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Optimize Index layout\n",
							"After calling incremental refreshes multiple times on newly appended data (e.g. if the user writes to data in small batches or in case of streaming scenarios), the number of index files tend to become large affecting the performance of the index (large number of small files problem). Hyperspace provides `hyperspace.optimizeIndex(\"indexName\")` API to optimize the index layout and reduce the large files problem.\n",
							"\n",
							"In the plan below, notice that Hyperspace has removed the additional `Sort` node in the query plan. Optimize can help avoiding sorting for any index bucket which contains only one file. However, this will only be true if ALL the index buckets have at most 1 file per bucket, after `optimizeIndex`."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Append some more data and call refresh again.\n",
							"append_data = [\n",
							"    (\"orange\", 13, \"2020-11-01\"),\n",
							"    (\"banana\", 5, \"2020-11-01\")\n",
							"]\n",
							"append_df = spark.createDataFrame(append_data, testdata_schema)\n",
							"append_df.write.mode(\"append\").parquet(testdata_location)\n",
							"\n",
							"hyperspace.refreshIndex(\"productIndex2\", \"incremental\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Call optimize. Ensure that Sort is removed after optimization (This is possible here because after optimize, in this case, every bucket contains only 1 file.).\n",
							"hyperspace.optimizeIndex(\"productIndex2\")\n",
							"\n",
							"hyperspace.explain(query(), True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Optimize modes\n",
							"\n",
							"The default mode for optimization is \"quick\" mode where files smaller than a predefined threshold are picked for optmization. To maximize the effect of optimization, Hyperspace allows another optimize mode \"full\" as shown below. This mode picks ALL index files for optimization irrespective of their file size and creates the best possible layout of the index. This is also slower than the default optimize mode because more data is being processed here.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.optimizeIndex(\"productIndex2\", \"full\")\n",
							"\n",
							"hyperspace.explain(query(), True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Clean Up\n",
							"To make this notebook self-contained and not leave any dangling data, we have some small clean-up code below. "
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.fs.rm(data_path, True)\n",
							"mssparkutils.fs.rm(index_location, True)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Hitchikers Guide to Delta Lake - Python')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder/python"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5b65f057-a110-4843-8b8b-eab6f259139f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Hitchhiker's Guide to Delta Lake (Python)\n",
							"\n",
							"This tutorial has been adapted for more clarity from its original counterpart [here](https://docs.delta.io/latest/quick-start.html). This notebook helps you quickly explore the main features of [Delta Lake](https://github.com/delta-io/delta). It provides code snippets that show how to read from and write to Delta Lake tables from interactive, batch, and streaming queries.\n",
							"\n",
							"Here's what we will cover:\n",
							"* Create a table\n",
							"* Understanding meta-data\n",
							"* Read data\n",
							"* Update table data\n",
							"* Overwrite table data\n",
							"* Conditional update without overwrite\n",
							"* Read older versions of data using Time Travel\n",
							"* Write a stream of data to a table\n",
							"* Read a stream of changes from a table"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Configuration\n",
							"Make sure you modify this as appropriate."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"import random\n",
							"\n",
							"session_id = random.randint(0,1000000)\n",
							"delta_table_path = \"/delta/delta-table-{0}\".format(session_id)\n",
							"\n",
							"delta_table_path"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Create a table\n",
							"To create a Delta Lake table, write a DataFrame out in the **delta** format. You can use existing Spark SQL code and change the format from parquet, csv, json, and so on, to delta.\n",
							"\n",
							"These operations create a new Delta Lake table using the schema that was inferred from your DataFrame. For the full set of options available when you create a new Delta Lake table, see Create a table and Write to a table (subsequent cells in this notebook)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"data = spark.range(0,5)\n",
							"data.show()\n",
							"data.write.format(\"delta\").save(delta_table_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Understanding Meta-data\n",
							"\n",
							"In Delta Lake, meta-data is no different from data i.e., it is stored next to the data. Therefore, an interesting side-effect here is that you can peek into meta-data using regular Spark APIs. "
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"[log_line.value for log_line in spark.read.text(delta_table_path + \"/_delta_log/\").collect()]"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read data\n",
							"\n",
							"You read data in your Delta Lake table by specifying the path to the files."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.format(\"delta\").load(delta_table_path)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Update table data\n",
							"\n",
							"Delta Lake supports several operations to modify tables using standard DataFrame APIs. This example runs a batch job to overwrite the data in the table.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"data = spark.range(5,10)\n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"When you now inspect the meta-data, what you will notice is that the original data is over-written. Well, not in a true sense but appropriate entries are added to Delta's transaction log so it can provide an \"illusion\" that the original data was deleted. We can verify this by re-inspecting the meta-data. You will see several entries indicating reference removal to the original data."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"[log_line.value for log_line in spark.read.text(delta_table_path + \"/_delta_log/\").collect()]"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Save as catalog tables\n",
							"\n",
							"Delta Lake can write to managed or external catalog tables."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Write data to a new managed catalog table.\n",
							"data.write.format(\"delta\").saveAsTable(\"ManagedDeltaTable\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"# Define an external catalog table that points to the existing Delta Lake data in storage.\n",
							"spark.sql(\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '{0}'\".format(delta_table_path))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"# List the 2 new tables.\n",
							"spark.sql(\"SHOW TABLES\").show()\n",
							"\n",
							"# Explore their properties.\n",
							"spark.sql(\"DESCRIBE EXTENDED ManagedDeltaTable\").show(truncate=False)\n",
							"spark.sql(\"DESCRIBE EXTENDED ExternalDeltaTable\").show(truncate=False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Conditional update without overwrite\n",
							"\n",
							"Delta Lake provides programmatic APIs to conditional update, delete, and merge (upsert) data into tables. For more information on these operations, see [Table Deletes, Updates, and Merges](https://docs.delta.io/latest/delta-update.html)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from delta.tables import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"delta_table = DeltaTable.forPath(spark, delta_table_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"# Update every even value by adding 100 to it\n",
							"delta_table.update(\n",
							"  condition = expr(\"id % 2 == 0\"),\n",
							"  set = { \"id\": expr(\"id + 100\") })\n",
							"delta_table.toDF().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"# Delete every even value\n",
							"delta_table.delete(\"id % 2 == 0\")\n",
							"delta_table.toDF().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"# Upsert (merge) new data\n",
							"new_data = spark.range(0,20).alias(\"newData\")\n",
							"\n",
							"delta_table.alias(\"oldData\")\\\n",
							"    .merge(new_data.alias(\"newData\"), \"oldData.id = newData.id\")\\\n",
							"    .whenMatchedUpdate(set = { \"id\": lit(\"-1\")})\\\n",
							"    .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\") })\\\n",
							"    .execute()\n",
							"\n",
							"delta_table.toDF().show(100)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"source": [
							"## History\n",
							"Delta's most powerful feature is the ability to allow looking into history i.e., the changes that were made to the underlying Delta Table. The cell below shows how simple it is to inspect the history."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().show(20, 1000, False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read older versions of data using Time Travel\n",
							"\n",
							"You can query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option.\n",
							"\n",
							"Once you run the cell below, you should see the first set of data, from before you overwrote it. Time Travel is an extremely powerful feature that takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table. Removing the version 0 option (or specifying version 1) would let you see the newer data again. For more information, see [Query an older snapshot of a table (time travel)](https://docs.delta.io/latest/delta-batch.html#deltatimetravel)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write a stream of data to a table\n",
							"\n",
							"You can also write to a Delta Lake table using Spark's Structured Streaming. The Delta Lake transaction log guarantees exactly-once processing, even when there are other streams or batch queries running concurrently against the table. By default, streams run in append mode, which adds new records to the table.\n",
							"\n",
							"For more information about Delta Lake integration with Structured Streaming, see [Table Streaming Reads and Writes](https://docs.delta.io/latest/delta-streaming.html).\n",
							"\n",
							"In the cells below, here's what we are doing:\n",
							"\n",
							"1. *Cell 28* Setup a simple Spark Structured Streaming job to generate a sequence and make the job write into our Delta Table\n",
							"2. *Cell 30* Show the newly appended data\n",
							"3. *Cell 31* Inspect history\n",
							"4. *Cell 32* Stop the structured streaming job\n",
							"5. *Cell 33* Inspect history <-- You'll notice appends have stopped"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"streaming_df = spark.readStream.format(\"rate\").load()\n",
							"stream = streaming_df\\\n",
							"    .selectExpr(\"value as id\")\\\n",
							"    .writeStream\\\n",
							"    .format(\"delta\")\\\n",
							"    .option(\"checkpointLocation\", \"/tmp/checkpoint-{0}\".format(session_id))\\\n",
							"    .start(delta_table_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read a stream of changes from a table\n",
							"\n",
							"While the stream is writing to the Delta Lake table, you can also read from that table as streaming source. For example, you can start another streaming query that prints all the changes made to the Delta Lake table."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.toDF().sort(col(\"id\").desc()).show(100)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(20, 1000, False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"source": [
							"stream.stop()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(100, 1000, False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Compaction\n",
							"\n",
							"If a Delta Table is growing too large, you can compact it by repartitioning into a smaller number of files.\n",
							"\n",
							"The option `dataChange = false` is an optimization that tells Delta Lake to do the repartition without marking the underlying data as \"modified\". This ensures that any other concurrent operations (such as streaming reads/writes) aren't negatively impacted.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"partition_count = 2\n",
							"\n",
							"spark.read\\\n",
							"    .format(\"delta\")\\\n",
							"    .load(delta_table_path)\\\n",
							"    .repartition(partition_count)\\\n",
							"    .write.option(\"dataChange\", \"false\")\\\n",
							"    .format(\"delta\")\\\n",
							"    .mode(\"overwrite\")\\\n",
							"    .save(delta_table_path)    "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Convert Parquet to Delta\n",
							"You can do an in-place conversion from the Parquet format to Delta."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_path = \"/parquet/parquet-table-{0}\".format(session_id)\n",
							"\n",
							"data = spark.range(0,5)\n",
							"data.write.parquet(parquet_path)\n",
							"\n",
							"# Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"source": [
							"DeltaTable.convertToDelta(spark, \"parquet.`{0}`\".format(parquet_path))\n",
							"\n",
							"# Confirm that the converted data is now in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "markdown",
						"source": [
							"## SQL Support\n",
							"Delta supports table utility commands through SQL.  You can use SQL to:\n",
							"* Get a DeltaTable's history\n",
							"* Vacuum a DeltaTable\n",
							"* Convert a Parquet file to Delta\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DESCRIBE HISTORY delta.`{0}`\".format(delta_table_path)).show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"VACUUM delta.`{0}`\".format(delta_table_path)).show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_id = random.randint(0,1000)\n",
							"parquet_path = \"/parquet/parquet-table-{0}-{1}\".format(session_id, parquet_path)\n",
							"\n",
							"data = spark.range(0,5)\n",
							"data.write.parquet(parquet_path)\n",
							"\n",
							"# Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)\n",
							"\n",
							"# Use SQL to convert the parquet table to Delta\n",
							"spark.sql(\"CONVERT TO DELTA parquet.`{0}`\".format(parquet_path))\n",
							"\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 26
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Load to DataFrame')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1fb763aa-c4ed-4eb5-9617-2ddb02dfc8b6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [
										"TOWNNAME"
									],
									"values": [
										"TOWNNAME"
									],
									"yLabel": "TOWNNAME",
									"xLabel": "TOWNNAME",
									"aggregation": "COUNT",
									"aggByBackend": false
								},
								"aggData": "{\"TOWNNAME\":{\"Dannevirke\":1,\"Okere\":1,\"Stratford\":1}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": false
							}
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://hozhao@hozhao0924gen2.dfs.core.windows.net/New folder 1105/0vg001140vg001140vg001140vg001140vg001g001.parquet', format='parquet')\r\n",
							"display(df.limit(10))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Magictest')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%lsmagic"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"a = 1\n",
							"def test():\n",
							"    b = %time 2\n",
							"    print(a, b)\n",
							"test()\n",
							"a"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%%time\n",
							"import time\n",
							"time.sleep(.3)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%%capture --no-stderr out\n",
							"import sys\n",
							"\n",
							"def eprint(*args, **kwargs):\n",
							"    print(*args, file=sys.stderr, **kwargs)\n",
							"\n",
							"print('std\\\\out')\n",
							"eprint('std\\nerr')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"out.stdout"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"%%writefile -a abc/test1.txt\n",
							"test,1\n",
							"map,2\n",
							"cool,10\n",
							"coolest,232"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.csv('abc/test1.csv')\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"display(df)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Magictestee')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0a614dcc-d1da-47ed-a16c-b23f96428d02"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%lsmagic"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"a = 1\n",
							"def test():\n",
							"    b = %time 2\n",
							"    print(a, b)\n",
							"test()\n",
							"a"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%%time\n",
							"import time\n",
							"time.sleep(.3)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%%capture --no-stderr out\n",
							"import sys\n",
							"\n",
							"def eprint(*args, **kwargs):\n",
							"    print(*args, file=sys.stderr, **kwargs)\n",
							"\n",
							"print('std\\\\out')\n",
							"eprint('std\\nerr')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"out.stdout"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"%%writefile -a abc/test1.txt\n",
							"test,1\n",
							"map,2\n",
							"cool,10\n",
							"coolest,232"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.csv('abc/test1.csv')\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"display(df)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/New Spark Table')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "428f9668-8551-4a23-b583-ccbb7b898856"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://hozhao@hozhao0924gen2.dfs.core.windows.net/New folder 1105/0vg001140vg001140vg001140vg001140vg001g001.parquet', format='parquet')\r\n",
							"df.write.mode(\"overwrite\").saveAsTable(\"default.YourTableName\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 11')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder22222 2"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9447646c-1496-45e1-891c-519b31c4e1db"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 12')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"language_info": {
						"name": "python"
					}
				},
				"cells": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 19')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8fb97a1f-b54c-42f7-ad78-a9ba49089d7a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"new_rows = [('CA',22, 45000),(\"WA\",35,65000) ,(\"WA\",50,85000)]\n",
							"demo_df = spark.createDataFrame(new_rows, ['state', 'age', 'salary'])\n",
							"demo_df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1_Copy1_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "97cb6d2e-f218-4e2e-af53-6194d86b438c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"new_rows = [('CA',22, 45000),(\"WA\",35,65000) ,(\"WA\",50,85000)]\n",
							"demo_df = spark.createDataFrame(new_rows, ['state', 'age', 'salary'])\n",
							"demo_df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1rename')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "856f1284-e85d-4c81-80a8-d50f28c6fccf"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"a=1"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8dbad5da-5da9-4f2b-8571-8084cee7e3d4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Write a Spark DataFrame into a Cosmos DB container\n",
							"# To select a preferred list of regions in a multi-region Cosmos DB account, add .option(\"spark.cosmos.preferredRegions\", \"<Region1>,<Region2>\")\n",
							"\n",
							"YOURDATAFRAME.write\\\n",
							"    .format(\"cosmos.oltp\")\\\n",
							"    .option(\"spark.synapse.linkedService\", \"SQLApi1105\")\\\n",
							"    .option(\"spark.cosmos.container\", \"yuwwang-contatiner1\")\\\n",
							"    .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\n",
							"    .mode('append')\\\n",
							"    .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 20')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 21')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 24')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8047c9a5-dc7e-49ce-84a4-84a357aed97f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import azureml.core\r\n",
							"\r\n",
							"from azureml.core import Experiment, Workspace, Dataset, Datastore, AutoMLRun\r\n",
							"from azureml.train.automl import AutoMLConfig\r\n",
							"\r\n",
							"subscription_id = \"051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3\"\r\n",
							"resource_group = \"chaxu-test\"\r\n",
							"workspace_name = \"chaxuamleus\"\r\n",
							"experiment_name = \"yifso1022scus-nyc_taxi-20201112092417\"\r\n",
							"\r\n",
							"ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\r\n",
							"experiment = Experiment(ws, experiment_name)\r\n",
							"\r\n",
							"run = AutoMLRun(experiment, run_id = 'AutoML_7bc3d9e3-7f17-4109-8dda-9c2fb1281311')\r\n",
							"\r\n",
							"# If you want to register the best model, please uncomment the following codes\r\n",
							"import onnxruntime\r\n",
							"import mlflow\r\n",
							"import mlflow.onnx\r\n",
							"\r\n",
							"from mlflow.models.signature import ModelSignature\r\n",
							"from mlflow.types import DataType\r\n",
							"from mlflow.types.schema import ColSpec, Schema\r\n",
							"\r\n",
							"# Get best model from automl run\r\n",
							"best_run, onnx_model = run.get_output(return_onnx_model=True)\r\n",
							"\r\n",
							"# Define utility functions to infer the schema of ONNX model\r\n",
							"def _infer_schema(data):\r\n",
							"    res = []\r\n",
							"    for _, col in enumerate(data):\r\n",
							"        t = col.type.replace(\"tensor(\", \"\").replace(\")\", \"\")\r\n",
							"        if t in [\"bool\"]:\r\n",
							"            dt = DataType.boolean\r\n",
							"        elif t in [\"int8\", \"uint8\", \"int16\", \"uint16\", \"int32\"]:\r\n",
							"            dt = DateType.integer\r\n",
							"        elif t in [\"uint32\", \"int64\"]:\r\n",
							"            dt = DataType.long\r\n",
							"        elif t in [\"float16\", \"bfloat16\", \"float\"]:\r\n",
							"            dt = DataType.float\r\n",
							"        elif t in [\"double\"]:\r\n",
							"            dt = DataType.double\r\n",
							"        elif t in [\"string\"]:\r\n",
							"            dt = DataType.string\r\n",
							"        else:\r\n",
							"            raise Exception(\"Unsupported type: \" + t)\r\n",
							"        res.append(ColSpec(type=dt, name=col.name))\r\n",
							"    return Schema(res)\r\n",
							"\r\n",
							"def _infer_signature(onnx_model):\r\n",
							"    onnx_model_bytes = onnx_model.SerializeToString()\r\n",
							"    onnx_runtime = onnxruntime.InferenceSession(onnx_model_bytes)\r\n",
							"    inputs = _infer_schema(onnx_runtime.get_inputs())\r\n",
							"    outputs = _infer_schema(onnx_runtime.get_outputs())\r\n",
							"    return ModelSignature(inputs, outputs)\r\n",
							"\r\n",
							"# Infer signature of ONNX model\r\n",
							"signature = _infer_signature(onnx_model)\r\n",
							"\r\n",
							"artifact_path = experiment_name + \"_artifact\"\r\n",
							"mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\r\n",
							"mlflow.set_experiment(experiment_name)\r\n",
							"\r\n",
							"with mlflow.start_run() as run:\r\n",
							"    # Save the model to the outputs directory for capture\r\n",
							"    mlflow.onnx.log_model(onnx_model, artifact_path, signature=signature)\r\n",
							"\r\n",
							"    # Register the model to AML model registry\r\n",
							"    mlflow.register_model(\"runs:/\" + run.info.run_id + \"/\" + artifact_path, \"test-fixed-VHD-Best\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 25')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c321cdcd-2739-4ffa-a9f5-affd9bac71d4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import azureml.core\n",
							"\n",
							"from azureml.core import Experiment, Workspace, Dataset, Datastore\n",
							"from azureml.train.automl import AutoMLConfig\n",
							"from azureml.data.dataset_factory import TabularDatasetFactory"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"subscription_id = \"051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3\"\n",
							"resource_group = \"chaxu-test\"\n",
							"workspace_name = \"chaxuamleus\"\n",
							"experiment_name = \"yifso1022scus-nyc_taxi-test-newvhd\"\n",
							"\n",
							"ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
							"experiment = Experiment(ws, experiment_name)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.sql(\"SELECT * FROM default.nyc_taxi\")\n",
							"\n",
							"datastore = Datastore.get_default(ws)\n",
							"dataset = TabularDatasetFactory.register_spark_dataframe(df, datastore, name = experiment_name + \"-dataset\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"automl_config = AutoMLConfig(spark_context = sc,\n",
							"                             task = \"classification\",\n",
							"                             training_data = dataset,\n",
							"                             label_column_name = \"tipped\",\n",
							"                             primary_metric = \"accuracy\",\n",
							"                             experiment_timeout_hours = 0.25,\n",
							"                             max_concurrent_iterations = 2,\n",
							"                             enable_onnx_compatible_models = True)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"run = experiment.submit(automl_config)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"displayHTML(\"<a href={} target='_blank'>Your experiment in Azure Machine Learning portal: {}</a>\".format(run.get_portal_url(), run.id))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"run.wait_for_completion()\n",
							"\n",
							"# Install required dependency\n",
							"import pip\n",
							"pip.main([\"install\", \"azure-storage-blob==12.5.0\"])\n",
							"\n",
							"import onnxruntime\n",
							"import mlflow\n",
							"import mlflow.onnx\n",
							"\n",
							"from mlflow.models.signature import ModelSignature\n",
							"from mlflow.types import DataType\n",
							"from mlflow.types.schema import ColSpec, Schema\n",
							"\n",
							"# Get best model from automl run\n",
							"best_run, onnx_model = run.get_output(return_onnx_model=True)\n",
							"\n",
							"# Define utility functions to infer the schema of ONNX model\n",
							"def _infer_schema(data):\n",
							"    res = []\n",
							"    for _, col in enumerate(data):\n",
							"        t = col.type.replace(\"tensor(\", \"\").replace(\")\", \"\")\n",
							"        if t in [\"bool\"]:\n",
							"            dt = DataType.boolean\n",
							"        elif t in [\"int8\", \"uint8\", \"int16\", \"uint16\", \"int32\"]:\n",
							"            dt = DateType.integer\n",
							"        elif t in [\"uint32\", \"int64\"]:\n",
							"            dt = DataType.long\n",
							"        elif t in [\"float16\", \"bfloat16\", \"float\"]:\n",
							"            dt = DataType.float\n",
							"        elif t in [\"double\"]:\n",
							"            dt = DataType.double\n",
							"        elif t in [\"string\"]:\n",
							"            dt = DataType.string\n",
							"        else:\n",
							"            raise Exception(\"Unsupported type: \" + t)\n",
							"        res.append(ColSpec(type=dt, name=col.name))\n",
							"    return Schema(res)\n",
							"\n",
							"def _infer_signature(onnx_model):\n",
							"    onnx_model_bytes = onnx_model.SerializeToString()\n",
							"    onnx_runtime = onnxruntime.InferenceSession(onnx_model_bytes)\n",
							"    inputs = _infer_schema(onnx_runtime.get_inputs())\n",
							"    outputs = _infer_schema(onnx_runtime.get_outputs())\n",
							"    return ModelSignature(inputs, outputs)\n",
							"\n",
							"# Infer signature of ONNX model\n",
							"signature = _infer_signature(onnx_model)\n",
							"\n",
							"artifact_path = experiment_name + \"_artifact\"\n",
							"mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
							"mlflow.set_experiment(experiment_name)\n",
							"\n",
							"with mlflow.start_run() as run:\n",
							"    # Save the model to the outputs directory for capture\n",
							"    mlflow.onnx.log_model(onnx_model, artifact_path, signature=signature)\n",
							"\n",
							"    # Register the model to AML model registry\n",
							"    mlflow.register_model(\"runs:/\" + run.info.run_id + \"/\" + artifact_path, \"yifso1022scus-nyc_taxi-test-newvhd-Best\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 26')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d2a01170-bff0-4397-b9a5-3d6ba7ac948a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 27')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					}
				},
				"cells": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 28')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 29')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6eb38fd8-201d-4582-a7c6-a0b695e82bd0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://hozhao@hozhao0924gen2.dfs.core.windows.net/New folder 1105/0vg001140vg001140vg001140vg001140vg001g001.parquet', format='parquet')\r\n",
							"df.write.mode(\"overwrite\").saveAsTable(\"default.YourTableName\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 30')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a86d3139-f23a-4abe-b0ab-1b2118f36cf3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://hozhao@hozhao0924gen2.dfs.core.windows.net/New folder 1105/0vg001140vg001140vg001140vg001140vg001g001.parquet', format='parquet')\r\n",
							"df.write.mode(\"overwrite\").saveAsTable(\"default.YourTableName\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 31')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4611d598-3943-49ab-ab06-a129d0b3f205"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"1+1"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 32')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 33')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "11591bc5-98e5-4b10-8c79-816a22ccaf5e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 4')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fb5272bc-8544-477c-8cb0-03c940764acf"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"a = 1"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 6')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 7')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 8')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 8_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook NET 3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "csharp"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"using Microsoft.Spark.Sql;\n",
							"using Microsoft.Spark.Sql.Types;\n",
							"using static Microsoft.Spark.Sql.Functions;\n",
							"var inputSchema = new StructType(new[]\n",
							"{\n",
							"    new StructField(\"age\", new IntegerType()),\n",
							"    new StructField(\"name\", new StringType())\n",
							"});\n",
							"DataFrame dfWithSchema = spark.Read().Schema(inputSchema).Json(\"abfss://hozhao@hozhao0924gen2.dfs.core.windows.net/people.json\");"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"Display(dfWithSchema);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"DataFrame df = spark.Read().Option(\"header\", true).Csv(\"abfss://hozhao@hozhao0924gen2.dfs.core.windows.net/diamonds.csv\");\n",
							"df.Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"Display(df);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"Display(df);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"DisplayHTML(\"<div><h2 style='color: green'>Hello, Spark NET</h2></div>\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"DisplayHTML(\"<div><h2 style='color: green'>???? ????????????????????????U1[?????]U2[????]U3[?????]</h2></div>\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"import matplotlib.pyplot as plt\n",
							"\n",
							"\n",
							"def demo_con_style(ax, connectionstyle):\n",
							"    x1, y1 = 0.3, 0.2\n",
							"    x2, y2 = 0.8, 0.6\n",
							"\n",
							"    ax.plot([x1, x2], [y1, y2], \".\")\n",
							"    ax.annotate(\"\",\n",
							"                xy=(x1, y1), xycoords='data',\n",
							"                xytext=(x2, y2), textcoords='data',\n",
							"                arrowprops=dict(arrowstyle=\"->\", color=\"0.5\",\n",
							"                                shrinkA=5, shrinkB=5,\n",
							"                                patchA=None, patchB=None,\n",
							"                                connectionstyle=connectionstyle,\n",
							"                                ),\n",
							"                )\n",
							"\n",
							"    ax.text(.05, .95, connectionstyle.replace(\",\", \",\\n\"),\n",
							"            transform=ax.transAxes, ha=\"left\", va=\"top\")\n",
							"\n",
							"\n",
							"fig, axs = plt.subplots(3, 5, figsize=(8, 4.8))\n",
							"demo_con_style(axs[0, 0], \"angle3,angleA=90,angleB=0\")\n",
							"demo_con_style(axs[1, 0], \"angle3,angleA=0,angleB=90\")\n",
							"demo_con_style(axs[0, 1], \"arc3,rad=0.\")\n",
							"demo_con_style(axs[1, 1], \"arc3,rad=0.3\")\n",
							"demo_con_style(axs[2, 1], \"arc3,rad=-0.3\")\n",
							"demo_con_style(axs[0, 2], \"angle,angleA=-90,angleB=180,rad=0\")\n",
							"demo_con_style(axs[1, 2], \"angle,angleA=-90,angleB=180,rad=5\")\n",
							"demo_con_style(axs[2, 2], \"angle,angleA=-90,angleB=10,rad=5\")\n",
							"demo_con_style(axs[0, 3], \"arc,angleA=-90,angleB=0,armA=30,armB=30,rad=0\")\n",
							"demo_con_style(axs[1, 3], \"arc,angleA=-90,angleB=0,armA=30,armB=30,rad=5\")\n",
							"demo_con_style(axs[2, 3], \"arc,angleA=-90,angleB=0,armA=0,armB=40,rad=0\")\n",
							"demo_con_style(axs[0, 4], \"bar,fraction=0.3\")\n",
							"demo_con_style(axs[1, 4], \"bar,fraction=-0.3\")\n",
							"demo_con_style(axs[2, 4], \"bar,angle=180,fraction=-0.2\")\n",
							"\n",
							"for ax in axs.flat:\n",
							"    ax.set(xlim=(0, 1), ylim=(0, 1), xticks=[], yticks=[], aspect=1)\n",
							"fig.tight_layout(pad=0.2)\n",
							"\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook03')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook_visualization')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "scala"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql \n",
							"SHOW TABLES"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\n",
							"display(spark.range(10))\n",
							"case class MapEntry(key: String, value: Int)\n",
							"val largeSeries = for (x <- 1 to 5000) yield MapEntry(\"k_%04d\".format(x), x)\n",
							"val largeDataFrame = sc.parallelize(largeSeries).toDF()\n",
							"largeDataFrame.registerTempTable(\"largeTable\")\n",
							"display(spark.sqlContext.sql(\"select * from largeTable\"))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"print(\"this is Python\")\n",
							"import platform\n",
							"print(platform.platform())"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"val blob_account_name = \"datasetarcadia\"\n",
							"val blob_container_name = \"wideworldimporters\"\n",
							"val blob_relative_path = \"dimension_Customer/\"\n",
							"val blob_sas_token = s\"?sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-07-13T07:32:42Z&st=2019-07-09T23:32:42Z&spr=https&sig=iVpGfb5yLi3pskn3%2Bl%2B1S3SQODcndTHyAhR5W5c7OYM%3D\"\n",
							"val wasbs_path = s\"wasbs://${blob_container_name}@${blob_account_name}.blob.core.windows.net/${blob_relative_path}\"\n",
							"spark.conf.set(\"fs.azure.account.key.datasetarcadia.blob.core.windows.net\" , \"k2wTXor/8YikczxgvK2RqcHq3iat7jE2WMxaTXHOfgpluoKlVOjwUjpDKE3xP+Blymx6Ba/VSPSVw33SZqKTyw==\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"// Cell 2\n",
							"import org.apache.spark.sql._\n",
							"import org.apache.spark.sql.types._\n",
							"val customSchema = (new StructType).\n",
							"    add(\"Customer_Keyyyyyyyyyyyyyyyyyy\",IntegerType,true).\n",
							"    add(\"WWI_Customer_ID\",IntegerType,true).\n",
							"    add(\"Customer\",StringType,true).\n",
							"    add(\"Bill_To_Customer\",StringType,true).\n",
							"    add(\"Category\",StringType,true).\n",
							"    add(\"Buying_Group\",StringType,true).\n",
							"    add(\"Primary_Contact\",StringType,true).\n",
							"    add(\"Postal_Code\",IntegerType,true).\n",
							"    add(\"Valid_From\",DateType,true).\n",
							"    add(\"Valid_To\",DateType,true).\n",
							"    add(\"Lineage_Key\",IntegerType,true)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"# This is markdown code cell.\n",
							"## This is markdown code cell."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Cell 3\n",
							"val dimcustomer = spark.read.option(\"header\", \"true\").option(\"delimiter\",\"|\").schema(customSchema).csv(wasbs_path)\n",
							"dimcustomer.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"dimcustomer.collect().length"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"display(dimcustomer)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\n",
							"df = pd.read_csv(\n",
							"  \"https://gist.githubusercontent.com/rgbkrk/a7984a8788a73e2afb8fd4b89c8ec6de/raw/db8d1db9f878ed448c3cac3eb3c9c0dc5e80891e/2015.csv\"\n",
							")\n",
							"display(df)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"// set all the properties and path towards the SQL Compute you want to access\n",
							"Class.forName(\"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n",
							"\n",
							"// val jdbcHostname = \" arcadiaworkspace.database.windows.net\"\n",
							"//val jdbcPort = 1433\n",
							"//val jdbcDatabase = \"arcadiadb\"\n",
							"//val jdbcUsername = \" sparkarcadia\"\n",
							"//val jdbcPassword = \"340$Uuxwp7Mcxo7Khy\"\n",
							"\n",
							"val jdbcHostname = \"a365metastoreprodwestus2.database.windows.net\"\n",
							"val jdbcPort = 1433\n",
							"val jdbcDatabase = \"HDInsight_Deployment\"\n",
							"val jdbcUsername = \"metastorelogin126@a365metastoreprodwestus2.database.windows.net\"\n",
							"val jdbcPassword = \"340$PsBiq?;@v@^OGDcS!)eAF#bf1(h{wD\"\n",
							"\n",
							"// Create the JDBC URL without passing in the user and password parameters.\n",
							"val jdbcUrl = s\"jdbc:sqlserver://${jdbcHostname}:${jdbcPort};database=${jdbcDatabase}\"\n",
							"\n",
							"// Create a Properties() object to hold the parameters.\n",
							"import java.util.Properties\n",
							"val connectionProperties = new Properties()\n",
							"\n",
							"connectionProperties.put(\"user\", s\"${jdbcUsername}\")\n",
							"connectionProperties.put(\"password\", s\"${jdbcPassword}\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"// load the fact table into a dataframe with an aggregate based on the Customer Key and an order by descending importance\n",
							"val facttx = spark.read.\n",
							"    jdbc(jdbcUrl, \"AzureRegions\", connectionProperties).show()\n",
							"\n",
							"    // groupBy(\"Customer Key\").sum().\n",
							"    // orderBy(desc(\"sum(Total Excluding Tax)\"))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"// Join the two tables based on two columns whose names are different (Customer Key)\n",
							"val result = facttx.as('a).\n",
							"    join(dimcustomer.as('b),$\"a.Customer Key\" === $\"b.Customer_Key\",joinType=\"left\").\n",
							"    groupBy(\"Customer\").sum().\n",
							"    select(\"Customer\",\"sum(sum(Total Excluding Tax))\").\n",
							"    toDF(\"Customer\",\"Sales\")\n",
							"result.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							"// Load the data into a new SQL Compute table and define the column data type\n",
							"val driverClass = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
							"connectionProperties.setProperty(\"Driver\", driverClass)\n",
							"result.write.option(\"createTableColumnTypes\", \"Customer varchar(80), Sales decimal(18,2)\").jdbc(jdbcUrl,\"dbo.sales_buyinggroup\", connectionProperties)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"spark.conf.getAll.filter(_._1.startsWith(\"spark.hadoop.javax.jdo\")).foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Obtain credentials with TokenLibrary')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder/python"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a2316d87-5e38-4aa5-b492-c7d3e8b7ff58"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"For latest documentation please run TokenLibrary.help() from Notebook\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# By default Synapse uses AAD passthrough for authentication\n",
							"# However, Linked services can be used for storing and retreiving credentials (e.g, account key)\n",
							"# Example connection string (for storage): \"DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>\"\n",
							"\n",
							"connection_string = TokenLibrary.getConnectionString(\"<linkedServiceName>\")\n",
							"account_key = TokenLibrary.getConnectionStringAsMap(\"<linkedServiceName>\").get(\"AccountKey\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Read and write data from Azure Blob Storage WASB')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "python"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "15e65a7f-afea-4cd8-a7db-0e7da487601c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Access data on Azure Storage Blob (WASB) with Synapse Spark\n",
							"\n",
							"You can access data on Azure Storage Blob (WASB) with Synapse Spark via following URL:\n",
							"\n",
							"    wasb[s]://<container_name>@<storage_account_name>.blob.core.windows.net/<path>\n",
							"\n",
							"This notebook provides examples of how to read data from WASB into a Spark context and how to write the output of Spark jobs directly into a WASB location."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Load a sample data\n",
							"\n",
							"Let's first load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) of last 6 months from Azure Open datasets as a sample."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import PublicHolidays\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"from dateutil.relativedelta import relativedelta\n",
							"\n",
							"\n",
							"end_date = datetime.today()\n",
							"start_date = datetime.today() - relativedelta(months=6)\n",
							"hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
							"hol_df = hol.to_spark_dataframe()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"# Display 5 rows\n",
							"hol_df.show(5, truncate = False)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write data to Azure Storage Blob\n",
							"\n",
							"We are going to write the spark dateframe to your Azure Blob Storage (WASB) path using **shared access signature (sas)**. Go to [Azure Portal](https://portal.azure.com/), open your Azure storage blob, select **shared access signature** in the **settings** and generate your sas token. Please make sure to allow contatiner level read and write permission. Fill in the access info for your Azure storage blob in the cell below. \n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.types import *\n",
							"\n",
							"# Azure storage access info\n",
							"blob_account_name = 'bdbjblobstorageqingtest' # replace with your blob name\n",
							"blob_container_name = 'container01' # replace with your container name\n",
							"blob_relative_path = 'samplenb/' # replace with your relative folder path\n",
							"blob_sas_token = r'?sv=2020-02-10&ss=b&srt=sco&sp=rwdlacx&se=2031-03-03T15:35:01Z&st=2021-03-03T07:35:01Z&spr=https&sig=6jPgwFH7%2FadnfhjGdIK8HOdl699krBPxsHhEdARhzqY%3D' # replace with your access key"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Allow SPARK to access from Blob remotely\n",
							"wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
							"spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\n",
							"print('Remote blob path: ' + wasbs_path)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Save a dataframe as Parquet, JSON or CSV\n",
							"If you have a dataframe, you can save it to Parquet or JSON with the .write.parquet(), .write.json() and .write.csv() methods respectively.\n",
							"\n",
							"Dataframes can be saved in any format, regardless of the input format.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_path = wasbs_path + 'holiday.parquet'\n",
							"json_path = wasbs_path + 'holiday.json'\n",
							"csv_path = wasbs_path + 'holiday.csv'\n",
							"print('parquet file path: ' + parquet_path)\n",
							"print('json file path ' + json_path)\n",
							"print('csv file path: ' + csv_path)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"hol_df.write.parquet(parquet_path, mode = 'overwrite')\n",
							"hol_df.write.json(json_path, mode = 'overwrite')\n",
							"hol_df.write.csv(csv_path, mode = 'overwrite', header = 'true')"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Save a dataframe as text files\n",
							"If you have a dataframe that you want ot save as text file, you must first covert it to an RDD and then save that RDD as a text file.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import time\n",
							"a = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Define the text file path\n",
							"text_path = wasbs_path + '/holiday_'+ a +\".txt\"\n",
							"print('text file path: ' + text_path)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							"# Covert spark dataframe into RDD \n",
							"hol_RDD = hol_df.rdd\n",
							"type(hol_RDD)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"source": [
							"If you have an RDD, you can convert it to a text file like the following:\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							" # Save RDD as text file\n",
							"hol_RDD.saveAsTextFile(text_path)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Read data from Azure Storage Blob\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create a dataframe from parquet files\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df_parquet = spark.read.parquet(parquet_path)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create a dataframe from JSON files\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df_json = spark.read.json(json_path)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create a dataframe from CSV files\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df_csv = spark.read.csv(csv_path, header = 'true')"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create an RDD from text file\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"text = sc.textFile(text_path)"
						],
						"outputs": [],
						"execution_count": 16
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Read and write data from Azure Blob Storage WASB_6mj')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "scala"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a8db71f6-b25d-4dd7-90ee-8bb2a6248de3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "scala"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Access data on Azure Storage Blob (WASB) with Synapse Spark\n",
							"\n",
							"You can access data on Azure Storage Blob (WASB) with Synapse Spark via following URL:\n",
							"\n",
							"    wasb[s]://<container_name>@<storage_account_name>.blob.core.windows.net/<path>\n",
							"\n",
							"This notebook provides examples of how to read data from WASB into a Spark context and how to write the output of Spark jobs directly into a WASB location."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Load a sample data\n",
							"\n",
							"Let's first load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) from Azure Open datasets as a sample."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// set blob storage account connection for open dataset\n",
							"\n",
							"val hol_blob_account_name = \"azureopendatastorage\"\n",
							"val hol_blob_container_name = \"holidaydatacontainer\"\n",
							"val hol_blob_relative_path = \"Processed\"\n",
							"val hol_blob_sas_token = \"\"\n",
							"\n",
							"val hol_wasbs_path = f\"wasbs://$hol_blob_container_name@$hol_blob_account_name.blob.core.windows.net/$hol_blob_relative_path\"\n",
							"spark.conf.set(f\"fs.azure.sas.$hol_blob_container_name.$hol_blob_account_name.blob.core.windows.net\",hol_blob_sas_token)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"// load the sample data as a Spark DataFrame\n",
							"val hol_df = spark.read.parquet(hol_wasbs_path) \n",
							"hol_df.show(5, truncate = false)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write data to Azure Storage Blob\n",
							"\n",
							"We are going to write the spark dateframe to your Azure Blob Storage (WASB) path using **shared access signature (sas)**. Go to [Azure Portal](https://portal.azure.com/), open your Azure storage blob, select **shared access signature** in the **settings** and generate your sas token. Please make sure to allow contatiner level read and write permission. Fill in the access info for your Azure storage blob in the cell below. \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// set your blob storage account connection\n",
							"\n",
							"val blob_account_name = \"samplenbblob\" // replace with your blob name\n",
							"val blob_container_name = \"data\" //replace with your container name\n",
							"val blob_relative_path = \"samplenb/\" //replace with your relative folder path\n",
							"val blob_sas_token = \"?sv=2019-02-02&ss=b&srt=sco&sp=rwdlac&se=2021-03-23T17:05:16Z&st=2020-03-24T09:05:16Z&spr=https,http&sig=drtIrL68s07nPW0Q9WEb5XFL6y5Eb7%2BOpmpxGyAHLaw%3D\" //replace with your sas token\n",
							"\n",
							"val wasbs_path = f\"wasbs://$blob_container_name@$blob_account_name.blob.core.windows.net/$blob_relative_path\"\n",
							"spark.conf.set(f\"fs.azure.sas.$blob_container_name.$blob_account_name.blob.core.windows.net\",blob_sas_token)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Save a dataframe as Parquet, JSON or CSV\n",
							"If you have a dataframe, you can save it to Parquet or JSON with the .write.parquet(), .write.json() and .write.csv() methods respectively.\n",
							"\n",
							"Dataframes can be saved in any format, regardless of the input format.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// set the path for the output file\n",
							"\n",
							"val parquet_path = wasbs_path + \"holiday.parquet\"\n",
							"val json_path = wasbs_path + \"holiday.json\"\n",
							"val csv_path = wasbs_path + \"holiday.csv\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"import org.apache.spark.sql.SaveMode\n",
							"\n",
							"hol_df.write.mode(SaveMode.Overwrite).parquet(parquet_path)\n",
							"hol_df.write.mode(SaveMode.Overwrite).json(json_path)\n",
							"hol_df.write.mode(SaveMode.Overwrite).option(\"header\", \"true\").csv(csv_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Save a dataframe as text files\n",
							"If you have a dataframe that you want ot save as text file, you must first covert it to an RDD and then save that RDD as a text file.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Define the text file path and covert spark dataframe into RDD\n",
							"val text_path = wasbs_path + \"holiday02233.txt\"\n",
							"val hol_RDD = hol_df.rdd"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"If you have an RDD, you can convert it to a text file like the following:\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Save RDD as text file\n",
							"hol_RDD.saveAsTextFile(text_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Read data from Azure Storage Blob\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create a dataframe from parquet files\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val df_parquet = spark.read.parquet(parquet_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create a dataframe from JSON files\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val df_json = spark.read.json(json_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create a dataframe from CSV files\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val df_csv = spark.read.option(\"header\", \"true\").csv(csv_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create an RDD from text file\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val text = sc.textFile(text_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"text.take(5).foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Read and write data from Azure Data Lake Storage Gen2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "python"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5c859648-af4b-4fd3-812a-2ce1aaed86a7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Access data on Azure Data Lake Storage Gen2 (ADLS Gen2) with Synapse Spark\n",
							"\n",
							"Azure Data Lake Storage Gen2 (ADLS Gen2) is used as the storage account associated with a Synapse workspace. A synapse workspace can have a default ADLS Gen2 storage account and additional linked storage accounts. \n",
							"\n",
							"You can access data on ADLS Gen2 with Synapse Spark via following URL:\n",
							"    \n",
							"    abfss://<container_name>@<storage_account_name>.dfs.core.windows.net/<path>\n",
							"\n",
							"This notebook provides examples of how to read data from ADLS Gen2 account into a Spark context and how to write the output of Spark jobs directly into an ADLS Gen2 location.\n",
							"\n",
							"## Pre-requisites\n",
							"Synapse leverage AAD pass-through to access any ADLS Gen2 account (or folder) to which you have a **Blob Storage Contributor** permission. No credentials or access token is required. "
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Load a sample data\n",
							"\n",
							"Let's first load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) of last 6 months from Azure Open datasets as a sample."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import PublicHolidays\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"from dateutil.relativedelta import relativedelta\n",
							"\n",
							"\n",
							"end_date = datetime.today()\n",
							"start_date = datetime.today() - relativedelta(months=6)\n",
							"hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
							"hol_df = hol.to_spark_dataframe()"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"# Display 5 rows\n",
							"hol_df.show(5, truncate = False)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write data to the default ADLS Gen2 storage\n",
							"\n",
							"We are going to write the spark dateframe to your default ADLS Gen2 storage account.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.types import *\n",
							"\n",
							"# Primary storage info\n",
							"account_name = 'bdbjgen2qingtest' # fill in your primary account name\n",
							"container_name = 'container01' # fill in your container name\n",
							"relative_path = 'test' # fill in your relative folder path\n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path)\n",
							"print('Primary storage account path: ' + adls_path)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Save a dataframe as Parquet, JSON or CSV\n",
							"If you have a dataframe, you can save it to Parquet or JSON with the .write.parquet(), .write.json() and .write.csv() methods respectively.\n",
							"\n",
							"Dataframes can be saved in any format, regardless of the input format.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_path = adls_path + 'holiday.parquet'\n",
							"json_path = adls_path + 'holiday.json'\n",
							"csv_path = adls_path + 'holiday.csv'\n",
							"print('parquet file path: ' + parquet_path)\n",
							"print('json file path ' + json_path)\n",
							"print('csv file path: ' + csv_path)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"hol_df.write.parquet(parquet_path, mode = 'overwrite')\n",
							"hol_df.write.json(json_path, mode = 'overwrite')\n",
							"hol_df.write.csv(csv_path, mode = 'overwrite', header = 'true')"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Save a dataframe as text files\n",
							"If you have a dataframe that you want ot save as text file, you must first covert it to an RDD and then save that RDD as a text file.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Define the text file path\n",
							"text_path = adls_path + 'holiday03081.txt'\n",
							"print('text file path: ' + text_path)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"# Covert spark dataframe into RDD \n",
							"hol_RDD = hol_df.rdd\n",
							"type(hol_RDD)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"If you have an RDD, you can convert it to a text file like the following:\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							" # Save RDD as text file\n",
							"hol_RDD.saveAsTextFile(text_path)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Read data from the default ADLS Gen2 storage\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create a dataframe from parquet files\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df_parquet = spark.read.parquet(parquet_path)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create a dataframe from JSON files\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df_json = spark.read.json(json_path)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create a dataframe from CSV files\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df_csv = spark.read.csv(csv_path, header = 'true')"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create an RDD from text file\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"text = sc.textFile(text_path)"
						],
						"outputs": [],
						"execution_count": 12
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Read and write data from Azure Data Lake Storage Gen2_bjr')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "scala"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a93eb8a2-f2fa-49f0-85f1-587908d9f2a1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "scala"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"# Access data on Azure Data Lake Storage Gen2 (ADLS Gen2) with Synapse Spark\n",
							"\n",
							"Azure Data Lake Storage Gen2 (ADLS Gen2) is used as the storage account associated with a Synapse workspace. A synapse workspace can have a default ADLS Gen2 storage account and additional linked storage accounts. \n",
							"\n",
							"You can access data on ADLS Gen2 with Synapse Spark via following URL:\n",
							"    \n",
							"    abfss://<container_name>@<storage_account_name>.dfs.core.windows.net/<path>\n",
							"\n",
							"This notebook provides examples of how to read data from ADLS Gen2 account into a Spark context and how to write the output of Spark jobs directly into an ADLS Gen2 location.\n",
							"\n",
							"## Pre-requisites\n",
							"Synapse leverage AAD pass-through to access any ADLS Gen2 account (or folder) to which you have a **Blob Storage Contributor** permission. No credentials or access token is required. "
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"## Load a sample data\n",
							"\n",
							"Let's first load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) from Azure Open datasets as a sample."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// set blob storage account connection for open dataset\n",
							"\n",
							"val hol_blob_account_name = \"azureopendatastorage\"\n",
							"val hol_blob_container_name = \"holidaydatacontainer\"\n",
							"val hol_blob_relative_path = \"Processed\"\n",
							"val hol_blob_sas_token = \"\"\n",
							"\n",
							"val hol_wasbs_path = f\"wasbs://$hol_blob_container_name@$hol_blob_account_name.blob.core.windows.net/$hol_blob_relative_path\"\n",
							"spark.conf.set(f\"fs.azure.sas.$hol_blob_container_name.$hol_blob_account_name.blob.core.windows.net\",hol_blob_sas_token)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// load the sample data as a Spark DataFrame\n",
							"val hol_df = spark.read.parquet(hol_wasbs_path) \n",
							"hol_df.show(5, truncate = false)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"## Write data to the default ADLS Gen2 storage\n",
							"\n",
							"We are going to write the spark dateframe to your default ADLS Gen2 storage account.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// set your storage account connection\n",
							"\n",
							"val account_name = \"bdbjtestgen2\" // replace with your blob name\n",
							"val container_name = \"zhaotest\" //replace with your container name\n",
							"val relative_path = \"test\" //replace with your relative folder path\n",
							"\n",
							"val adls_path = f\"abfss://$container_name@$account_name.dfs.core.windows.net/$relative_path\""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Save a dataframe as Parquet, JSON or CSV\n",
							"If you have a dataframe, you can save it to Parquet or JSON with the .write.parquet(), .write.json() and .write.csv() methods respectively.\n",
							"\n",
							"Dataframes can be saved in any format, regardless of the input format.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// set the path for the output file\n",
							"\n",
							"val parquet_path = adls_path + \"holiday.parquet\"\n",
							"val json_path = adls_path + \"holiday.json\"\n",
							"val csv_path = adls_path + \"holiday.csv\""
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"import org.apache.spark.sql.SaveMode\n",
							"\n",
							"hol_df.write.mode(SaveMode.Overwrite).parquet(parquet_path)\n",
							"hol_df.write.mode(SaveMode.Overwrite).json(json_path)\n",
							"hol_df.write.mode(SaveMode.Overwrite).option(\"header\", \"true\").csv(csv_path)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Save a dataframe as text files\n",
							"If you have a dataframe that you want ot save as text file, you must first covert it to an RDD and then save that RDD as a text file.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// Define the text file path and covert spark dataframe into RDD\n",
							"val text_path = adls_path + \"holiday1111111.txt\"\n",
							"val hol_RDD = hol_df.rdd"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// Save RDD as text file\n",
							"hol_RDD.saveAsTextFile(text_path)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"# Read data from the default ADLS Gen2 storage\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Create a dataframe from parquet files\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"val df_parquet = spark.read.parquet(parquet_path)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Create a dataframe from JSON files\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"val df_json = spark.read.json(json_path)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Create a dataframe from CSV files\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"val df_csv = spark.read.option(\"header\", \"true\").csv(csv_path)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Create an RDD from text file\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"val text = sc.textFile(text_path)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"text.take(5).foreach(println)"
						],
						"outputs": [],
						"execution_count": 13
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Updates and GDPR using Delta Lake - PySpark')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder/python"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e7113dd1-3ddc-49a8-9c9e-e661ae0daa63"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"#  Updates and GDPR using Delta Lake - PySpark\n",
							"\n",
							"In this notebook, we will review Delta Lake's end-to-end capabilities in PySpark. You can also look at the original Quick Start guide if you are not familiar with [Delta Lake](https://github.com/delta-io/delta) [here](https://docs.delta.io/latest/quick-start.html). It provides code snippets that show how to read from and write to Delta Lake tables from interactive, batch, and streaming queries.\n",
							"\n",
							"In this notebook, we will cover the following:\n",
							"\n",
							"- Creating sample mock data containing customer orders\n",
							"- Writing this data into storage in Delta Lake table format (or in short, Delta table)\n",
							"- Querying the Delta table using functional and SQL\n",
							"- The Curious Case of Forgotten Discount - Making corrections to data\n",
							"- Enforcing GDPR on your data\n",
							"- Oops, enforced it on the wrong customer! - Looking at the audit log to find mistakes in operations\n",
							"- Rollback all the way!\n",
							"- Closing the loop - 'defrag' your data"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Creating sample mock data containing customer orders\n",
							"\n",
							"For this tutorial, we will setup a sample file containing customer orders with a simple schema: (order_id, order_date, customer_name, price)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DROP TABLE IF EXISTS input\");\n",
							"spark.sql(\"\"\"\n",
							"          CREATE TEMPORARY VIEW input \n",
							"          AS SELECT 1 order_id, '2019-11-01' order_date, 'Saveen' customer_name, 100 price\n",
							"          UNION ALL SELECT 2, '2019-11-01', 'Terry', 50\n",
							"          UNION ALL SELECT 3, '2019-11-01', 'Priyanka', 100\n",
							"          UNION ALL SELECT 4, '2019-11-02', 'Steve', 10\n",
							"          UNION ALL SELECT 5, '2019-11-03', 'Rahul', 10\n",
							"          UNION ALL SELECT 6, '2019-11-03', 'Niharika', 75\n",
							"          UNION ALL SELECT 7, '2019-11-03', 'Elva', 90\n",
							"          UNION ALL SELECT 8, '2019-11-04', 'Andrew', 70\n",
							"          UNION ALL SELECT 9, '2019-11-05', 'Michael', 20\n",
							"          UNION ALL SELECT 10, '2019-11-05', 'Brigit', 25\"\"\")\n",
							"orders = spark.sql(\"SELECT * FROM input\")\n",
							"orders.show()\n",
							"orders.printSchema()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Writing this data into storage in Delta Lake table format (or in short, Delta table)\n",
							"\n",
							"To create a Delta Lake table, you can write a DataFrame out in the **delta** format. You can use existing Spark SQL code and change the format from parquet, csv, json, and so on, to delta. These operations create a new Delta Lake table using the schema that was inferred from your DataFrame. \n",
							"\n",
							"If you already have existing data in Parquet format, you can do an \"in-place\" conversion to Delta Lake format. The code would look like following:\n",
							"\n",
							"DeltaTable.convertToDelta(spark, $\"parquet.`{path_to_data}`\");\n",
							"\n",
							"//Confirm that the converted data is now in the Delta format\n",
							"DeltaTable.isDeltaTable(parquetPath)"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"import random\n",
							"\n",
							"session_id = random.randint(0,1000)\n",
							"path = \"/delta/delta-table-{0}\".format(session_id)\n",
							"path"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"# Here's how you'd do this in Parquet: \n",
							"# orders.repartition(1).write().format(\"parquet\").save(path)\n",
							"\n",
							"orders.repartition(1).write.format(\"delta\").save(path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Querying the Delta table using functional and SQL\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"ordersDataFrame = spark.read.format(\"delta\").load(path)\n",
							"ordersDataFrame.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"ordersDataFrame.createOrReplaceTempView(\"ordersDeltaTable\")\n",
							"spark.sql(\"SELECT * FROM ordersDeltaTable\").show"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Understanding Meta-data\n",
							"\n",
							"In Delta Lake, meta-data is no different from data i.e., it is stored next to the data. Therefore, an interesting side-effect here is that you can peek into meta-data using regular Spark APIs. "
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"[log_line.value for log_line in spark.read.text(path + \"/_delta_log/\").collect()]"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"# The Curious Case of Forgotten Discount - Making corrections to data\n",
							"\n",
							"Now that you are able to look at the orders table, you realize that you forgot to discount the orders that came in on November 1, 2019. Worry not! You can quickly make that correction."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from delta.tables import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"table = DeltaTable.forPath(spark, path)\n",
							"\n",
							"# Update every transaction that took place on November 1, 2019 and apply a discount of 10%\n",
							"table.update(\n",
							"  condition = expr(\"order_date == '2019-11-01'\"),\n",
							"  set = {\"price\": expr(\"price - price*0.1\") })"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"table.toDF()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"When you now inspect the meta-data, what you will notice is that the original data is over-written. Well, not in a true sense but appropriate entries are added to Delta's transaction log so it can provide an \"illusion\" that the original data was deleted. We can verify this by re-inspecting the meta-data. You will see several entries indicating reference removal to the original data."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"[log_line.value for log_line in spark.read.text(path + \"/_delta_log/\").collect()]"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Enforcing GDPR on your data\n",
							"\n",
							"One of your customers wanted their data to be deleted. But wait, you are working with data stored on an immutable file system (e.g., HDFS, ADLS, WASB). How would you delete it? Using Delta Lake's Delete API.\n",
							"\n",
							"Delta Lake provides programmatic APIs to conditionally update, delete, and merge (upsert) data into tables. For more information on these operations, see [Table Deletes, Updates, and Merges](https://docs.delta.io/latest/delta-update.html)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Delete the appropriate customer\n",
							"table.delete(condition = expr(\"customer_name == 'Saveen'\"))\n",
							"table.toDF().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Oops, enforced it on the wrong customer! - Looking at the audit/history log to find mistakes in operations\n",
							"\n",
							"Delta's most powerful feature is the ability to allow looking into history i.e., the changes that were made to the underlying Delta Table. The cell below shows how simple it is to inspect the history.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(20, 1000, False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Rollback all the way using Time Travel!\n",
							"\n",
							"You can query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option.\n",
							"\n",
							"Once you run the cell below, you should see the first set of data, from before you overwrote it. Time Travel is an extremely powerful feature that takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table. Removing the version 0 option (or specifying version 1) would let you see the newer data again. For more information, see [Query an older snapshot of a table (time travel)](https://docs.delta.io/latest/delta-batch.html#deltatimetravel)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(path).write.mode(\"overwrite\").format(\"delta\").save(path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"# Delete the correct customer - REMOVE\n",
							"table.delete(condition = expr(\"customer_name == 'Rahul'\"))\n",
							"table.toDF().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"source": [
							"table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(20, 1000, False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Closing the loop - 'defrag' your data\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
							"table.vacuum(0.01)\n",
							"\n",
							"# Alternate Syntax: spark.sql($\"VACUUM delta.`{path}`\").show"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 19
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Using Azure Open Datasets in Synapse')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "python"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "medium",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/medium",
						"name": "medium",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/medium",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Using Azure Open Datasets in Synapse - Enrich NYC Green Taxi Data with Holiday and Weather\n",
							"\n",
							"Synapse has [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/) package pre-installed. This notebook provides examples of how to enrich NYC Green Taxi Data with Holiday and Weather with focusing on :\n",
							"- read Azure Open Dataset\n",
							"- manipulate the data to prepare for further analysis, including column projection, filtering, grouping and joins etc. \n",
							"- create a Spark table to be used in other notebooks for modeling training"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data loading \n",
							"Let's first load the [NYC green taxi trip records](https://azure.microsoft.com/en-us/services/open-datasets/catalog/nyc-taxi-limousine-commission-green-taxi-trip-records/). The Open Datasets package contains a class representing each data source (NycTlcGreen for example) to easily filter date parameters before downloading."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import NycTlcGreen\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"end_date = parser.parse('2018-06-06')\n",
							"start_date = parser.parse('2018-05-01')\n",
							"\n",
							"nyc_tlc = NycTlcGreen(start_date=start_date, end_date=end_date)\n",
							"nyc_tlc_df = nyc_tlc.to_spark_dataframe()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"# Display 5 rows\n",
							"\n",
							"nyc_tlc_df.show(5, truncate = False)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"Now that the initial data is loaded. Let's do some projection on the data to \n",
							"- create new columns for the month number, day of month, day of week, and hour of day. These info is going to be used in the training model to factor in time-based seasonality.\n",
							"- add a static feature for the country code to join holiday data. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Extract month, day of month, and day of week from pickup datetime and add a static column for the country code to join holiday data. \n",
							"\n",
							"import pyspark.sql.functions as f\n",
							"\n",
							"nyc_tlc_df_expand = nyc_tlc_df.withColumn('datetime',f.to_date('lpepPickupDatetime'))\\\n",
							"                .withColumn('month_num',f.month(nyc_tlc_df.lpepPickupDatetime))\\\n",
							"                .withColumn('day_of_month',f.dayofmonth(nyc_tlc_df.lpepPickupDatetime))\\\n",
							"                .withColumn('day_of_week',f.dayofweek(nyc_tlc_df.lpepPickupDatetime))\\\n",
							"                .withColumn('hour_of_day',f.hour(nyc_tlc_df.lpepPickupDatetime))\\\n",
							"                .withColumn('country_code',f.lit('US'))"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"Remove some of the columns that won't need for modeling or additional feature building.\n",
							"\n",
							"\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Remove unused columns from nyc green taxi data\n",
							"\n",
							"columns_to_remove = [\"lpepDropoffDatetime\", \"puLocationId\", \"doLocationId\", \"pickupLongitude\", \n",
							"                     \"pickupLatitude\", \"dropoffLongitude\",\"dropoffLatitude\" ,\"rateCodeID\", \n",
							"                     \"storeAndFwdFlag\",\"paymentType\", \"fareAmount\", \"extra\", \"mtaTax\",\n",
							"                     \"improvementSurcharge\", \"tollsAmount\", \"ehailFee\", \"tripType \"  \n",
							"                    ]\n",
							"\n",
							"nyc_tlc_df_clean = nyc_tlc_df_expand.select([column for column in nyc_tlc_df_expand.columns if column not in columns_to_remove])"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"# Display 5 rows\n",
							"nyc_tlc_df_clean.show(5)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Enrich with holiday data\n",
							"Now that we have taxi data downloaded and roughly prepared, add in holiday data as additional features. Holiday-specific features will assist model accuracy, as major holidays are times where taxi demand increases dramatically and supply becomes limited. \n",
							"\n",
							"Let's load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) from Azure Open datasets.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import PublicHolidays\n",
							"\n",
							"hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
							"hol_df = hol.to_spark_dataframe()\n",
							"\n",
							"# Display data\n",
							"hol_df.show(5, truncate = False)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"Rename the countryRegionCode and date columns to match the respective field names from the taxi data, and also normalize the time so it can be used as a key. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"hol_df_clean = hol_df.withColumnRenamed('countryRegionCode','country_code')\\\n",
							"            .withColumn('datetime',f.to_date('date'))\n",
							"\n",
							"hol_df_clean.show(5)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"Next, join the holiday data with the taxi data by performing a left-join. This will preserve all records from taxi data, but add in holiday data where it exists for the corresponding datetime and country_code, which in this case is always \"US\". Preview the data to verify that they were merged correctly."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# enrich taxi data with holiday data\n",
							"nyc_taxi_holiday_df = nyc_tlc_df_clean.join(hol_df_clean, on = ['datetime', 'country_code'] , how = 'left')\n",
							"\n",
							"nyc_taxi_holiday_df.show(5)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"# Create a temp table and filter out non empty holiday rows\n",
							"\n",
							"nyc_taxi_holiday_df.createOrReplaceTempView(\"nyc_taxi_holiday_df\")\n",
							"spark.sql(\"SELECT * from nyc_taxi_holiday_df WHERE holidayName is NOT NULL \").show(5, truncate = False)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Enrich with weather data\n",
							"\n",
							"Now we append NOAA surface weather data to the taxi and holiday data. Use a similar approach to fetch the [NOAA weather history data](https://azure.microsoft.com/en-us/services/open-datasets/catalog/noaa-integrated-surface-data/) from Azure Open Datasets. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import NoaaIsdWeather\n",
							"\n",
							"isd = NoaaIsdWeather(start_date, end_date)\n",
							"isd_df = isd.to_spark_dataframe()"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"isd_df.show(5, truncate = False)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							"# Filter out weather info for new york city, remove the recording with null temperature \n",
							"\n",
							"weather_df = isd_df.filter(isd_df.latitude >= '40.53')\\\n",
							"                        .filter(isd_df.latitude <= '40.88')\\\n",
							"                        .filter(isd_df.longitude >= '-74.09')\\\n",
							"                        .filter(isd_df.longitude <= '-73.72')\\\n",
							"                        .filter(isd_df.temperature.isNotNull())\\\n",
							"                        .withColumnRenamed('datetime','datetime_full')\n",
							"                         "
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"# Remove unused columns\n",
							"\n",
							"columns_to_remove_weather = [\"usaf\", \"wban\", \"longitude\", \"latitude\"]\n",
							"weather_df_clean = weather_df.select([column for column in weather_df.columns if column not in columns_to_remove_weather])\\\n",
							"                        .withColumn('datetime',f.to_date('datetime_full'))\n",
							"\n",
							"weather_df_clean.show(5, truncate = False)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"source": [
							"Next group the weather data so that you have daily aggregated weather values. \n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Enrich weather data with aggregation statistics\n",
							"\n",
							"aggregations = {\"snowDepth\": \"mean\", \"precipTime\": \"max\", \"temperature\": \"mean\", \"precipDepth\": \"max\"}\n",
							"weather_df_grouped = weather_df_clean.groupby(\"datetime\").agg(aggregations)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"source": [
							"weather_df_grouped.show(5)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"# Rename columns\n",
							"\n",
							"weather_df_grouped = weather_df_grouped.withColumnRenamed('avg(snowDepth)','avg_snowDepth')\\\n",
							"                                       .withColumnRenamed('avg(temperature)','avg_temperature')\\\n",
							"                                       .withColumnRenamed('max(precipTime)','max_precipTime')\\\n",
							"                                       .withColumnRenamed('max(precipDepth)','max_precipDepth')"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"source": [
							"Merge the taxi and holiday data you prepared with the new weather data. This time you only need the datetime key, and again perform a left-join of the data. Run the describe() function on the new dataframe to see summary statistics for each field."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# enrich taxi data with weather\n",
							"nyc_taxi_holiday_weather_df = nyc_taxi_holiday_df.join(weather_df_grouped, on = 'datetime' , how = 'left')\n",
							"nyc_taxi_holiday_weather_df.cache()"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"source": [
							"nyc_taxi_holiday_weather_df.show(5)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"source": [
							"# Run the describe() function on the new dataframe to see summary statistics for each field.\n",
							"\n",
							"display(nyc_taxi_holiday_weather_df.describe())"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "markdown",
						"source": [
							"The summary statistics shows that the totalAmount field has negative values, which don't make sense in the context.\n",
							"\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Remove invalid rows with less than 0 taxi fare or tip\n",
							"final_df = nyc_taxi_holiday_weather_df.filter(nyc_taxi_holiday_weather_df.tipAmount > 0)\\\n",
							"                                      .filter(nyc_taxi_holiday_weather_df.totalAmount > 0)"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Cleaning up the existing Database\n",
							"\n",
							"First we need to drop the tables since Spark requires that a database is empty before we can drop the Database.\n",
							"\n",
							"Then we recreate the database and set the default database context to it."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DROP TABLE IF EXISTS NYCTaxi.nyc_taxi_holiday_weather\"); "
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DROP DATABASE IF EXISTS NYCTaxi\"); \n",
							"spark.sql(\"CREATE DATABASE NYCTaxi\"); \n",
							"spark.sql(\"USE NYCTaxi\");"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Creating a new table\n",
							"We create a nyc_taxi_holiday_weather table from the nyc_taxi_holiday_weather dataframe.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.types import *\n",
							"\n",
							"final_df.write.saveAsTable(\"nyc_taxi_holiday_weather\");\n",
							"spark.sql(\"SELECT COUNT(*) FROM nyc_taxi_holiday_weather\").show();"
						],
						"outputs": [],
						"execution_count": 25
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/drop tables')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "samples"
				},
				"nbformat": 0,
				"nbformat_minor": 0,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"DROP TABLE externaldeltatable"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"DROP TABLE externaldeltatable1"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"DROP TABLE manageddeltatable1"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"DROP TABLE  nyc_taxi_holiday_weather"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/graph-test')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "weko"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"test_data = [{'a': 1, 'b': 2, 'c': 3},{'a': 4, 'b': 5, 'c': 6},{'a': 7, 'b': 8, 'c': 9}]\n",
							"df = spark.createDataFrame(test_data)\n",
							"df.createOrReplaceTempView(\"abc\")\n",
							"df2 = spark.sql(\"SELECT * FROM abc\")\n",
							"df2.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/language test')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "cdb36d64-ed03-4a3e-8803-bcf003039163"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/long code')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "scala"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import org.apache.spark.HashPartitioner\n",
							"val sc = spark.sparkContext\n",
							"val data1 = Array[(Int, Char)](\n",
							"  (1, 'a'), (2, 'b'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (2, 'g'), (1, 'h'))\n",
							"\n",
							"val rangePairs1 = sc.parallelize(data1, 3)\n",
							"\n",
							"val hashPairs1 = rangePairs1.partitionBy(new HashPartitioner(3))\n",
							"\n",
							"val data2 = Array[(Int, String)]((1, \"A\"), (2, \"B\"),\n",
							"  (3, \"C\"), (4, \"D\"))\n",
							"\n",
							"val pairs2 = sc.parallelize(data2, 2)\n",
							"val rangePairs2 = pairs2.map(x => (x._1, x._2.charAt(0)))\n",
							"\n",
							"\n",
							"val data3 = Array[(Int, Char)]((1, 'X'), (2, 'Y'))\n",
							"val rangePairs3 = sc.parallelize(data3, 2)\n",
							"\n",
							"\n",
							"val rangePairs = rangePairs2.union(rangePairs3)\n",
							"\n",
							"\n",
							"val result = hashPairs1.join(rangePairs)\n",
							"\n",
							"print(result.collect())"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"import org.apache.spark.HashPartitioner\n",
							"val sc = spark.sparkContext\n",
							"val data1 = Array[(Int, Char)](\n",
							"  (1, 'a'), (2, 'b'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (2, 'g'), (1, 'h'))\n",
							"\n",
							"val rangePairs1 = sc.parallelize(data1, 3)\n",
							"\n",
							"val hashPairs1 = rangePairs1.partitionBy(new HashPartitioner(3))\n",
							"\n",
							"\n",
							"val data2 = Array[(Int, String)]((1, \"A\"), (2, \"B\"),\n",
							"  (3, \"C\"), (4, \"D\"))\n",
							"\n",
							"val pairs2 = sc.parallelize(data2, 2)\n",
							"val rangePairs2 = pairs2.map(x => (x._1, x._2.charAt(0)))\n",
							"\n",
							"\n",
							"val data3 = Array[(Int, Char)]((1, 'X'), (2, 'Y'))\n",
							"val rangePairs3 = sc.parallelize(data3, 2)\n",
							"\n",
							"\n",
							"val rangePairs = rangePairs2.union(rangePairs3)\n",
							"\n",
							"\n",
							"val result = hashPairs1.join(rangePairs)\n",
							"\n",
							"print(result.collect())"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"import org.apache.spark.HashPartitioner\n",
							"val sc = spark.sparkContext\n",
							"val data1 = Array[(Int, Char)](\n",
							"  (1, 'a'), (2, 'b'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (2, 'g'), (1, 'h'))\n",
							"\n",
							"val rangePairs1 = sc.parallelize(data1, 3)\n",
							"\n",
							"val hashPairs1 = rangePairs1.partitionBy(new HashPartitioner(3))\n",
							"\n",
							"\n",
							"val data2 = Array[(Int, String)]((1, \"A\"), (2, \"B\"),\n",
							"  (3, \"C\"), (4, \"D\"))\n",
							"\n",
							"val pairs2 = sc.parallelize(data2, 2)\n",
							"val rangePairs2 = pairs2.map(x => (x._1, x._2.charAt(0)))\n",
							"\n",
							"\n",
							"val data3 = Array[(Int, Char)]((1, 'X'), (2, 'Y'))\n",
							"val rangePairs3 = sc.parallelize(data3, 2)\n",
							"\n",
							"\n",
							"val rangePairs = rangePairs2.union(rangePairs3)\n",
							"\n",
							"\n",
							"val result = hashPairs1.join(rangePairs)\n",
							"\n",
							"print(result.collect())"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"import org.apache.spark.HashPartitioner\n",
							"val sc = spark.sparkContext\n",
							"val data1 = Array[(Int, Char)](\n",
							"  (1, 'a'), (2, 'b'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (2, 'g'), (1, 'h'))\n",
							"\n",
							"val rangePairs1 = sc.parallelize(data1, 3)\n",
							"\n",
							"val hashPairs1 = rangePairs1.partitionBy(new HashPartitioner(3))\n",
							"\n",
							"\n",
							"val data2 = Array[(Int, String)]((1, \"A\"), (2, \"B\"),\n",
							"  (3, \"C\"), (4, \"D\"))\n",
							"\n",
							"val pairs2 = sc.parallelize(data2, 2)\n",
							"val rangePairs2 = pairs2.map(x => (x._1, x._2.charAt(0)))\n",
							"\n",
							"\n",
							"val data3 = Array[(Int, Char)]((1, 'X'), (2, 'Y'))\n",
							"val rangePairs3 = sc.parallelize(data3, 2)\n",
							"\n",
							"\n",
							"val rangePairs = rangePairs2.union(rangePairs3)\n",
							"\n",
							"\n",
							"val result = hashPairs1.join(rangePairs)\n",
							"\n",
							"print(result.collect())"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"import org.apache.spark.HashPartitioner\n",
							"val sc = spark.sparkContext\n",
							"val data1 = Array[(Int, Char)](\n",
							"  (1, 'a'), (2, 'b'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (2, 'g'), (1, 'h'))\n",
							"\n",
							"val rangePairs1 = sc.parallelize(data1, 3)\n",
							"\n",
							"val hashPairs1 = rangePairs1.partitionBy(new HashPartitioner(3))\n",
							"\n",
							"\n",
							"val data2 = Array[(Int, String)]((1, \"A\"), (2, \"B\"),\n",
							"  (3, \"C\"), (4, \"D\"))\n",
							"\n",
							"val pairs2 = sc.parallelize(data2, 2)\n",
							"val rangePairs2 = pairs2.map(x => (x._1, x._2.charAt(0)))\n",
							"\n",
							"\n",
							"val data3 = Array[(Int, Char)]((1, 'X'), (2, 'Y'))\n",
							"val rangePairs3 = sc.parallelize(data3, 2)\n",
							"\n",
							"\n",
							"val rangePairs = rangePairs2.union(rangePairs3)\n",
							"\n",
							"\n",
							"val result = hashPairs1.join(rangePairs)\n",
							"\n",
							"print(result.collect())"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"import org.apache.spark.HashPartitioner\n",
							"val sc = spark.sparkContext\n",
							"val data1 = Array[(Int, Char)](\n",
							"  (1, 'a'), (2, 'b'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (2, 'g'), (1, 'h'))\n",
							"\n",
							"val rangePairs1 = sc.parallelize(data1, 3)\n",
							"\n",
							"val hashPairs1 = rangePairs1.partitionBy(new HashPartitioner(3))\n",
							"\n",
							"\n",
							"val data2 = Array[(Int, String)]((1, \"A\"), (2, \"B\"),\n",
							"  (3, \"C\"), (4, \"D\"))\n",
							"\n",
							"val pairs2 = sc.parallelize(data2, 2)\n",
							"val rangePairs2 = pairs2.map(x => (x._1, x._2.charAt(0)))\n",
							"\n",
							"\n",
							"val data3 = Array[(Int, Char)]((1, 'X'), (2, 'Y'))\n",
							"val rangePairs3 = sc.parallelize(data3, 2)\n",
							"\n",
							"\n",
							"val rangePairs = rangePairs2.union(rangePairs3)\n",
							"\n",
							"\n",
							"val result = hashPairs1.join(rangePairs)\n",
							"\n",
							"print(result.collect())"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"import org.apache.spark.HashPartitioner\n",
							"val sc = spark.sparkContext\n",
							"val data1 = Array[(Int, Char)](\n",
							"  (1, 'a'), (2, 'b'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (2, 'g'), (1, 'h'))\n",
							"\n",
							"val rangePairs1 = sc.parallelize(data1, 3)\n",
							"\n",
							"val hashPairs1 = rangePairs1.partitionBy(new HashPartitioner(3))\n",
							"\n",
							"\n",
							"val data2 = Array[(Int, String)]((1, \"A\"), (2, \"B\"),\n",
							"  (3, \"C\"), (4, \"D\"))\n",
							"\n",
							"val pairs2 = sc.parallelize(data2, 2)\n",
							"val rangePairs2 = pairs2.map(x => (x._1, x._2.charAt(0)))\n",
							"\n",
							"\n",
							"val data3 = Array[(Int, Char)]((1, 'X'), (2, 'Y'))\n",
							"val rangePairs3 = sc.parallelize(data3, 2)\n",
							"\n",
							"\n",
							"val rangePairs = rangePairs2.union(rangePairs3)\n",
							"\n",
							"\n",
							"val result = hashPairs1.join(rangePairs)\n",
							"\n",
							"print(result.collect())"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							"import org.apache.spark.HashPartitioner\n",
							"val sc = spark.sparkContext\n",
							"val data1 = Array[(Int, Char)](\n",
							"  (1, 'a'), (2, 'b'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (2, 'g'), (1, 'h'))\n",
							"\n",
							"val rangePairs1 = sc.parallelize(data1, 3)\n",
							"\n",
							"val hashPairs1 = rangePairs1.partitionBy(new HashPartitioner(3))\n",
							"\n",
							"\n",
							"val data2 = Array[(Int, String)]((1, \"A\"), (2, \"B\"),\n",
							"  (3, \"C\"), (4, \"D\"))\n",
							"\n",
							"val pairs2 = sc.parallelize(data2, 2)\n",
							"val rangePairs2 = pairs2.map(x => (x._1, x._2.charAt(0)))\n",
							"\n",
							"\n",
							"val data3 = Array[(Int, Char)]((1, 'X'), (2, 'Y'))\n",
							"val rangePairs3 = sc.parallelize(data3, 2)\n",
							"\n",
							"\n",
							"val rangePairs = rangePairs2.union(rangePairs3)\n",
							"\n",
							"\n",
							"val result = hashPairs1.join(rangePairs)\n",
							"\n",
							"print(result.collect())"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"import org.apache.spark.HashPartitioner\n",
							"val sc = spark.sparkContext\n",
							"val data1 = Array[(Int, Char)](\n",
							"  (1, 'a'), (2, 'b'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (2, 'g'), (1, 'h'))\n",
							"\n",
							"val rangePairs1 = sc.parallelize(data1, 3)\n",
							"\n",
							"val hashPairs1 = rangePairs1.partitionBy(new HashPartitioner(3))\n",
							"\n",
							"\n",
							"val data2 = Array[(Int, String)]((1, \"A\"), (2, \"B\"),\n",
							"  (3, \"C\"), (4, \"D\"))\n",
							"\n",
							"val pairs2 = sc.parallelize(data2, 2)\n",
							"val rangePairs2 = pairs2.map(x => (x._1, x._2.charAt(0)))\n",
							"\n",
							"\n",
							"val data3 = Array[(Int, Char)]((1, 'X'), (2, 'Y'))\n",
							"val rangePairs3 = sc.parallelize(data3, 2)\n",
							"\n",
							"\n",
							"val rangePairs = rangePairs2.union(rangePairs3)\n",
							"\n",
							"\n",
							"val result = hashPairs1.join(rangePairs)\n",
							"\n",
							"print(result.collect())"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"import org.apache.spark.HashPartitioner\n",
							"val sc = spark.sparkContext\n",
							"val data1 = Array[(Int, Char)](\n",
							"  (1, 'a'), (2, 'b'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (3, 'c'), (4, 'd'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (5, 'e'), (3, 'f'),\n",
							"  (2, 'g'), (1, 'h'))\n",
							"\n",
							"val rangePairs1 = sc.parallelize(data1, 3)\n",
							"\n",
							"val hashPairs1 = rangePairs1.partitionBy(new HashPartitioner(3))\n",
							"\n",
							"\n",
							"val data2 = Array[(Int, String)]((1, \"A\"), (2, \"B\"),\n",
							"  (3, \"C\"), (4, \"D\"))\n",
							"\n",
							"val pairs2 = sc.parallelize(data2, 2)\n",
							"val rangePairs2 = pairs2.map(x => (x._1, x._2.charAt(0)))\n",
							"\n",
							"\n",
							"val data3 = Array[(Int, Char)]((1, 'X'), (2, 'Y'))\n",
							"val rangePairs3 = sc.parallelize(data3, 2)\n",
							"\n",
							"\n",
							"val rangePairs = rangePairs2.union(rangePairs3)\n",
							"\n",
							"\n",
							"val result = hashPairs1.join(rangePairs)\n",
							"\n",
							"print(result.collect())"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/table for four languages')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "samples"
				},
				"nbformat": 0,
				"nbformat_minor": 0,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1",
						"state": {
							"bd7ee8f5-1ad4-4938-a4cc-0beb8c38cb50": {
								"type": "Synapse.DataFrame",
								"sync_state": {
									"table": {
										"rows": [
											{
												"employees": "[[michael, armbrust, noreply@berkeley.edore, 130000], [xiangrui, meng, no-reply@stanford.edu, 120000]]",
												"department": "[123456, Computer Science]"
											},
											{
												"employees": "[[matei,, no-reply@waterloo.edu, 140000], [, wendell, no-reply@berkeley.edu, 160000]]",
												"department": "[789012, Mechanical Engineering]"
											}
										],
										"schema": {
											"department": "StructType(StructField(id,StringType,true), StructField(name,StringType,true))",
											"employees": "ArrayType(StructType(StructField(firstName,StringType,true), StructField(lastName,StringType,true), StructField(email,StringType,true), StructField(salary,LongType,true)),true)"
										}
									},
									"isSummary": false,
									"language": "scala"
								},
								"persist_state": {
									"view": {
										"type": "details",
										"chartOptions": {
											"chartType": "bar",
											"aggregationType": "count",
											"categoryFieldKeys": [],
											"seriesFieldKeys": [
												"department"
											],
											"isStacked": false
										}
									}
								}
							},
							"980ee633-1cb2-4eeb-a51e-b7386976ae79": {
								"type": "Synapse.DataFrame",
								"sync_state": {
									"table": {
										"rows": [
											{
												"_3": "36",
												"_2": "34",
												"_1": "jack"
											},
											{
												"_3": "Delhi",
												"_2": "30",
												"_1": "Riti"
											},
											{
												"_3": "New York",
												"_2": "16",
												"_1": "Aadi"
											}
										],
										"schema": {
											"_1": "string",
											"_2": "bigint",
											"_3": "string"
										}
									},
									"isSummary": false,
									"language": "scala"
								},
								"persist_state": {
									"view": {
										"type": "details",
										"chartOptions": {
											"chartType": "bar",
											"aggregationType": "sum",
											"categoryFieldKeys": [
												"_1"
											],
											"seriesFieldKeys": [
												"_2"
											],
											"isStacked": false
										}
									}
								}
							},
							"b8e4bd1b-4bff-4097-a206-889daaa11cc1": {
								"type": "Synapse.DataFrame",
								"sync_state": {
									"table": {
										"rows": [
											{
												"col12": "3",
												"col11": "1"
											},
											{
												"col12": "4",
												"col11": "2"
											},
											{
												"col12": "5",
												"col11": "3"
											},
											{
												"col12": "6",
												"col11": "4"
											},
											{
												"col12": "8",
												"col11": "5"
											},
											{
												"col12": "3",
												"col11": "6"
											},
											{
												"col12": "16",
												"col11": "7"
											},
											{
												"col12": "20",
												"col11": "8"
											}
										],
										"schema": {
											"col11": "bigint",
											"col12": "bigint"
										}
									},
									"isSummary": false,
									"language": "scala"
								},
								"persist_state": {
									"view": {
										"type": "details",
										"chartOptions": {
											"chartType": "bar",
											"aggregationType": "sum",
											"categoryFieldKeys": [
												"col12"
											],
											"seriesFieldKeys": [
												"col11"
											],
											"isStacked": false
										}
									}
								}
							},
							"03eb3943-939b-4990-aaa7-3e1aa00259d3": {
								"type": "Synapse.DataFrame",
								"sync_state": {
									"table": {
										"rows": [
											{
												"id": "0"
											},
											{
												"id": "1"
											},
											{
												"id": "2"
											},
											{
												"id": "3"
											},
											{
												"id": "4"
											},
											{
												"id": "5"
											},
											{
												"id": "6"
											},
											{
												"id": "7"
											},
											{
												"id": "8"
											},
											{
												"id": "9"
											}
										],
										"schema": {
											"id": "bigint"
										}
									},
									"isSummary": false,
									"language": "scala"
								},
								"persist_state": {
									"view": {
										"type": "details",
										"chartOptions": {
											"chartType": "bar",
											"aggregationType": "sum",
											"categoryFieldKeys": [],
											"seriesFieldKeys": [
												"id"
											],
											"isStacked": false
										}
									}
								}
							},
							"53ca1b06-fbcd-4889-809d-6ab23e358465": {
								"type": "Synapse.DataFrame",
								"sync_state": {
									"table": {
										"rows": [
											{
												"key": "k_0001",
												"value": "1"
											},
											{
												"key": "k_0002",
												"value": "2"
											},
											{
												"key": "k_0003",
												"value": "3"
											},
											{
												"key": "k_0004",
												"value": "4"
											},
											{
												"key": "k_0005",
												"value": "5"
											},
											{
												"key": "k_0006",
												"value": "6"
											},
											{
												"key": "k_0007",
												"value": "7"
											},
											{
												"key": "k_0008",
												"value": "8"
											},
											{
												"key": "k_0009",
												"value": "9"
											},
											{
												"key": "k_0010",
												"value": "10"
											},
											{
												"key": "k_0011",
												"value": "11"
											},
											{
												"key": "k_0012",
												"value": "12"
											},
											{
												"key": "k_0013",
												"value": "13"
											},
											{
												"key": "k_0014",
												"value": "14"
											},
											{
												"key": "k_0015",
												"value": "15"
											},
											{
												"key": "k_0016",
												"value": "16"
											},
											{
												"key": "k_0017",
												"value": "17"
											},
											{
												"key": "k_0018",
												"value": "18"
											},
											{
												"key": "k_0019",
												"value": "19"
											},
											{
												"key": "k_0020",
												"value": "20"
											},
											{
												"key": "k_0021",
												"value": "21"
											},
											{
												"key": "k_0022",
												"value": "22"
											},
											{
												"key": "k_0023",
												"value": "23"
											},
											{
												"key": "k_0024",
												"value": "24"
											},
											{
												"key": "k_0025",
												"value": "25"
											},
											{
												"key": "k_0026",
												"value": "26"
											},
											{
												"key": "k_0027",
												"value": "27"
											},
											{
												"key": "k_0028",
												"value": "28"
											},
											{
												"key": "k_0029",
												"value": "29"
											},
											{
												"key": "k_0030",
												"value": "30"
											},
											{
												"key": "k_0031",
												"value": "31"
											},
											{
												"key": "k_0032",
												"value": "32"
											},
											{
												"key": "k_0033",
												"value": "33"
											},
											{
												"key": "k_0034",
												"value": "34"
											},
											{
												"key": "k_0035",
												"value": "35"
											},
											{
												"key": "k_0036",
												"value": "36"
											},
											{
												"key": "k_0037",
												"value": "37"
											},
											{
												"key": "k_0038",
												"value": "38"
											},
											{
												"key": "k_0039",
												"value": "39"
											},
											{
												"key": "k_0040",
												"value": "40"
											},
											{
												"key": "k_0041",
												"value": "41"
											},
											{
												"key": "k_0042",
												"value": "42"
											},
											{
												"key": "k_0043",
												"value": "43"
											},
											{
												"key": "k_0044",
												"value": "44"
											},
											{
												"key": "k_0045",
												"value": "45"
											},
											{
												"key": "k_0046",
												"value": "46"
											},
											{
												"key": "k_0047",
												"value": "47"
											},
											{
												"key": "k_0048",
												"value": "48"
											},
											{
												"key": "k_0049",
												"value": "49"
											},
											{
												"key": "k_0050",
												"value": "50"
											},
											{
												"key": "k_0051",
												"value": "51"
											},
											{
												"key": "k_0052",
												"value": "52"
											},
											{
												"key": "k_0053",
												"value": "53"
											},
											{
												"key": "k_0054",
												"value": "54"
											},
											{
												"key": "k_0055",
												"value": "55"
											},
											{
												"key": "k_0056",
												"value": "56"
											},
											{
												"key": "k_0057",
												"value": "57"
											},
											{
												"key": "k_0058",
												"value": "58"
											},
											{
												"key": "k_0059",
												"value": "59"
											},
											{
												"key": "k_0060",
												"value": "60"
											},
											{
												"key": "k_0061",
												"value": "61"
											},
											{
												"key": "k_0062",
												"value": "62"
											},
											{
												"key": "k_0063",
												"value": "63"
											},
											{
												"key": "k_0064",
												"value": "64"
											},
											{
												"key": "k_0065",
												"value": "65"
											},
											{
												"key": "k_0066",
												"value": "66"
											},
											{
												"key": "k_0067",
												"value": "67"
											},
											{
												"key": "k_0068",
												"value": "68"
											},
											{
												"key": "k_0069",
												"value": "69"
											},
											{
												"key": "k_0070",
												"value": "70"
											},
											{
												"key": "k_0071",
												"value": "71"
											},
											{
												"key": "k_0072",
												"value": "72"
											},
											{
												"key": "k_0073",
												"value": "73"
											},
											{
												"key": "k_0074",
												"value": "74"
											},
											{
												"key": "k_0075",
												"value": "75"
											},
											{
												"key": "k_0076",
												"value": "76"
											},
											{
												"key": "k_0077",
												"value": "77"
											},
											{
												"key": "k_0078",
												"value": "78"
											},
											{
												"key": "k_0079",
												"value": "79"
											},
											{
												"key": "k_0080",
												"value": "80"
											},
											{
												"key": "k_0081",
												"value": "81"
											},
											{
												"key": "k_0082",
												"value": "82"
											},
											{
												"key": "k_0083",
												"value": "83"
											},
											{
												"key": "k_0084",
												"value": "84"
											},
											{
												"key": "k_0085",
												"value": "85"
											},
											{
												"key": "k_0086",
												"value": "86"
											},
											{
												"key": "k_0087",
												"value": "87"
											},
											{
												"key": "k_0088",
												"value": "88"
											},
											{
												"key": "k_0089",
												"value": "89"
											},
											{
												"key": "k_0090",
												"value": "90"
											},
											{
												"key": "k_0091",
												"value": "91"
											},
											{
												"key": "k_0092",
												"value": "92"
											},
											{
												"key": "k_0093",
												"value": "93"
											},
											{
												"key": "k_0094",
												"value": "94"
											},
											{
												"key": "k_0095",
												"value": "95"
											},
											{
												"key": "k_0096",
												"value": "96"
											},
											{
												"key": "k_0097",
												"value": "97"
											},
											{
												"key": "k_0098",
												"value": "98"
											},
											{
												"key": "k_0099",
												"value": "99"
											},
											{
												"key": "k_0100",
												"value": "100"
											},
											{
												"key": "k_0101",
												"value": "101"
											},
											{
												"key": "k_0102",
												"value": "102"
											},
											{
												"key": "k_0103",
												"value": "103"
											},
											{
												"key": "k_0104",
												"value": "104"
											},
											{
												"key": "k_0105",
												"value": "105"
											},
											{
												"key": "k_0106",
												"value": "106"
											},
											{
												"key": "k_0107",
												"value": "107"
											},
											{
												"key": "k_0108",
												"value": "108"
											},
											{
												"key": "k_0109",
												"value": "109"
											},
											{
												"key": "k_0110",
												"value": "110"
											},
											{
												"key": "k_0111",
												"value": "111"
											},
											{
												"key": "k_0112",
												"value": "112"
											},
											{
												"key": "k_0113",
												"value": "113"
											},
											{
												"key": "k_0114",
												"value": "114"
											},
											{
												"key": "k_0115",
												"value": "115"
											},
											{
												"key": "k_0116",
												"value": "116"
											},
											{
												"key": "k_0117",
												"value": "117"
											},
											{
												"key": "k_0118",
												"value": "118"
											},
											{
												"key": "k_0119",
												"value": "119"
											},
											{
												"key": "k_0120",
												"value": "120"
											},
											{
												"key": "k_0121",
												"value": "121"
											},
											{
												"key": "k_0122",
												"value": "122"
											},
											{
												"key": "k_0123",
												"value": "123"
											},
											{
												"key": "k_0124",
												"value": "124"
											},
											{
												"key": "k_0125",
												"value": "125"
											},
											{
												"key": "k_0126",
												"value": "126"
											},
											{
												"key": "k_0127",
												"value": "127"
											},
											{
												"key": "k_0128",
												"value": "128"
											},
											{
												"key": "k_0129",
												"value": "129"
											},
											{
												"key": "k_0130",
												"value": "130"
											},
											{
												"key": "k_0131",
												"value": "131"
											},
											{
												"key": "k_0132",
												"value": "132"
											},
											{
												"key": "k_0133",
												"value": "133"
											},
											{
												"key": "k_0134",
												"value": "134"
											},
											{
												"key": "k_0135",
												"value": "135"
											},
											{
												"key": "k_0136",
												"value": "136"
											},
											{
												"key": "k_0137",
												"value": "137"
											},
											{
												"key": "k_0138",
												"value": "138"
											},
											{
												"key": "k_0139",
												"value": "139"
											},
											{
												"key": "k_0140",
												"value": "140"
											},
											{
												"key": "k_0141",
												"value": "141"
											},
											{
												"key": "k_0142",
												"value": "142"
											},
											{
												"key": "k_0143",
												"value": "143"
											},
											{
												"key": "k_0144",
												"value": "144"
											},
											{
												"key": "k_0145",
												"value": "145"
											},
											{
												"key": "k_0146",
												"value": "146"
											},
											{
												"key": "k_0147",
												"value": "147"
											},
											{
												"key": "k_0148",
												"value": "148"
											},
											{
												"key": "k_0149",
												"value": "149"
											},
											{
												"key": "k_0150",
												"value": "150"
											},
											{
												"key": "k_0151",
												"value": "151"
											},
											{
												"key": "k_0152",
												"value": "152"
											},
											{
												"key": "k_0153",
												"value": "153"
											},
											{
												"key": "k_0154",
												"value": "154"
											},
											{
												"key": "k_0155",
												"value": "155"
											},
											{
												"key": "k_0156",
												"value": "156"
											},
											{
												"key": "k_0157",
												"value": "157"
											},
											{
												"key": "k_0158",
												"value": "158"
											},
											{
												"key": "k_0159",
												"value": "159"
											},
											{
												"key": "k_0160",
												"value": "160"
											},
											{
												"key": "k_0161",
												"value": "161"
											},
											{
												"key": "k_0162",
												"value": "162"
											},
											{
												"key": "k_0163",
												"value": "163"
											},
											{
												"key": "k_0164",
												"value": "164"
											},
											{
												"key": "k_0165",
												"value": "165"
											},
											{
												"key": "k_0166",
												"value": "166"
											},
											{
												"key": "k_0167",
												"value": "167"
											},
											{
												"key": "k_0168",
												"value": "168"
											},
											{
												"key": "k_0169",
												"value": "169"
											},
											{
												"key": "k_0170",
												"value": "170"
											},
											{
												"key": "k_0171",
												"value": "171"
											},
											{
												"key": "k_0172",
												"value": "172"
											},
											{
												"key": "k_0173",
												"value": "173"
											},
											{
												"key": "k_0174",
												"value": "174"
											},
											{
												"key": "k_0175",
												"value": "175"
											},
											{
												"key": "k_0176",
												"value": "176"
											},
											{
												"key": "k_0177",
												"value": "177"
											},
											{
												"key": "k_0178",
												"value": "178"
											},
											{
												"key": "k_0179",
												"value": "179"
											},
											{
												"key": "k_0180",
												"value": "180"
											},
											{
												"key": "k_0181",
												"value": "181"
											},
											{
												"key": "k_0182",
												"value": "182"
											},
											{
												"key": "k_0183",
												"value": "183"
											},
											{
												"key": "k_0184",
												"value": "184"
											},
											{
												"key": "k_0185",
												"value": "185"
											},
											{
												"key": "k_0186",
												"value": "186"
											},
											{
												"key": "k_0187",
												"value": "187"
											},
											{
												"key": "k_0188",
												"value": "188"
											},
											{
												"key": "k_0189",
												"value": "189"
											},
											{
												"key": "k_0190",
												"value": "190"
											},
											{
												"key": "k_0191",
												"value": "191"
											},
											{
												"key": "k_0192",
												"value": "192"
											},
											{
												"key": "k_0193",
												"value": "193"
											},
											{
												"key": "k_0194",
												"value": "194"
											},
											{
												"key": "k_0195",
												"value": "195"
											},
											{
												"key": "k_0196",
												"value": "196"
											},
											{
												"key": "k_0197",
												"value": "197"
											},
											{
												"key": "k_0198",
												"value": "198"
											},
											{
												"key": "k_0199",
												"value": "199"
											},
											{
												"key": "k_0200",
												"value": "200"
											},
											{
												"key": "k_0201",
												"value": "201"
											},
											{
												"key": "k_0202",
												"value": "202"
											},
											{
												"key": "k_0203",
												"value": "203"
											},
											{
												"key": "k_0204",
												"value": "204"
											},
											{
												"key": "k_0205",
												"value": "205"
											},
											{
												"key": "k_0206",
												"value": "206"
											},
											{
												"key": "k_0207",
												"value": "207"
											},
											{
												"key": "k_0208",
												"value": "208"
											},
											{
												"key": "k_0209",
												"value": "209"
											},
											{
												"key": "k_0210",
												"value": "210"
											},
											{
												"key": "k_0211",
												"value": "211"
											},
											{
												"key": "k_0212",
												"value": "212"
											},
											{
												"key": "k_0213",
												"value": "213"
											},
											{
												"key": "k_0214",
												"value": "214"
											},
											{
												"key": "k_0215",
												"value": "215"
											},
											{
												"key": "k_0216",
												"value": "216"
											},
											{
												"key": "k_0217",
												"value": "217"
											},
											{
												"key": "k_0218",
												"value": "218"
											},
											{
												"key": "k_0219",
												"value": "219"
											},
											{
												"key": "k_0220",
												"value": "220"
											},
											{
												"key": "k_0221",
												"value": "221"
											},
											{
												"key": "k_0222",
												"value": "222"
											},
											{
												"key": "k_0223",
												"value": "223"
											},
											{
												"key": "k_0224",
												"value": "224"
											},
											{
												"key": "k_0225",
												"value": "225"
											},
											{
												"key": "k_0226",
												"value": "226"
											},
											{
												"key": "k_0227",
												"value": "227"
											},
											{
												"key": "k_0228",
												"value": "228"
											},
											{
												"key": "k_0229",
												"value": "229"
											},
											{
												"key": "k_0230",
												"value": "230"
											},
											{
												"key": "k_0231",
												"value": "231"
											},
											{
												"key": "k_0232",
												"value": "232"
											},
											{
												"key": "k_0233",
												"value": "233"
											},
											{
												"key": "k_0234",
												"value": "234"
											},
											{
												"key": "k_0235",
												"value": "235"
											},
											{
												"key": "k_0236",
												"value": "236"
											},
											{
												"key": "k_0237",
												"value": "237"
											},
											{
												"key": "k_0238",
												"value": "238"
											},
											{
												"key": "k_0239",
												"value": "239"
											},
											{
												"key": "k_0240",
												"value": "240"
											},
											{
												"key": "k_0241",
												"value": "241"
											},
											{
												"key": "k_0242",
												"value": "242"
											},
											{
												"key": "k_0243",
												"value": "243"
											},
											{
												"key": "k_0244",
												"value": "244"
											},
											{
												"key": "k_0245",
												"value": "245"
											},
											{
												"key": "k_0246",
												"value": "246"
											},
											{
												"key": "k_0247",
												"value": "247"
											},
											{
												"key": "k_0248",
												"value": "248"
											},
											{
												"key": "k_0249",
												"value": "249"
											},
											{
												"key": "k_0250",
												"value": "250"
											},
											{
												"key": "k_0251",
												"value": "251"
											},
											{
												"key": "k_0252",
												"value": "252"
											},
											{
												"key": "k_0253",
												"value": "253"
											},
											{
												"key": "k_0254",
												"value": "254"
											},
											{
												"key": "k_0255",
												"value": "255"
											},
											{
												"key": "k_0256",
												"value": "256"
											},
											{
												"key": "k_0257",
												"value": "257"
											},
											{
												"key": "k_0258",
												"value": "258"
											},
											{
												"key": "k_0259",
												"value": "259"
											},
											{
												"key": "k_0260",
												"value": "260"
											},
											{
												"key": "k_0261",
												"value": "261"
											},
											{
												"key": "k_0262",
												"value": "262"
											},
											{
												"key": "k_0263",
												"value": "263"
											},
											{
												"key": "k_0264",
												"value": "264"
											},
											{
												"key": "k_0265",
												"value": "265"
											},
											{
												"key": "k_0266",
												"value": "266"
											},
											{
												"key": "k_0267",
												"value": "267"
											},
											{
												"key": "k_0268",
												"value": "268"
											},
											{
												"key": "k_0269",
												"value": "269"
											},
											{
												"key": "k_0270",
												"value": "270"
											},
											{
												"key": "k_0271",
												"value": "271"
											},
											{
												"key": "k_0272",
												"value": "272"
											},
											{
												"key": "k_0273",
												"value": "273"
											},
											{
												"key": "k_0274",
												"value": "274"
											},
											{
												"key": "k_0275",
												"value": "275"
											},
											{
												"key": "k_0276",
												"value": "276"
											},
											{
												"key": "k_0277",
												"value": "277"
											},
											{
												"key": "k_0278",
												"value": "278"
											},
											{
												"key": "k_0279",
												"value": "279"
											},
											{
												"key": "k_0280",
												"value": "280"
											},
											{
												"key": "k_0281",
												"value": "281"
											},
											{
												"key": "k_0282",
												"value": "282"
											},
											{
												"key": "k_0283",
												"value": "283"
											},
											{
												"key": "k_0284",
												"value": "284"
											},
											{
												"key": "k_0285",
												"value": "285"
											},
											{
												"key": "k_0286",
												"value": "286"
											},
											{
												"key": "k_0287",
												"value": "287"
											},
											{
												"key": "k_0288",
												"value": "288"
											},
											{
												"key": "k_0289",
												"value": "289"
											},
											{
												"key": "k_0290",
												"value": "290"
											},
											{
												"key": "k_0291",
												"value": "291"
											},
											{
												"key": "k_0292",
												"value": "292"
											},
											{
												"key": "k_0293",
												"value": "293"
											},
											{
												"key": "k_0294",
												"value": "294"
											},
											{
												"key": "k_0295",
												"value": "295"
											},
											{
												"key": "k_0296",
												"value": "296"
											},
											{
												"key": "k_0297",
												"value": "297"
											},
											{
												"key": "k_0298",
												"value": "298"
											},
											{
												"key": "k_0299",
												"value": "299"
											},
											{
												"key": "k_0300",
												"value": "300"
											},
											{
												"key": "k_0301",
												"value": "301"
											},
											{
												"key": "k_0302",
												"value": "302"
											},
											{
												"key": "k_0303",
												"value": "303"
											},
											{
												"key": "k_0304",
												"value": "304"
											},
											{
												"key": "k_0305",
												"value": "305"
											},
											{
												"key": "k_0306",
												"value": "306"
											},
											{
												"key": "k_0307",
												"value": "307"
											},
											{
												"key": "k_0308",
												"value": "308"
											},
											{
												"key": "k_0309",
												"value": "309"
											},
											{
												"key": "k_0310",
												"value": "310"
											},
											{
												"key": "k_0311",
												"value": "311"
											},
											{
												"key": "k_0312",
												"value": "312"
											},
											{
												"key": "k_0313",
												"value": "313"
											},
											{
												"key": "k_0314",
												"value": "314"
											},
											{
												"key": "k_0315",
												"value": "315"
											},
											{
												"key": "k_0316",
												"value": "316"
											},
											{
												"key": "k_0317",
												"value": "317"
											},
											{
												"key": "k_0318",
												"value": "318"
											},
											{
												"key": "k_0319",
												"value": "319"
											},
											{
												"key": "k_0320",
												"value": "320"
											},
											{
												"key": "k_0321",
												"value": "321"
											},
											{
												"key": "k_0322",
												"value": "322"
											},
											{
												"key": "k_0323",
												"value": "323"
											},
											{
												"key": "k_0324",
												"value": "324"
											},
											{
												"key": "k_0325",
												"value": "325"
											},
											{
												"key": "k_0326",
												"value": "326"
											},
											{
												"key": "k_0327",
												"value": "327"
											},
											{
												"key": "k_0328",
												"value": "328"
											},
											{
												"key": "k_0329",
												"value": "329"
											},
											{
												"key": "k_0330",
												"value": "330"
											},
											{
												"key": "k_0331",
												"value": "331"
											},
											{
												"key": "k_0332",
												"value": "332"
											},
											{
												"key": "k_0333",
												"value": "333"
											},
											{
												"key": "k_0334",
												"value": "334"
											},
											{
												"key": "k_0335",
												"value": "335"
											},
											{
												"key": "k_0336",
												"value": "336"
											},
											{
												"key": "k_0337",
												"value": "337"
											},
											{
												"key": "k_0338",
												"value": "338"
											},
											{
												"key": "k_0339",
												"value": "339"
											},
											{
												"key": "k_0340",
												"value": "340"
											},
											{
												"key": "k_0341",
												"value": "341"
											},
											{
												"key": "k_0342",
												"value": "342"
											},
											{
												"key": "k_0343",
												"value": "343"
											},
											{
												"key": "k_0344",
												"value": "344"
											},
											{
												"key": "k_0345",
												"value": "345"
											},
											{
												"key": "k_0346",
												"value": "346"
											},
											{
												"key": "k_0347",
												"value": "347"
											},
											{
												"key": "k_0348",
												"value": "348"
											},
											{
												"key": "k_0349",
												"value": "349"
											},
											{
												"key": "k_0350",
												"value": "350"
											},
											{
												"key": "k_0351",
												"value": "351"
											},
											{
												"key": "k_0352",
												"value": "352"
											},
											{
												"key": "k_0353",
												"value": "353"
											},
											{
												"key": "k_0354",
												"value": "354"
											},
											{
												"key": "k_0355",
												"value": "355"
											},
											{
												"key": "k_0356",
												"value": "356"
											},
											{
												"key": "k_0357",
												"value": "357"
											},
											{
												"key": "k_0358",
												"value": "358"
											},
											{
												"key": "k_0359",
												"value": "359"
											},
											{
												"key": "k_0360",
												"value": "360"
											},
											{
												"key": "k_0361",
												"value": "361"
											},
											{
												"key": "k_0362",
												"value": "362"
											},
											{
												"key": "k_0363",
												"value": "363"
											},
											{
												"key": "k_0364",
												"value": "364"
											},
											{
												"key": "k_0365",
												"value": "365"
											},
											{
												"key": "k_0366",
												"value": "366"
											},
											{
												"key": "k_0367",
												"value": "367"
											},
											{
												"key": "k_0368",
												"value": "368"
											},
											{
												"key": "k_0369",
												"value": "369"
											},
											{
												"key": "k_0370",
												"value": "370"
											},
											{
												"key": "k_0371",
												"value": "371"
											},
											{
												"key": "k_0372",
												"value": "372"
											},
											{
												"key": "k_0373",
												"value": "373"
											},
											{
												"key": "k_0374",
												"value": "374"
											},
											{
												"key": "k_0375",
												"value": "375"
											},
											{
												"key": "k_0376",
												"value": "376"
											},
											{
												"key": "k_0377",
												"value": "377"
											},
											{
												"key": "k_0378",
												"value": "378"
											},
											{
												"key": "k_0379",
												"value": "379"
											},
											{
												"key": "k_0380",
												"value": "380"
											},
											{
												"key": "k_0381",
												"value": "381"
											},
											{
												"key": "k_0382",
												"value": "382"
											},
											{
												"key": "k_0383",
												"value": "383"
											},
											{
												"key": "k_0384",
												"value": "384"
											},
											{
												"key": "k_0385",
												"value": "385"
											},
											{
												"key": "k_0386",
												"value": "386"
											},
											{
												"key": "k_0387",
												"value": "387"
											},
											{
												"key": "k_0388",
												"value": "388"
											},
											{
												"key": "k_0389",
												"value": "389"
											},
											{
												"key": "k_0390",
												"value": "390"
											},
											{
												"key": "k_0391",
												"value": "391"
											},
											{
												"key": "k_0392",
												"value": "392"
											},
											{
												"key": "k_0393",
												"value": "393"
											},
											{
												"key": "k_0394",
												"value": "394"
											},
											{
												"key": "k_0395",
												"value": "395"
											},
											{
												"key": "k_0396",
												"value": "396"
											},
											{
												"key": "k_0397",
												"value": "397"
											},
											{
												"key": "k_0398",
												"value": "398"
											},
											{
												"key": "k_0399",
												"value": "399"
											},
											{
												"key": "k_0400",
												"value": "400"
											},
											{
												"key": "k_0401",
												"value": "401"
											},
											{
												"key": "k_0402",
												"value": "402"
											},
											{
												"key": "k_0403",
												"value": "403"
											},
											{
												"key": "k_0404",
												"value": "404"
											},
											{
												"key": "k_0405",
												"value": "405"
											},
											{
												"key": "k_0406",
												"value": "406"
											},
											{
												"key": "k_0407",
												"value": "407"
											},
											{
												"key": "k_0408",
												"value": "408"
											},
											{
												"key": "k_0409",
												"value": "409"
											},
											{
												"key": "k_0410",
												"value": "410"
											},
											{
												"key": "k_0411",
												"value": "411"
											},
											{
												"key": "k_0412",
												"value": "412"
											},
											{
												"key": "k_0413",
												"value": "413"
											},
											{
												"key": "k_0414",
												"value": "414"
											},
											{
												"key": "k_0415",
												"value": "415"
											},
											{
												"key": "k_0416",
												"value": "416"
											},
											{
												"key": "k_0417",
												"value": "417"
											},
											{
												"key": "k_0418",
												"value": "418"
											},
											{
												"key": "k_0419",
												"value": "419"
											},
											{
												"key": "k_0420",
												"value": "420"
											},
											{
												"key": "k_0421",
												"value": "421"
											},
											{
												"key": "k_0422",
												"value": "422"
											},
											{
												"key": "k_0423",
												"value": "423"
											},
											{
												"key": "k_0424",
												"value": "424"
											},
											{
												"key": "k_0425",
												"value": "425"
											},
											{
												"key": "k_0426",
												"value": "426"
											},
											{
												"key": "k_0427",
												"value": "427"
											},
											{
												"key": "k_0428",
												"value": "428"
											},
											{
												"key": "k_0429",
												"value": "429"
											},
											{
												"key": "k_0430",
												"value": "430"
											},
											{
												"key": "k_0431",
												"value": "431"
											},
											{
												"key": "k_0432",
												"value": "432"
											},
											{
												"key": "k_0433",
												"value": "433"
											},
											{
												"key": "k_0434",
												"value": "434"
											},
											{
												"key": "k_0435",
												"value": "435"
											},
											{
												"key": "k_0436",
												"value": "436"
											},
											{
												"key": "k_0437",
												"value": "437"
											},
											{
												"key": "k_0438",
												"value": "438"
											},
											{
												"key": "k_0439",
												"value": "439"
											},
											{
												"key": "k_0440",
												"value": "440"
											},
											{
												"key": "k_0441",
												"value": "441"
											},
											{
												"key": "k_0442",
												"value": "442"
											},
											{
												"key": "k_0443",
												"value": "443"
											},
											{
												"key": "k_0444",
												"value": "444"
											},
											{
												"key": "k_0445",
												"value": "445"
											},
											{
												"key": "k_0446",
												"value": "446"
											},
											{
												"key": "k_0447",
												"value": "447"
											},
											{
												"key": "k_0448",
												"value": "448"
											},
											{
												"key": "k_0449",
												"value": "449"
											},
											{
												"key": "k_0450",
												"value": "450"
											},
											{
												"key": "k_0451",
												"value": "451"
											},
											{
												"key": "k_0452",
												"value": "452"
											},
											{
												"key": "k_0453",
												"value": "453"
											},
											{
												"key": "k_0454",
												"value": "454"
											},
											{
												"key": "k_0455",
												"value": "455"
											},
											{
												"key": "k_0456",
												"value": "456"
											},
											{
												"key": "k_0457",
												"value": "457"
											},
											{
												"key": "k_0458",
												"value": "458"
											},
											{
												"key": "k_0459",
												"value": "459"
											},
											{
												"key": "k_0460",
												"value": "460"
											},
											{
												"key": "k_0461",
												"value": "461"
											},
											{
												"key": "k_0462",
												"value": "462"
											},
											{
												"key": "k_0463",
												"value": "463"
											},
											{
												"key": "k_0464",
												"value": "464"
											},
											{
												"key": "k_0465",
												"value": "465"
											},
											{
												"key": "k_0466",
												"value": "466"
											},
											{
												"key": "k_0467",
												"value": "467"
											},
											{
												"key": "k_0468",
												"value": "468"
											},
											{
												"key": "k_0469",
												"value": "469"
											},
											{
												"key": "k_0470",
												"value": "470"
											},
											{
												"key": "k_0471",
												"value": "471"
											},
											{
												"key": "k_0472",
												"value": "472"
											},
											{
												"key": "k_0473",
												"value": "473"
											},
											{
												"key": "k_0474",
												"value": "474"
											},
											{
												"key": "k_0475",
												"value": "475"
											},
											{
												"key": "k_0476",
												"value": "476"
											},
											{
												"key": "k_0477",
												"value": "477"
											},
											{
												"key": "k_0478",
												"value": "478"
											},
											{
												"key": "k_0479",
												"value": "479"
											},
											{
												"key": "k_0480",
												"value": "480"
											},
											{
												"key": "k_0481",
												"value": "481"
											},
											{
												"key": "k_0482",
												"value": "482"
											},
											{
												"key": "k_0483",
												"value": "483"
											},
											{
												"key": "k_0484",
												"value": "484"
											},
											{
												"key": "k_0485",
												"value": "485"
											},
											{
												"key": "k_0486",
												"value": "486"
											},
											{
												"key": "k_0487",
												"value": "487"
											},
											{
												"key": "k_0488",
												"value": "488"
											},
											{
												"key": "k_0489",
												"value": "489"
											},
											{
												"key": "k_0490",
												"value": "490"
											},
											{
												"key": "k_0491",
												"value": "491"
											},
											{
												"key": "k_0492",
												"value": "492"
											},
											{
												"key": "k_0493",
												"value": "493"
											},
											{
												"key": "k_0494",
												"value": "494"
											},
											{
												"key": "k_0495",
												"value": "495"
											},
											{
												"key": "k_0496",
												"value": "496"
											},
											{
												"key": "k_0497",
												"value": "497"
											},
											{
												"key": "k_0498",
												"value": "498"
											},
											{
												"key": "k_0499",
												"value": "499"
											},
											{
												"key": "k_0500",
												"value": "500"
											},
											{
												"key": "k_0501",
												"value": "501"
											},
											{
												"key": "k_0502",
												"value": "502"
											},
											{
												"key": "k_0503",
												"value": "503"
											},
											{
												"key": "k_0504",
												"value": "504"
											},
											{
												"key": "k_0505",
												"value": "505"
											},
											{
												"key": "k_0506",
												"value": "506"
											},
											{
												"key": "k_0507",
												"value": "507"
											},
											{
												"key": "k_0508",
												"value": "508"
											},
											{
												"key": "k_0509",
												"value": "509"
											},
											{
												"key": "k_0510",
												"value": "510"
											},
											{
												"key": "k_0511",
												"value": "511"
											},
											{
												"key": "k_0512",
												"value": "512"
											},
											{
												"key": "k_0513",
												"value": "513"
											},
											{
												"key": "k_0514",
												"value": "514"
											},
											{
												"key": "k_0515",
												"value": "515"
											},
											{
												"key": "k_0516",
												"value": "516"
											},
											{
												"key": "k_0517",
												"value": "517"
											},
											{
												"key": "k_0518",
												"value": "518"
											},
											{
												"key": "k_0519",
												"value": "519"
											},
											{
												"key": "k_0520",
												"value": "520"
											},
											{
												"key": "k_0521",
												"value": "521"
											},
											{
												"key": "k_0522",
												"value": "522"
											},
											{
												"key": "k_0523",
												"value": "523"
											},
											{
												"key": "k_0524",
												"value": "524"
											},
											{
												"key": "k_0525",
												"value": "525"
											},
											{
												"key": "k_0526",
												"value": "526"
											},
											{
												"key": "k_0527",
												"value": "527"
											},
											{
												"key": "k_0528",
												"value": "528"
											},
											{
												"key": "k_0529",
												"value": "529"
											},
											{
												"key": "k_0530",
												"value": "530"
											},
											{
												"key": "k_0531",
												"value": "531"
											},
											{
												"key": "k_0532",
												"value": "532"
											},
											{
												"key": "k_0533",
												"value": "533"
											},
											{
												"key": "k_0534",
												"value": "534"
											},
											{
												"key": "k_0535",
												"value": "535"
											},
											{
												"key": "k_0536",
												"value": "536"
											},
											{
												"key": "k_0537",
												"value": "537"
											},
											{
												"key": "k_0538",
												"value": "538"
											},
											{
												"key": "k_0539",
												"value": "539"
											},
											{
												"key": "k_0540",
												"value": "540"
											},
											{
												"key": "k_0541",
												"value": "541"
											},
											{
												"key": "k_0542",
												"value": "542"
											},
											{
												"key": "k_0543",
												"value": "543"
											},
											{
												"key": "k_0544",
												"value": "544"
											},
											{
												"key": "k_0545",
												"value": "545"
											},
											{
												"key": "k_0546",
												"value": "546"
											},
											{
												"key": "k_0547",
												"value": "547"
											},
											{
												"key": "k_0548",
												"value": "548"
											},
											{
												"key": "k_0549",
												"value": "549"
											},
											{
												"key": "k_0550",
												"value": "550"
											},
											{
												"key": "k_0551",
												"value": "551"
											},
											{
												"key": "k_0552",
												"value": "552"
											},
											{
												"key": "k_0553",
												"value": "553"
											},
											{
												"key": "k_0554",
												"value": "554"
											},
											{
												"key": "k_0555",
												"value": "555"
											},
											{
												"key": "k_0556",
												"value": "556"
											},
											{
												"key": "k_0557",
												"value": "557"
											},
											{
												"key": "k_0558",
												"value": "558"
											},
											{
												"key": "k_0559",
												"value": "559"
											},
											{
												"key": "k_0560",
												"value": "560"
											},
											{
												"key": "k_0561",
												"value": "561"
											},
											{
												"key": "k_0562",
												"value": "562"
											},
											{
												"key": "k_0563",
												"value": "563"
											},
											{
												"key": "k_0564",
												"value": "564"
											},
											{
												"key": "k_0565",
												"value": "565"
											},
											{
												"key": "k_0566",
												"value": "566"
											},
											{
												"key": "k_0567",
												"value": "567"
											},
											{
												"key": "k_0568",
												"value": "568"
											},
											{
												"key": "k_0569",
												"value": "569"
											},
											{
												"key": "k_0570",
												"value": "570"
											},
											{
												"key": "k_0571",
												"value": "571"
											},
											{
												"key": "k_0572",
												"value": "572"
											},
											{
												"key": "k_0573",
												"value": "573"
											},
											{
												"key": "k_0574",
												"value": "574"
											},
											{
												"key": "k_0575",
												"value": "575"
											},
											{
												"key": "k_0576",
												"value": "576"
											},
											{
												"key": "k_0577",
												"value": "577"
											},
											{
												"key": "k_0578",
												"value": "578"
											},
											{
												"key": "k_0579",
												"value": "579"
											},
											{
												"key": "k_0580",
												"value": "580"
											},
											{
												"key": "k_0581",
												"value": "581"
											},
											{
												"key": "k_0582",
												"value": "582"
											},
											{
												"key": "k_0583",
												"value": "583"
											},
											{
												"key": "k_0584",
												"value": "584"
											},
											{
												"key": "k_0585",
												"value": "585"
											},
											{
												"key": "k_0586",
												"value": "586"
											},
											{
												"key": "k_0587",
												"value": "587"
											},
											{
												"key": "k_0588",
												"value": "588"
											},
											{
												"key": "k_0589",
												"value": "589"
											},
											{
												"key": "k_0590",
												"value": "590"
											},
											{
												"key": "k_0591",
												"value": "591"
											},
											{
												"key": "k_0592",
												"value": "592"
											},
											{
												"key": "k_0593",
												"value": "593"
											},
											{
												"key": "k_0594",
												"value": "594"
											},
											{
												"key": "k_0595",
												"value": "595"
											},
											{
												"key": "k_0596",
												"value": "596"
											},
											{
												"key": "k_0597",
												"value": "597"
											},
											{
												"key": "k_0598",
												"value": "598"
											},
											{
												"key": "k_0599",
												"value": "599"
											},
											{
												"key": "k_0600",
												"value": "600"
											},
											{
												"key": "k_0601",
												"value": "601"
											},
											{
												"key": "k_0602",
												"value": "602"
											},
											{
												"key": "k_0603",
												"value": "603"
											},
											{
												"key": "k_0604",
												"value": "604"
											},
											{
												"key": "k_0605",
												"value": "605"
											},
											{
												"key": "k_0606",
												"value": "606"
											},
											{
												"key": "k_0607",
												"value": "607"
											},
											{
												"key": "k_0608",
												"value": "608"
											},
											{
												"key": "k_0609",
												"value": "609"
											},
											{
												"key": "k_0610",
												"value": "610"
											},
											{
												"key": "k_0611",
												"value": "611"
											},
											{
												"key": "k_0612",
												"value": "612"
											},
											{
												"key": "k_0613",
												"value": "613"
											},
											{
												"key": "k_0614",
												"value": "614"
											},
											{
												"key": "k_0615",
												"value": "615"
											},
											{
												"key": "k_0616",
												"value": "616"
											},
											{
												"key": "k_0617",
												"value": "617"
											},
											{
												"key": "k_0618",
												"value": "618"
											},
											{
												"key": "k_0619",
												"value": "619"
											},
											{
												"key": "k_0620",
												"value": "620"
											},
											{
												"key": "k_0621",
												"value": "621"
											},
											{
												"key": "k_0622",
												"value": "622"
											},
											{
												"key": "k_0623",
												"value": "623"
											},
											{
												"key": "k_0624",
												"value": "624"
											},
											{
												"key": "k_0625",
												"value": "625"
											},
											{
												"key": "k_0626",
												"value": "626"
											},
											{
												"key": "k_0627",
												"value": "627"
											},
											{
												"key": "k_0628",
												"value": "628"
											},
											{
												"key": "k_0629",
												"value": "629"
											},
											{
												"key": "k_0630",
												"value": "630"
											},
											{
												"key": "k_0631",
												"value": "631"
											},
											{
												"key": "k_0632",
												"value": "632"
											},
											{
												"key": "k_0633",
												"value": "633"
											},
											{
												"key": "k_0634",
												"value": "634"
											},
											{
												"key": "k_0635",
												"value": "635"
											},
											{
												"key": "k_0636",
												"value": "636"
											},
											{
												"key": "k_0637",
												"value": "637"
											},
											{
												"key": "k_0638",
												"value": "638"
											},
											{
												"key": "k_0639",
												"value": "639"
											},
											{
												"key": "k_0640",
												"value": "640"
											},
											{
												"key": "k_0641",
												"value": "641"
											},
											{
												"key": "k_0642",
												"value": "642"
											},
											{
												"key": "k_0643",
												"value": "643"
											},
											{
												"key": "k_0644",
												"value": "644"
											},
											{
												"key": "k_0645",
												"value": "645"
											},
											{
												"key": "k_0646",
												"value": "646"
											},
											{
												"key": "k_0647",
												"value": "647"
											},
											{
												"key": "k_0648",
												"value": "648"
											},
											{
												"key": "k_0649",
												"value": "649"
											},
											{
												"key": "k_0650",
												"value": "650"
											},
											{
												"key": "k_0651",
												"value": "651"
											},
											{
												"key": "k_0652",
												"value": "652"
											},
											{
												"key": "k_0653",
												"value": "653"
											},
											{
												"key": "k_0654",
												"value": "654"
											},
											{
												"key": "k_0655",
												"value": "655"
											},
											{
												"key": "k_0656",
												"value": "656"
											},
											{
												"key": "k_0657",
												"value": "657"
											},
											{
												"key": "k_0658",
												"value": "658"
											},
											{
												"key": "k_0659",
												"value": "659"
											},
											{
												"key": "k_0660",
												"value": "660"
											},
											{
												"key": "k_0661",
												"value": "661"
											},
											{
												"key": "k_0662",
												"value": "662"
											},
											{
												"key": "k_0663",
												"value": "663"
											},
											{
												"key": "k_0664",
												"value": "664"
											},
											{
												"key": "k_0665",
												"value": "665"
											},
											{
												"key": "k_0666",
												"value": "666"
											},
											{
												"key": "k_0667",
												"value": "667"
											},
											{
												"key": "k_0668",
												"value": "668"
											},
											{
												"key": "k_0669",
												"value": "669"
											},
											{
												"key": "k_0670",
												"value": "670"
											},
											{
												"key": "k_0671",
												"value": "671"
											},
											{
												"key": "k_0672",
												"value": "672"
											},
											{
												"key": "k_0673",
												"value": "673"
											},
											{
												"key": "k_0674",
												"value": "674"
											},
											{
												"key": "k_0675",
												"value": "675"
											},
											{
												"key": "k_0676",
												"value": "676"
											},
											{
												"key": "k_0677",
												"value": "677"
											},
											{
												"key": "k_0678",
												"value": "678"
											},
											{
												"key": "k_0679",
												"value": "679"
											},
											{
												"key": "k_0680",
												"value": "680"
											},
											{
												"key": "k_0681",
												"value": "681"
											},
											{
												"key": "k_0682",
												"value": "682"
											},
											{
												"key": "k_0683",
												"value": "683"
											},
											{
												"key": "k_0684",
												"value": "684"
											},
											{
												"key": "k_0685",
												"value": "685"
											},
											{
												"key": "k_0686",
												"value": "686"
											},
											{
												"key": "k_0687",
												"value": "687"
											},
											{
												"key": "k_0688",
												"value": "688"
											},
											{
												"key": "k_0689",
												"value": "689"
											},
											{
												"key": "k_0690",
												"value": "690"
											},
											{
												"key": "k_0691",
												"value": "691"
											},
											{
												"key": "k_0692",
												"value": "692"
											},
											{
												"key": "k_0693",
												"value": "693"
											},
											{
												"key": "k_0694",
												"value": "694"
											},
											{
												"key": "k_0695",
												"value": "695"
											},
											{
												"key": "k_0696",
												"value": "696"
											},
											{
												"key": "k_0697",
												"value": "697"
											},
											{
												"key": "k_0698",
												"value": "698"
											},
											{
												"key": "k_0699",
												"value": "699"
											},
											{
												"key": "k_0700",
												"value": "700"
											},
											{
												"key": "k_0701",
												"value": "701"
											},
											{
												"key": "k_0702",
												"value": "702"
											},
											{
												"key": "k_0703",
												"value": "703"
											},
											{
												"key": "k_0704",
												"value": "704"
											},
											{
												"key": "k_0705",
												"value": "705"
											},
											{
												"key": "k_0706",
												"value": "706"
											},
											{
												"key": "k_0707",
												"value": "707"
											},
											{
												"key": "k_0708",
												"value": "708"
											},
											{
												"key": "k_0709",
												"value": "709"
											},
											{
												"key": "k_0710",
												"value": "710"
											},
											{
												"key": "k_0711",
												"value": "711"
											},
											{
												"key": "k_0712",
												"value": "712"
											},
											{
												"key": "k_0713",
												"value": "713"
											},
											{
												"key": "k_0714",
												"value": "714"
											},
											{
												"key": "k_0715",
												"value": "715"
											},
											{
												"key": "k_0716",
												"value": "716"
											},
											{
												"key": "k_0717",
												"value": "717"
											},
											{
												"key": "k_0718",
												"value": "718"
											},
											{
												"key": "k_0719",
												"value": "719"
											},
											{
												"key": "k_0720",
												"value": "720"
											},
											{
												"key": "k_0721",
												"value": "721"
											},
											{
												"key": "k_0722",
												"value": "722"
											},
											{
												"key": "k_0723",
												"value": "723"
											},
											{
												"key": "k_0724",
												"value": "724"
											},
											{
												"key": "k_0725",
												"value": "725"
											},
											{
												"key": "k_0726",
												"value": "726"
											},
											{
												"key": "k_0727",
												"value": "727"
											},
											{
												"key": "k_0728",
												"value": "728"
											},
											{
												"key": "k_0729",
												"value": "729"
											},
											{
												"key": "k_0730",
												"value": "730"
											},
											{
												"key": "k_0731",
												"value": "731"
											},
											{
												"key": "k_0732",
												"value": "732"
											},
											{
												"key": "k_0733",
												"value": "733"
											},
											{
												"key": "k_0734",
												"value": "734"
											},
											{
												"key": "k_0735",
												"value": "735"
											},
											{
												"key": "k_0736",
												"value": "736"
											},
											{
												"key": "k_0737",
												"value": "737"
											},
											{
												"key": "k_0738",
												"value": "738"
											},
											{
												"key": "k_0739",
												"value": "739"
											},
											{
												"key": "k_0740",
												"value": "740"
											},
											{
												"key": "k_0741",
												"value": "741"
											},
											{
												"key": "k_0742",
												"value": "742"
											},
											{
												"key": "k_0743",
												"value": "743"
											},
											{
												"key": "k_0744",
												"value": "744"
											},
											{
												"key": "k_0745",
												"value": "745"
											},
											{
												"key": "k_0746",
												"value": "746"
											},
											{
												"key": "k_0747",
												"value": "747"
											},
											{
												"key": "k_0748",
												"value": "748"
											},
											{
												"key": "k_0749",
												"value": "749"
											},
											{
												"key": "k_0750",
												"value": "750"
											},
											{
												"key": "k_0751",
												"value": "751"
											},
											{
												"key": "k_0752",
												"value": "752"
											},
											{
												"key": "k_0753",
												"value": "753"
											},
											{
												"key": "k_0754",
												"value": "754"
											},
											{
												"key": "k_0755",
												"value": "755"
											},
											{
												"key": "k_0756",
												"value": "756"
											},
											{
												"key": "k_0757",
												"value": "757"
											},
											{
												"key": "k_0758",
												"value": "758"
											},
											{
												"key": "k_0759",
												"value": "759"
											},
											{
												"key": "k_0760",
												"value": "760"
											},
											{
												"key": "k_0761",
												"value": "761"
											},
											{
												"key": "k_0762",
												"value": "762"
											},
											{
												"key": "k_0763",
												"value": "763"
											},
											{
												"key": "k_0764",
												"value": "764"
											},
											{
												"key": "k_0765",
												"value": "765"
											},
											{
												"key": "k_0766",
												"value": "766"
											},
											{
												"key": "k_0767",
												"value": "767"
											},
											{
												"key": "k_0768",
												"value": "768"
											},
											{
												"key": "k_0769",
												"value": "769"
											},
											{
												"key": "k_0770",
												"value": "770"
											},
											{
												"key": "k_0771",
												"value": "771"
											},
											{
												"key": "k_0772",
												"value": "772"
											},
											{
												"key": "k_0773",
												"value": "773"
											},
											{
												"key": "k_0774",
												"value": "774"
											},
											{
												"key": "k_0775",
												"value": "775"
											},
											{
												"key": "k_0776",
												"value": "776"
											},
											{
												"key": "k_0777",
												"value": "777"
											},
											{
												"key": "k_0778",
												"value": "778"
											},
											{
												"key": "k_0779",
												"value": "779"
											},
											{
												"key": "k_0780",
												"value": "780"
											},
											{
												"key": "k_0781",
												"value": "781"
											},
											{
												"key": "k_0782",
												"value": "782"
											},
											{
												"key": "k_0783",
												"value": "783"
											},
											{
												"key": "k_0784",
												"value": "784"
											},
											{
												"key": "k_0785",
												"value": "785"
											},
											{
												"key": "k_0786",
												"value": "786"
											},
											{
												"key": "k_0787",
												"value": "787"
											},
											{
												"key": "k_0788",
												"value": "788"
											},
											{
												"key": "k_0789",
												"value": "789"
											},
											{
												"key": "k_0790",
												"value": "790"
											},
											{
												"key": "k_0791",
												"value": "791"
											},
											{
												"key": "k_0792",
												"value": "792"
											},
											{
												"key": "k_0793",
												"value": "793"
											},
											{
												"key": "k_0794",
												"value": "794"
											},
											{
												"key": "k_0795",
												"value": "795"
											},
											{
												"key": "k_0796",
												"value": "796"
											},
											{
												"key": "k_0797",
												"value": "797"
											},
											{
												"key": "k_0798",
												"value": "798"
											},
											{
												"key": "k_0799",
												"value": "799"
											},
											{
												"key": "k_0800",
												"value": "800"
											},
											{
												"key": "k_0801",
												"value": "801"
											},
											{
												"key": "k_0802",
												"value": "802"
											},
											{
												"key": "k_0803",
												"value": "803"
											},
											{
												"key": "k_0804",
												"value": "804"
											},
											{
												"key": "k_0805",
												"value": "805"
											},
											{
												"key": "k_0806",
												"value": "806"
											},
											{
												"key": "k_0807",
												"value": "807"
											},
											{
												"key": "k_0808",
												"value": "808"
											},
											{
												"key": "k_0809",
												"value": "809"
											},
											{
												"key": "k_0810",
												"value": "810"
											},
											{
												"key": "k_0811",
												"value": "811"
											},
											{
												"key": "k_0812",
												"value": "812"
											},
											{
												"key": "k_0813",
												"value": "813"
											},
											{
												"key": "k_0814",
												"value": "814"
											},
											{
												"key": "k_0815",
												"value": "815"
											},
											{
												"key": "k_0816",
												"value": "816"
											},
											{
												"key": "k_0817",
												"value": "817"
											},
											{
												"key": "k_0818",
												"value": "818"
											},
											{
												"key": "k_0819",
												"value": "819"
											},
											{
												"key": "k_0820",
												"value": "820"
											},
											{
												"key": "k_0821",
												"value": "821"
											},
											{
												"key": "k_0822",
												"value": "822"
											},
											{
												"key": "k_0823",
												"value": "823"
											},
											{
												"key": "k_0824",
												"value": "824"
											},
											{
												"key": "k_0825",
												"value": "825"
											},
											{
												"key": "k_0826",
												"value": "826"
											},
											{
												"key": "k_0827",
												"value": "827"
											},
											{
												"key": "k_0828",
												"value": "828"
											},
											{
												"key": "k_0829",
												"value": "829"
											},
											{
												"key": "k_0830",
												"value": "830"
											},
											{
												"key": "k_0831",
												"value": "831"
											},
											{
												"key": "k_0832",
												"value": "832"
											},
											{
												"key": "k_0833",
												"value": "833"
											},
											{
												"key": "k_0834",
												"value": "834"
											},
											{
												"key": "k_0835",
												"value": "835"
											},
											{
												"key": "k_0836",
												"value": "836"
											},
											{
												"key": "k_0837",
												"value": "837"
											},
											{
												"key": "k_0838",
												"value": "838"
											},
											{
												"key": "k_0839",
												"value": "839"
											},
											{
												"key": "k_0840",
												"value": "840"
											},
											{
												"key": "k_0841",
												"value": "841"
											},
											{
												"key": "k_0842",
												"value": "842"
											},
											{
												"key": "k_0843",
												"value": "843"
											},
											{
												"key": "k_0844",
												"value": "844"
											},
											{
												"key": "k_0845",
												"value": "845"
											},
											{
												"key": "k_0846",
												"value": "846"
											},
											{
												"key": "k_0847",
												"value": "847"
											},
											{
												"key": "k_0848",
												"value": "848"
											},
											{
												"key": "k_0849",
												"value": "849"
											},
											{
												"key": "k_0850",
												"value": "850"
											},
											{
												"key": "k_0851",
												"value": "851"
											},
											{
												"key": "k_0852",
												"value": "852"
											},
											{
												"key": "k_0853",
												"value": "853"
											},
											{
												"key": "k_0854",
												"value": "854"
											},
											{
												"key": "k_0855",
												"value": "855"
											},
											{
												"key": "k_0856",
												"value": "856"
											},
											{
												"key": "k_0857",
												"value": "857"
											},
											{
												"key": "k_0858",
												"value": "858"
											},
											{
												"key": "k_0859",
												"value": "859"
											},
											{
												"key": "k_0860",
												"value": "860"
											},
											{
												"key": "k_0861",
												"value": "861"
											},
											{
												"key": "k_0862",
												"value": "862"
											},
											{
												"key": "k_0863",
												"value": "863"
											},
											{
												"key": "k_0864",
												"value": "864"
											},
											{
												"key": "k_0865",
												"value": "865"
											},
											{
												"key": "k_0866",
												"value": "866"
											},
											{
												"key": "k_0867",
												"value": "867"
											},
											{
												"key": "k_0868",
												"value": "868"
											},
											{
												"key": "k_0869",
												"value": "869"
											},
											{
												"key": "k_0870",
												"value": "870"
											},
											{
												"key": "k_0871",
												"value": "871"
											},
											{
												"key": "k_0872",
												"value": "872"
											},
											{
												"key": "k_0873",
												"value": "873"
											},
											{
												"key": "k_0874",
												"value": "874"
											},
											{
												"key": "k_0875",
												"value": "875"
											},
											{
												"key": "k_0876",
												"value": "876"
											},
											{
												"key": "k_0877",
												"value": "877"
											},
											{
												"key": "k_0878",
												"value": "878"
											},
											{
												"key": "k_0879",
												"value": "879"
											},
											{
												"key": "k_0880",
												"value": "880"
											},
											{
												"key": "k_0881",
												"value": "881"
											},
											{
												"key": "k_0882",
												"value": "882"
											},
											{
												"key": "k_0883",
												"value": "883"
											},
											{
												"key": "k_0884",
												"value": "884"
											},
											{
												"key": "k_0885",
												"value": "885"
											},
											{
												"key": "k_0886",
												"value": "886"
											},
											{
												"key": "k_0887",
												"value": "887"
											},
											{
												"key": "k_0888",
												"value": "888"
											},
											{
												"key": "k_0889",
												"value": "889"
											},
											{
												"key": "k_0890",
												"value": "890"
											},
											{
												"key": "k_0891",
												"value": "891"
											},
											{
												"key": "k_0892",
												"value": "892"
											},
											{
												"key": "k_0893",
												"value": "893"
											},
											{
												"key": "k_0894",
												"value": "894"
											},
											{
												"key": "k_0895",
												"value": "895"
											},
											{
												"key": "k_0896",
												"value": "896"
											},
											{
												"key": "k_0897",
												"value": "897"
											},
											{
												"key": "k_0898",
												"value": "898"
											},
											{
												"key": "k_0899",
												"value": "899"
											},
											{
												"key": "k_0900",
												"value": "900"
											},
											{
												"key": "k_0901",
												"value": "901"
											},
											{
												"key": "k_0902",
												"value": "902"
											},
											{
												"key": "k_0903",
												"value": "903"
											},
											{
												"key": "k_0904",
												"value": "904"
											},
											{
												"key": "k_0905",
												"value": "905"
											},
											{
												"key": "k_0906",
												"value": "906"
											},
											{
												"key": "k_0907",
												"value": "907"
											},
											{
												"key": "k_0908",
												"value": "908"
											},
											{
												"key": "k_0909",
												"value": "909"
											},
											{
												"key": "k_0910",
												"value": "910"
											},
											{
												"key": "k_0911",
												"value": "911"
											},
											{
												"key": "k_0912",
												"value": "912"
											},
											{
												"key": "k_0913",
												"value": "913"
											},
											{
												"key": "k_0914",
												"value": "914"
											},
											{
												"key": "k_0915",
												"value": "915"
											},
											{
												"key": "k_0916",
												"value": "916"
											},
											{
												"key": "k_0917",
												"value": "917"
											},
											{
												"key": "k_0918",
												"value": "918"
											},
											{
												"key": "k_0919",
												"value": "919"
											},
											{
												"key": "k_0920",
												"value": "920"
											},
											{
												"key": "k_0921",
												"value": "921"
											},
											{
												"key": "k_0922",
												"value": "922"
											},
											{
												"key": "k_0923",
												"value": "923"
											},
											{
												"key": "k_0924",
												"value": "924"
											},
											{
												"key": "k_0925",
												"value": "925"
											},
											{
												"key": "k_0926",
												"value": "926"
											},
											{
												"key": "k_0927",
												"value": "927"
											},
											{
												"key": "k_0928",
												"value": "928"
											},
											{
												"key": "k_0929",
												"value": "929"
											},
											{
												"key": "k_0930",
												"value": "930"
											},
											{
												"key": "k_0931",
												"value": "931"
											},
											{
												"key": "k_0932",
												"value": "932"
											},
											{
												"key": "k_0933",
												"value": "933"
											},
											{
												"key": "k_0934",
												"value": "934"
											},
											{
												"key": "k_0935",
												"value": "935"
											},
											{
												"key": "k_0936",
												"value": "936"
											},
											{
												"key": "k_0937",
												"value": "937"
											},
											{
												"key": "k_0938",
												"value": "938"
											},
											{
												"key": "k_0939",
												"value": "939"
											},
											{
												"key": "k_0940",
												"value": "940"
											},
											{
												"key": "k_0941",
												"value": "941"
											},
											{
												"key": "k_0942",
												"value": "942"
											},
											{
												"key": "k_0943",
												"value": "943"
											},
											{
												"key": "k_0944",
												"value": "944"
											},
											{
												"key": "k_0945",
												"value": "945"
											},
											{
												"key": "k_0946",
												"value": "946"
											},
											{
												"key": "k_0947",
												"value": "947"
											},
											{
												"key": "k_0948",
												"value": "948"
											},
											{
												"key": "k_0949",
												"value": "949"
											},
											{
												"key": "k_0950",
												"value": "950"
											},
											{
												"key": "k_0951",
												"value": "951"
											},
											{
												"key": "k_0952",
												"value": "952"
											},
											{
												"key": "k_0953",
												"value": "953"
											},
											{
												"key": "k_0954",
												"value": "954"
											},
											{
												"key": "k_0955",
												"value": "955"
											},
											{
												"key": "k_0956",
												"value": "956"
											},
											{
												"key": "k_0957",
												"value": "957"
											},
											{
												"key": "k_0958",
												"value": "958"
											},
											{
												"key": "k_0959",
												"value": "959"
											},
											{
												"key": "k_0960",
												"value": "960"
											},
											{
												"key": "k_0961",
												"value": "961"
											},
											{
												"key": "k_0962",
												"value": "962"
											},
											{
												"key": "k_0963",
												"value": "963"
											},
											{
												"key": "k_0964",
												"value": "964"
											},
											{
												"key": "k_0965",
												"value": "965"
											},
											{
												"key": "k_0966",
												"value": "966"
											},
											{
												"key": "k_0967",
												"value": "967"
											},
											{
												"key": "k_0968",
												"value": "968"
											},
											{
												"key": "k_0969",
												"value": "969"
											},
											{
												"key": "k_0970",
												"value": "970"
											},
											{
												"key": "k_0971",
												"value": "971"
											},
											{
												"key": "k_0972",
												"value": "972"
											},
											{
												"key": "k_0973",
												"value": "973"
											},
											{
												"key": "k_0974",
												"value": "974"
											},
											{
												"key": "k_0975",
												"value": "975"
											},
											{
												"key": "k_0976",
												"value": "976"
											},
											{
												"key": "k_0977",
												"value": "977"
											},
											{
												"key": "k_0978",
												"value": "978"
											},
											{
												"key": "k_0979",
												"value": "979"
											},
											{
												"key": "k_0980",
												"value": "980"
											},
											{
												"key": "k_0981",
												"value": "981"
											},
											{
												"key": "k_0982",
												"value": "982"
											},
											{
												"key": "k_0983",
												"value": "983"
											},
											{
												"key": "k_0984",
												"value": "984"
											},
											{
												"key": "k_0985",
												"value": "985"
											},
											{
												"key": "k_0986",
												"value": "986"
											},
											{
												"key": "k_0987",
												"value": "987"
											},
											{
												"key": "k_0988",
												"value": "988"
											},
											{
												"key": "k_0989",
												"value": "989"
											},
											{
												"key": "k_0990",
												"value": "990"
											},
											{
												"key": "k_0991",
												"value": "991"
											},
											{
												"key": "k_0992",
												"value": "992"
											},
											{
												"key": "k_0993",
												"value": "993"
											},
											{
												"key": "k_0994",
												"value": "994"
											},
											{
												"key": "k_0995",
												"value": "995"
											},
											{
												"key": "k_0996",
												"value": "996"
											},
											{
												"key": "k_0997",
												"value": "997"
											},
											{
												"key": "k_0998",
												"value": "998"
											},
											{
												"key": "k_0999",
												"value": "999"
											},
											{
												"key": "k_1000",
												"value": "1000"
											},
											{
												"key": "k_1001",
												"value": "1001"
											}
										],
										"schema": {
											"key": "string",
											"value": "int"
										}
									},
									"isSummary": false,
									"language": "scala"
								},
								"persist_state": {
									"view": {
										"type": "details",
										"chartOptions": {
											"chartType": "bar",
											"aggregationType": "sum",
											"categoryFieldKeys": [
												"key"
											],
											"seriesFieldKeys": [
												"value"
											],
											"isStacked": false
										}
									}
								}
							},
							"cfdff1ed-bcd0-4ed2-8319-7a1af9f18551": {
								"type": "Synapse.DataFrame",
								"sync_state": {
									"table": {
										"rows": [
											{
												"name": "Michael",
												"age": "null"
											},
											{
												"name": "Andy",
												"age": "30"
											},
											{
												"name": "Justin",
												"age": "19"
											}
										],
										"schema": {
											"age": "int",
											"name": "string"
										}
									},
									"isSummary": false,
									"language": "scala"
								},
								"persist_state": {
									"view": {
										"type": "details",
										"chartOptions": {
											"chartType": "bar",
											"aggregationType": "sum",
											"categoryFieldKeys": [
												"name"
											],
											"seriesFieldKeys": [
												"age"
											],
											"isStacked": false
										}
									}
								}
							},
							"8c287a4d-a5ee-4113-95e9-3cdced25ffae": {
								"type": "Synapse.DataFrame",
								"sync_state": {
									"table": {
										"rows": [
											{
												"x": "3.95",
												"y": "3.98",
												"cut": "Ideal",
												"price": "326",
												"color": "E",
												"carat": "0.23",
												"depth": "61.5",
												"clarity": "SI2",
												"table": "55",
												"z": "2.43"
											},
											{
												"x": "3.89",
												"y": "3.84",
												"cut": "Premium",
												"price": "326",
												"color": "E",
												"carat": "0.21",
												"depth": "59.8",
												"clarity": "SI1",
												"table": "61",
												"z": "2.31"
											},
											{
												"x": "4.05",
												"y": "4.07",
												"cut": "Good",
												"price": "327",
												"color": "E",
												"carat": "0.23",
												"depth": "56.9",
												"clarity": "VS1",
												"table": "65",
												"z": "2.31"
											},
											{
												"x": "4.2",
												"y": "4.23",
												"cut": "Premium",
												"price": "334",
												"color": "I",
												"carat": "0.29",
												"depth": "62.4",
												"clarity": "VS2",
												"table": "58",
												"z": "2.63"
											},
											{
												"x": "4.34",
												"y": "4.35",
												"cut": "Good",
												"price": "335",
												"color": "J",
												"carat": "0.31",
												"depth": "63.3",
												"clarity": "SI2",
												"table": "58",
												"z": "2.75"
											},
											{
												"x": "3.94",
												"y": "3.96",
												"cut": "Very Good",
												"price": "336",
												"color": "J",
												"carat": "0.24",
												"depth": "62.8",
												"clarity": "VVS2",
												"table": "57",
												"z": "2.48"
											},
											{
												"x": "3.95",
												"y": "3.98",
												"cut": "Very Good",
												"price": "336",
												"color": "I",
												"carat": "0.24",
												"depth": "62.3",
												"clarity": "VVS1",
												"table": "57",
												"z": "2.47"
											},
											{
												"x": "4.07",
												"y": "4.11",
												"cut": "Very Good",
												"price": "337",
												"color": "H",
												"carat": "0.26",
												"depth": "61.9",
												"clarity": "SI1",
												"table": "55",
												"z": "2.53"
											},
											{
												"x": "3.87",
												"y": "3.78",
												"cut": "Fair",
												"price": "337",
												"color": "E",
												"carat": "0.22",
												"depth": "65.1",
												"clarity": "VS2",
												"table": "61",
												"z": "2.49"
											},
											{
												"x": "4",
												"y": "4.05",
												"cut": "Very Good",
												"price": "338",
												"color": "H",
												"carat": "0.23",
												"depth": "59.4",
												"clarity": "VS1",
												"table": "61",
												"z": "2.39"
											},
											{
												"x": "4.25",
												"y": "4.28",
												"cut": "Good",
												"price": "339",
												"color": "J",
												"carat": "0.3",
												"depth": "64",
												"clarity": "SI1",
												"table": "55",
												"z": "2.73"
											},
											{
												"x": "3.93",
												"y": "3.9",
												"cut": "Ideal",
												"price": "340",
												"color": "J",
												"carat": "0.23",
												"depth": "62.8",
												"clarity": "VS1",
												"table": "56",
												"z": "2.46"
											},
											{
												"x": "3.88",
												"y": "3.84",
												"cut": "Premium",
												"price": "342",
												"color": "F",
												"carat": "0.22",
												"depth": "60.4",
												"clarity": "SI1",
												"table": "61",
												"z": "2.33"
											},
											{
												"x": "4.35",
												"y": "4.37",
												"cut": "Ideal",
												"price": "344",
												"color": "J",
												"carat": "0.31",
												"depth": "62.2",
												"clarity": "SI2",
												"table": "54",
												"z": "2.71"
											},
											{
												"x": "3.79",
												"y": "3.75",
												"cut": "Premium",
												"price": "345",
												"color": "E",
												"carat": "0.2",
												"depth": "60.2",
												"clarity": "SI2",
												"table": "62",
												"z": "2.27"
											},
											{
												"x": "4.38",
												"y": "4.42",
												"cut": "Premium",
												"price": "345",
												"color": "E",
												"carat": "0.32",
												"depth": "60.9",
												"clarity": "I1",
												"table": "58",
												"z": "2.68"
											},
											{
												"x": "4.31",
												"y": "4.34",
												"cut": "Ideal",
												"price": "348",
												"color": "I",
												"carat": "0.3",
												"depth": "62",
												"clarity": "SI2",
												"table": "54",
												"z": "2.68"
											},
											{
												"x": "4.23",
												"y": "4.29",
												"cut": "Good",
												"price": "351",
												"color": "J",
												"carat": "0.3",
												"depth": "63.4",
												"clarity": "SI1",
												"table": "54",
												"z": "2.7"
											},
											{
												"x": "4.23",
												"y": "4.26",
												"cut": "Good",
												"price": "351",
												"color": "J",
												"carat": "0.3",
												"depth": "63.8",
												"clarity": "SI1",
												"table": "56",
												"z": "2.71"
											},
											{
												"x": "4.21",
												"y": "4.27",
												"cut": "Very Good",
												"price": "351",
												"color": "J",
												"carat": "0.3",
												"depth": "62.7",
												"clarity": "SI1",
												"table": "59",
												"z": "2.66"
											},
											{
												"x": "4.26",
												"y": "4.3",
												"cut": "Good",
												"price": "351",
												"color": "I",
												"carat": "0.3",
												"depth": "63.3",
												"clarity": "SI2",
												"table": "56",
												"z": "2.71"
											},
											{
												"x": "3.85",
												"y": "3.92",
												"cut": "Very Good",
												"price": "352",
												"color": "E",
												"carat": "0.23",
												"depth": "63.8",
												"clarity": "VS2",
												"table": "55",
												"z": "2.48"
											},
											{
												"x": "3.94",
												"y": "3.96",
												"cut": "Very Good",
												"price": "353",
												"color": "H",
												"carat": "0.23",
												"depth": "61",
												"clarity": "VS1",
												"table": "57",
												"z": "2.41"
											},
											{
												"x": "4.39",
												"y": "4.43",
												"cut": "Very Good",
												"price": "353",
												"color": "J",
												"carat": "0.31",
												"depth": "59.4",
												"clarity": "SI1",
												"table": "62",
												"z": "2.62"
											},
											{
												"x": "4.44",
												"y": "4.47",
												"cut": "Very Good",
												"price": "353",
												"color": "J",
												"carat": "0.31",
												"depth": "58.1",
												"clarity": "SI1",
												"table": "62",
												"z": "2.59"
											},
											{
												"x": "3.97",
												"y": "4.01",
												"cut": "Very Good",
												"price": "354",
												"color": "G",
												"carat": "0.23",
												"depth": "60.4",
												"clarity": "VVS2",
												"table": "58",
												"z": "2.41"
											},
											{
												"x": "3.97",
												"y": "3.94",
												"cut": "Premium",
												"price": "355",
												"color": "I",
												"carat": "0.24",
												"depth": "62.5",
												"clarity": "VS1",
												"table": "57",
												"z": "2.47"
											},
											{
												"x": "4.28",
												"y": "4.3",
												"cut": "Very Good",
												"price": "357",
												"color": "J",
												"carat": "0.3",
												"depth": "62.2",
												"clarity": "VS2",
												"table": "57",
												"z": "2.67"
											},
											{
												"x": "3.96",
												"y": "3.97",
												"cut": "Very Good",
												"price": "357",
												"color": "D",
												"carat": "0.23",
												"depth": "60.5",
												"clarity": "VS2",
												"table": "61",
												"z": "2.4"
											},
											{
												"x": "3.96",
												"y": "3.99",
												"cut": "Very Good",
												"price": "357",
												"color": "F",
												"carat": "0.23",
												"depth": "60.9",
												"clarity": "VS1",
												"table": "57",
												"z": "2.42"
											},
											{
												"x": "4",
												"y": "4.03",
												"cut": "Very Good",
												"price": "402",
												"color": "F",
												"carat": "0.23",
												"depth": "60",
												"clarity": "VS1",
												"table": "57",
												"z": "2.41"
											},
											{
												"x": "4.04",
												"y": "4.06",
												"cut": "Very Good",
												"price": "402",
												"color": "F",
												"carat": "0.23",
												"depth": "59.8",
												"clarity": "VS1",
												"table": "57",
												"z": "2.42"
											},
											{
												"x": "3.97",
												"y": "4.01",
												"cut": "Very Good",
												"price": "402",
												"color": "E",
												"carat": "0.23",
												"depth": "60.7",
												"clarity": "VS1",
												"table": "59",
												"z": "2.42"
											},
											{
												"x": "4.01",
												"y": "4.06",
												"cut": "Very Good",
												"price": "402",
												"color": "E",
												"carat": "0.23",
												"depth": "59.5",
												"clarity": "VS1",
												"table": "58",
												"z": "2.4"
											},
											{
												"x": "3.92",
												"y": "3.96",
												"cut": "Very Good",
												"price": "402",
												"color": "D",
												"carat": "0.23",
												"depth": "61.9",
												"clarity": "VS1",
												"table": "58",
												"z": "2.44"
											},
											{
												"x": "4.06",
												"y": "4.08",
												"cut": "Good",
												"price": "402",
												"color": "F",
												"carat": "0.23",
												"depth": "58.2",
												"clarity": "VS1",
												"table": "59",
												"z": "2.37"
											},
											{
												"x": "3.83",
												"y": "3.85",
												"cut": "Good",
												"price": "402",
												"color": "E",
												"carat": "0.23",
												"depth": "64.1",
												"clarity": "VS1",
												"table": "59",
												"z": "2.46"
											},
											{
												"x": "4.29",
												"y": "4.31",
												"cut": "Good",
												"price": "402",
												"color": "H",
												"carat": "0.31",
												"depth": "64",
												"clarity": "SI1",
												"table": "54",
												"z": "2.75"
											},
											{
												"x": "4.13",
												"y": "4.16",
												"cut": "Very Good",
												"price": "403",
												"color": "D",
												"carat": "0.26",
												"depth": "60.8",
												"clarity": "VS2",
												"table": "59",
												"z": "2.52"
											},
											{
												"x": "4.49",
												"y": "4.51",
												"cut": "Ideal",
												"price": "403",
												"color": "I",
												"carat": "0.33",
												"depth": "61.8",
												"clarity": "SI2",
												"table": "55",
												"z": "2.78"
											},
											{
												"x": "4.49",
												"y": "4.5",
												"cut": "Ideal",
												"price": "403",
												"color": "I",
												"carat": "0.33",
												"depth": "61.2",
												"clarity": "SI2",
												"table": "56",
												"z": "2.75"
											},
											{
												"x": "4.49",
												"y": "4.55",
												"cut": "Ideal",
												"price": "403",
												"color": "J",
												"carat": "0.33",
												"depth": "61.1",
												"clarity": "SI1",
												"table": "56",
												"z": "2.76"
											},
											{
												"x": "3.99",
												"y": "4.02",
												"cut": "Good",
												"price": "403",
												"color": "D",
												"carat": "0.26",
												"depth": "65.2",
												"clarity": "VS2",
												"table": "56",
												"z": "2.61"
											},
											{
												"x": "4.19",
												"y": "4.24",
												"cut": "Good",
												"price": "403",
												"color": "D",
												"carat": "0.26",
												"depth": "58.4",
												"clarity": "VS1",
												"table": "63",
												"z": "2.46"
											},
											{
												"x": "4.34",
												"y": "4.37",
												"cut": "Good",
												"price": "403",
												"color": "H",
												"carat": "0.32",
												"depth": "63.1",
												"clarity": "SI2",
												"table": "56",
												"z": "2.75"
											},
											{
												"x": "4.24",
												"y": "4.26",
												"cut": "Premium",
												"price": "403",
												"color": "F",
												"carat": "0.29",
												"depth": "62.4",
												"clarity": "SI1",
												"table": "58",
												"z": "2.65"
											},
											{
												"x": "4.35",
												"y": "4.42",
												"cut": "Very Good",
												"price": "403",
												"color": "H",
												"carat": "0.32",
												"depth": "61.8",
												"clarity": "SI2",
												"table": "55",
												"z": "2.71"
											},
											{
												"x": "4.36",
												"y": "4.38",
												"cut": "Good",
												"price": "403",
												"color": "H",
												"carat": "0.32",
												"depth": "63.8",
												"clarity": "SI2",
												"table": "56",
												"z": "2.79"
											},
											{
												"x": "4",
												"y": "4.03",
												"cut": "Very Good",
												"price": "404",
												"color": "E",
												"carat": "0.25",
												"depth": "63.3",
												"clarity": "VS2",
												"table": "60",
												"z": "2.54"
											},
											{
												"x": "4.33",
												"y": "4.37",
												"cut": "Very Good",
												"price": "404",
												"color": "H",
												"carat": "0.29",
												"depth": "60.7",
												"clarity": "SI2",
												"table": "60",
												"z": "2.64"
											},
											{
												"x": "4.02",
												"y": "4.03",
												"cut": "Very Good",
												"price": "404",
												"color": "F",
												"carat": "0.24",
												"depth": "60.9",
												"clarity": "SI1",
												"table": "61",
												"z": "2.45"
											},
											{
												"x": "3.93",
												"y": "3.95",
												"cut": "Ideal",
												"price": "404",
												"color": "G",
												"carat": "0.23",
												"depth": "61.9",
												"clarity": "VS1",
												"table": "54",
												"z": "2.44"
											},
											{
												"x": "4.45",
												"y": "4.48",
												"cut": "Ideal",
												"price": "404",
												"color": "I",
												"carat": "0.32",
												"depth": "60.9",
												"clarity": "SI1",
												"table": "55",
												"z": "2.72"
											},
											{
												"x": "3.93",
												"y": "3.89",
												"cut": "Premium",
												"price": "404",
												"color": "E",
												"carat": "0.22",
												"depth": "61.6",
												"clarity": "VS2",
												"table": "58",
												"z": "2.41"
											},
											{
												"x": "3.91",
												"y": "3.88",
												"cut": "Premium",
												"price": "404",
												"color": "D",
												"carat": "0.22",
												"depth": "59.3",
												"clarity": "VS2",
												"table": "62",
												"z": "2.31"
											},
											{
												"x": "4.3",
												"y": "4.33",
												"cut": "Ideal",
												"price": "405",
												"color": "I",
												"carat": "0.3",
												"depth": "61",
												"clarity": "SI2",
												"table": "59",
												"z": "2.63"
											},
											{
												"x": "4.43",
												"y": "4.38",
												"cut": "Premium",
												"price": "405",
												"color": "J",
												"carat": "0.3",
												"depth": "59.3",
												"clarity": "SI2",
												"table": "61",
												"z": "2.61"
											},
											{
												"x": "4.25",
												"y": "4.28",
												"cut": "Very Good",
												"price": "405",
												"color": "I",
												"carat": "0.3",
												"depth": "62.6",
												"clarity": "SI1",
												"table": "57",
												"z": "2.67"
											},
											{
												"x": "4.28",
												"y": "4.32",
												"cut": "Very Good",
												"price": "405",
												"color": "I",
												"carat": "0.3",
												"depth": "63",
												"clarity": "SI1",
												"table": "57",
												"z": "2.71"
											},
											{
												"x": "4.25",
												"y": "4.29",
												"cut": "Good",
												"price": "405",
												"color": "I",
												"carat": "0.3",
												"depth": "63.2",
												"clarity": "SI1",
												"table": "55",
												"z": "2.7"
											},
											{
												"x": "4.54",
												"y": "4.59",
												"cut": "Ideal",
												"price": "552",
												"color": "I",
												"carat": "0.35",
												"depth": "60.9",
												"clarity": "VS1",
												"table": "57",
												"z": "2.78"
											},
											{
												"x": "4.23",
												"y": "4.27",
												"cut": "Premium",
												"price": "552",
												"color": "D",
												"carat": "0.3",
												"depth": "62.6",
												"clarity": "SI1",
												"table": "59",
												"z": "2.66"
											},
											{
												"x": "4.29",
												"y": "4.32",
												"cut": "Ideal",
												"price": "552",
												"color": "D",
												"carat": "0.3",
												"depth": "62.5",
												"clarity": "SI1",
												"table": "57",
												"z": "2.69"
											},
											{
												"x": "4.3",
												"y": "4.33",
												"cut": "Ideal",
												"price": "552",
												"color": "D",
												"carat": "0.3",
												"depth": "62.1",
												"clarity": "SI1",
												"table": "56",
												"z": "2.68"
											},
											{
												"x": "4.78",
												"y": "4.84",
												"cut": "Premium",
												"price": "552",
												"color": "I",
												"carat": "0.42",
												"depth": "61.5",
												"clarity": "SI2",
												"table": "59",
												"z": "2.96"
											},
											{
												"x": "4.19",
												"y": "4.22",
												"cut": "Ideal",
												"price": "553",
												"color": "G",
												"carat": "0.28",
												"depth": "61.4",
												"clarity": "VVS2",
												"table": "56",
												"z": "2.58"
											},
											{
												"x": "4.39",
												"y": "4.42",
												"cut": "Ideal",
												"price": "553",
												"color": "I",
												"carat": "0.32",
												"depth": "62",
												"clarity": "VVS1",
												"table": "55.3",
												"z": "2.73"
											},
											{
												"x": "4.33",
												"y": "4.3",
												"cut": "Very Good",
												"price": "553",
												"color": "G",
												"carat": "0.31",
												"depth": "63.3",
												"clarity": "SI1",
												"table": "57",
												"z": "2.73"
											},
											{
												"x": "4.35",
												"y": "4.32",
												"cut": "Premium",
												"price": "553",
												"color": "G",
												"carat": "0.31",
												"depth": "61.8",
												"clarity": "SI1",
												"table": "58",
												"z": "2.68"
											},
											{
												"x": "4.01",
												"y": "4.03",
												"cut": "Premium",
												"price": "553",
												"color": "E",
												"carat": "0.24",
												"depth": "60.7",
												"clarity": "VVS1",
												"table": "58",
												"z": "2.44"
											},
											{
												"x": "3.97",
												"y": "4",
												"cut": "Very Good",
												"price": "553",
												"color": "D",
												"carat": "0.24",
												"depth": "61.5",
												"clarity": "VVS1",
												"table": "60",
												"z": "2.45"
											},
											{
												"x": "4.29",
												"y": "4.27",
												"cut": "Very Good",
												"price": "554",
												"color": "H",
												"carat": "0.3",
												"depth": "63.1",
												"clarity": "SI1",
												"table": "56",
												"z": "2.7"
											},
											{
												"x": "4.28",
												"y": "4.24",
												"cut": "Premium",
												"price": "554",
												"color": "H",
												"carat": "0.3",
												"depth": "62.9",
												"clarity": "SI1",
												"table": "59",
												"z": "2.68"
											},
											{
												"x": "4.29",
												"y": "4.25",
												"cut": "Premium",
												"price": "554",
												"color": "H",
												"carat": "0.3",
												"depth": "62.5",
												"clarity": "SI1",
												"table": "57",
												"z": "2.67"
											},
											{
												"x": "4.28",
												"y": "4.26",
												"cut": "Good",
												"price": "554",
												"color": "H",
												"carat": "0.3",
												"depth": "63.7",
												"clarity": "SI1",
												"table": "57",
												"z": "2.72"
											},
											{
												"x": "4.19",
												"y": "4.22",
												"cut": "Very Good",
												"price": "554",
												"color": "F",
												"carat": "0.26",
												"depth": "59.2",
												"clarity": "VVS2",
												"table": "60",
												"z": "2.49"
											},
											{
												"x": "4.15",
												"y": "4.23",
												"cut": "Very Good",
												"price": "554",
												"color": "E",
												"carat": "0.26",
												"depth": "59.9",
												"clarity": "VVS2",
												"table": "58",
												"z": "2.51"
											},
											{
												"x": "4.08",
												"y": "4.13",
												"cut": "Very Good",
												"price": "554",
												"color": "D",
												"carat": "0.26",
												"depth": "62.4",
												"clarity": "VVS2",
												"table": "54",
												"z": "2.56"
											},
											{
												"x": "4.01",
												"y": "4.05",
												"cut": "Very Good",
												"price": "554",
												"color": "D",
												"carat": "0.26",
												"depth": "62.8",
												"clarity": "VVS2",
												"table": "60",
												"z": "2.53"
											},
											{
												"x": "4.06",
												"y": "4.09",
												"cut": "Very Good",
												"price": "554",
												"color": "E",
												"carat": "0.26",
												"depth": "62.6",
												"clarity": "VVS1",
												"table": "59",
												"z": "2.55"
											},
											{
												"x": "4",
												"y": "4.04",
												"cut": "Very Good",
												"price": "554",
												"color": "E",
												"carat": "0.26",
												"depth": "63.4",
												"clarity": "VVS1",
												"table": "59",
												"z": "2.55"
											},
											{
												"x": "4.03",
												"y": "4.12",
												"cut": "Very Good",
												"price": "554",
												"color": "D",
												"carat": "0.26",
												"depth": "62.1",
												"clarity": "VVS1",
												"table": "60",
												"z": "2.53"
											},
											{
												"x": "4.02",
												"y": "4.06",
												"cut": "Ideal",
												"price": "554",
												"color": "E",
												"carat": "0.26",
												"depth": "62.9",
												"clarity": "VVS2",
												"table": "58",
												"z": "2.54"
											},
											{
												"x": "4.65",
												"y": "4.67",
												"cut": "Ideal",
												"price": "554",
												"color": "I",
												"carat": "0.38",
												"depth": "61.6",
												"clarity": "SI2",
												"table": "56",
												"z": "2.87"
											},
											{
												"x": "4.22",
												"y": "4.25",
												"cut": "Good",
												"price": "554",
												"color": "E",
												"carat": "0.26",
												"depth": "57.9",
												"clarity": "VVS1",
												"table": "60",
												"z": "2.45"
											},
											{
												"x": "3.95",
												"y": "3.92",
												"cut": "Premium",
												"price": "554",
												"color": "G",
												"carat": "0.24",
												"depth": "62.3",
												"clarity": "VVS1",
												"table": "59",
												"z": "2.45"
											},
											{
												"x": "4.01",
												"y": "3.96",
												"cut": "Premium",
												"price": "554",
												"color": "H",
												"carat": "0.24",
												"depth": "61.2",
												"clarity": "VVS1",
												"table": "58",
												"z": "2.44"
											},
											{
												"x": "4.02",
												"y": "4",
												"cut": "Premium",
												"price": "554",
												"color": "H",
												"carat": "0.24",
												"depth": "60.8",
												"clarity": "VVS1",
												"table": "59",
												"z": "2.44"
											},
											{
												"x": "4.07",
												"y": "4.04",
												"cut": "Premium",
												"price": "554",
												"color": "H",
												"carat": "0.24",
												"depth": "60.7",
												"clarity": "VVS2",
												"table": "58",
												"z": "2.46"
											},
											{
												"x": "4.35",
												"y": "4.33",
												"cut": "Premium",
												"price": "554",
												"color": "I",
												"carat": "0.32",
												"depth": "62.9",
												"clarity": "SI1",
												"table": "58",
												"z": "2.73"
											},
											{
												"x": "5.7",
												"y": "5.72",
												"cut": "Ideal",
												"price": "2757",
												"color": "E",
												"carat": "0.7",
												"depth": "62.5",
												"clarity": "SI1",
												"table": "57",
												"z": "3.57"
											},
											{
												"x": "6.45",
												"y": "6.33",
												"cut": "Fair",
												"price": "2757",
												"color": "E",
												"carat": "0.86",
												"depth": "55.1",
												"clarity": "SI2",
												"table": "69",
												"z": "3.52"
											},
											{
												"x": "5.7",
												"y": "5.67",
												"cut": "Ideal",
												"price": "2757",
												"color": "G",
												"carat": "0.7",
												"depth": "61.6",
												"clarity": "VS2",
												"table": "56",
												"z": "3.5"
											},
											{
												"x": "5.68",
												"y": "5.73",
												"cut": "Very Good",
												"price": "2759",
												"color": "E",
												"carat": "0.71",
												"depth": "62.4",
												"clarity": "VS2",
												"table": "57",
												"z": "3.56"
											},
											{
												"x": "5.81",
												"y": "5.85",
												"cut": "Very Good",
												"price": "2759",
												"color": "G",
												"carat": "0.78",
												"depth": "63.8",
												"clarity": "SI2",
												"table": "56",
												"z": "3.72"
											},
											{
												"x": "5.85",
												"y": "5.9",
												"cut": "Good",
												"price": "2759",
												"color": "E",
												"carat": "0.7",
												"depth": "57.5",
												"clarity": "VS2",
												"table": "58",
												"z": "3.38"
											},
											{
												"x": "5.71",
												"y": "5.76",
												"cut": "Good",
												"price": "2759",
												"color": "F",
												"carat": "0.7",
												"depth": "59.4",
												"clarity": "VS1",
												"table": "62",
												"z": "3.4"
											},
											{
												"x": "6.27",
												"y": "5.95",
												"cut": "Fair",
												"price": "2759",
												"color": "F",
												"carat": "0.96",
												"depth": "66.3",
												"clarity": "SI2",
												"table": "62",
												"z": "4.07"
											},
											{
												"x": "5.77",
												"y": "5.78",
												"cut": "Very Good",
												"price": "2760",
												"color": "E",
												"carat": "0.73",
												"depth": "61.6",
												"clarity": "SI1",
												"table": "59",
												"z": "3.56"
											},
											{
												"x": "5.97",
												"y": "5.93",
												"cut": "Premium",
												"price": "2760",
												"color": "H",
												"carat": "0.8",
												"depth": "61.5",
												"clarity": "SI1",
												"table": "58",
												"z": "3.66"
											},
											{
												"x": "5.8",
												"y": "5.75",
												"cut": "Very Good",
												"price": "2760",
												"color": "D",
												"carat": "0.75",
												"depth": "63.2",
												"clarity": "SI1",
												"table": "56",
												"z": "3.65"
											},
											{
												"x": "6",
												"y": "5.96",
												"cut": "Premium",
												"price": "2760",
												"color": "E",
												"carat": "0.75",
												"depth": "59.9",
												"clarity": "SI1",
												"table": "54",
												"z": "3.58"
											},
											{
												"x": "5.8",
												"y": "5.85",
												"cut": "Ideal",
												"price": "2760",
												"color": "G",
												"carat": "0.74",
												"depth": "61.6",
												"clarity": "SI1",
												"table": "55",
												"z": "3.59"
											},
											{
												"x": "5.85",
												"y": "5.79",
												"cut": "Premium",
												"price": "2760",
												"color": "G",
												"carat": "0.75",
												"depth": "61.7",
												"clarity": "VS2",
												"table": "58",
												"z": "3.59"
											},
											{
												"x": "5.94",
												"y": "5.87",
												"cut": "Ideal",
												"price": "2760",
												"color": "I",
												"carat": "0.8",
												"depth": "62.9",
												"clarity": "VS1",
												"table": "56",
												"z": "3.72"
											},
											{
												"x": "5.87",
												"y": "5.8",
												"cut": "Ideal",
												"price": "2760",
												"color": "G",
												"carat": "0.75",
												"depth": "62.2",
												"clarity": "SI1",
												"table": "55",
												"z": "3.63"
											},
											{
												"x": "5.9",
												"y": "5.81",
												"cut": "Premium",
												"price": "2760",
												"color": "G",
												"carat": "0.8",
												"depth": "63",
												"clarity": "SI1",
												"table": "59",
												"z": "3.69"
											},
											{
												"x": "5.77",
												"y": "5.81",
												"cut": "Ideal",
												"price": "2761",
												"color": "I",
												"carat": "0.74",
												"depth": "62.3",
												"clarity": "VVS2",
												"table": "55",
												"z": "3.61"
											},
											{
												"x": "6.14",
												"y": "6.11",
												"cut": "Ideal",
												"price": "2761",
												"color": "F",
												"carat": "0.81",
												"depth": "58.8",
												"clarity": "SI2",
												"table": "57",
												"z": "3.6"
											},
											{
												"x": "5.38",
												"y": "5.43",
												"cut": "Ideal",
												"price": "2761",
												"color": "E",
												"carat": "0.59",
												"depth": "62",
												"clarity": "VVS2",
												"table": "55",
												"z": "3.35"
											},
											{
												"x": "5.96",
												"y": "6",
												"cut": "Ideal",
												"price": "2761",
												"color": "F",
												"carat": "0.8",
												"depth": "61.4",
												"clarity": "SI2",
												"table": "57",
												"z": "3.67"
											},
											{
												"x": "5.8",
												"y": "5.84",
												"cut": "Ideal",
												"price": "2761",
												"color": "E",
												"carat": "0.74",
												"depth": "62.2",
												"clarity": "SI2",
												"table": "56",
												"z": "3.62"
											},
											{
												"x": "6.16",
												"y": "6.12",
												"cut": "Premium",
												"price": "2761",
												"color": "I",
												"carat": "0.9",
												"depth": "63",
												"clarity": "VS2",
												"table": "58",
												"z": "3.87"
											},
											{
												"x": "5.73",
												"y": "5.82",
												"cut": "Very Good",
												"price": "2762",
												"color": "G",
												"carat": "0.74",
												"depth": "62.2",
												"clarity": "SI1",
												"table": "59",
												"z": "3.59"
											},
											{
												"x": "5.77",
												"y": "5.74",
												"cut": "Ideal",
												"price": "2762",
												"color": "F",
												"carat": "0.73",
												"depth": "62.6",
												"clarity": "VS2",
												"table": "56",
												"z": "3.6"
											},
											{
												"x": "5.8",
												"y": "5.75",
												"cut": "Ideal",
												"price": "2762",
												"color": "F",
												"carat": "0.73",
												"depth": "62.7",
												"clarity": "VS2",
												"table": "53",
												"z": "3.62"
											},
											{
												"x": "5.98",
												"y": "5.94",
												"cut": "Premium",
												"price": "2762",
												"color": "F",
												"carat": "0.8",
												"depth": "61.7",
												"clarity": "SI2",
												"table": "58",
												"z": "3.68"
											},
											{
												"x": "5.72",
												"y": "5.76",
												"cut": "Ideal",
												"price": "2762",
												"color": "G",
												"carat": "0.71",
												"depth": "62.4",
												"clarity": "VS2",
												"table": "54",
												"z": "3.58"
											},
											{
												"x": "5.73",
												"y": "5.76",
												"cut": "Ideal",
												"price": "2762",
												"color": "E",
												"carat": "0.7",
												"depth": "60.7",
												"clarity": "VS2",
												"table": "58",
												"z": "3.49"
											},
											{
												"x": "6.01",
												"y": "6.07",
												"cut": "Ideal",
												"price": "2762",
												"color": "F",
												"carat": "0.8",
												"depth": "59.9",
												"clarity": "SI2",
												"table": "59",
												"z": "3.62"
											},
											{
												"x": "5.73",
												"y": "5.69",
												"cut": "Ideal",
												"price": "2762",
												"color": "D",
												"carat": "0.71",
												"depth": "62.3",
												"clarity": "SI2",
												"table": "56",
												"z": "3.56"
											},
											{
												"x": "5.8",
												"y": "5.83",
												"cut": "Ideal",
												"price": "2762",
												"color": "E",
												"carat": "0.74",
												"depth": "62.3",
												"clarity": "SI1",
												"table": "54",
												"z": "3.62"
											},
											{
												"x": "5.64",
												"y": "5.61",
												"cut": "Very Good",
												"price": "2762",
												"color": "F",
												"carat": "0.7",
												"depth": "61.7",
												"clarity": "VS2",
												"table": "63",
												"z": "3.47"
											},
											{
												"x": "5.57",
												"y": "5.53",
												"cut": "Fair",
												"price": "2762",
												"color": "F",
												"carat": "0.7",
												"depth": "64.5",
												"clarity": "VS2",
												"table": "57",
												"z": "3.58"
											},
											{
												"x": "5.63",
												"y": "5.58",
												"cut": "Fair",
												"price": "2762",
												"color": "F",
												"carat": "0.7",
												"depth": "65.3",
												"clarity": "VS2",
												"table": "55",
												"z": "3.66"
											},
											{
												"x": "5.65",
												"y": "5.59",
												"cut": "Premium",
												"price": "2762",
												"color": "F",
												"carat": "0.7",
												"depth": "61.6",
												"clarity": "VS2",
												"table": "60",
												"z": "3.46"
											},
											{
												"x": "6.09",
												"y": "5.97",
												"cut": "Premium",
												"price": "2763",
												"color": "H",
												"carat": "0.91",
												"depth": "61.4",
												"clarity": "SI1",
												"table": "56",
												"z": "3.7"
											},
											{
												"x": "5.56",
												"y": "5.58",
												"cut": "Very Good",
												"price": "2763",
												"color": "D",
												"carat": "0.61",
												"depth": "59.6",
												"clarity": "VVS2",
												"table": "57",
												"z": "3.32"
											},
											{
												"x": "6.11",
												"y": "6.09",
												"cut": "Fair",
												"price": "2763",
												"color": "H",
												"carat": "0.91",
												"depth": "64.4",
												"clarity": "SI2",
												"table": "57",
												"z": "3.93"
											},
											{
												"x": "6.03",
												"y": "5.99",
												"cut": "Fair",
												"price": "2763",
												"color": "H",
												"carat": "0.91",
												"depth": "65.7",
												"clarity": "SI2",
												"table": "60",
												"z": "3.95"
											},
											{
												"x": "5.89",
												"y": "5.86",
												"cut": "Ideal",
												"price": "2763",
												"color": "H",
												"carat": "0.77",
												"depth": "62",
												"clarity": "VS2",
												"table": "56",
												"z": "3.64"
											},
											{
												"x": "5.64",
												"y": "5.68",
												"cut": "Very Good",
												"price": "2764",
												"color": "D",
												"carat": "0.71",
												"depth": "63.6",
												"clarity": "SI1",
												"table": "58",
												"z": "3.6"
											},
											{
												"x": "5.69",
												"y": "5.72",
												"cut": "Ideal",
												"price": "2764",
												"color": "D",
												"carat": "0.71",
												"depth": "61.9",
												"clarity": "SI1",
												"table": "59",
												"z": "3.53"
											},
											{
												"x": "5.62",
												"y": "5.65",
												"cut": "Very Good",
												"price": "2765",
												"color": "E",
												"carat": "0.7",
												"depth": "62.6",
												"clarity": "VS2",
												"table": "60",
												"z": "3.53"
											},
											{
												"x": "5.88",
												"y": "5.9",
												"cut": "Very Good",
												"price": "2765",
												"color": "H",
												"carat": "0.77",
												"depth": "61.3",
												"clarity": "VS1",
												"table": "60",
												"z": "3.61"
											},
											{
												"x": "5.52",
												"y": "5.55",
												"cut": "Premium",
												"price": "2765",
												"color": "E",
												"carat": "0.63",
												"depth": "60.9",
												"clarity": "VVS1",
												"table": "60",
												"z": "3.37"
											},
											{
												"x": "5.74",
												"y": "5.77",
												"cut": "Very Good",
												"price": "2765",
												"color": "F",
												"carat": "0.71",
												"depth": "60.1",
												"clarity": "VS1",
												"table": "62",
												"z": "3.46"
											},
											{
												"x": "5.69",
												"y": "5.73",
												"cut": "Premium",
												"price": "2765",
												"color": "F",
												"carat": "0.71",
												"depth": "61.8",
												"clarity": "VS1",
												"table": "59",
												"z": "3.53"
											},
											{
												"x": "5.88",
												"y": "5.91",
												"cut": "Ideal",
												"price": "2765",
												"color": "H",
												"carat": "0.76",
												"depth": "61.2",
												"clarity": "SI1",
												"table": "57",
												"z": "3.61"
											},
											{
												"x": "5.53",
												"y": "5.56",
												"cut": "Ideal",
												"price": "2766",
												"color": "G",
												"carat": "0.64",
												"depth": "61.9",
												"clarity": "VVS1",
												"table": "56",
												"z": "3.43"
											},
											{
												"x": "5.78",
												"y": "5.75",
												"cut": "Premium",
												"price": "2766",
												"color": "G",
												"carat": "0.71",
												"depth": "60.9",
												"clarity": "VS2",
												"table": "57",
												"z": "3.51"
											},
											{
												"x": "5.89",
												"y": "5.81",
												"cut": "Premium",
												"price": "2766",
												"color": "G",
												"carat": "0.71",
												"depth": "59.8",
												"clarity": "VS2",
												"table": "56",
												"z": "3.5"
											},
											{
												"x": "5.68",
												"y": "5.72",
												"cut": "Very Good",
												"price": "2767",
												"color": "D",
												"carat": "0.7",
												"depth": "61.8",
												"clarity": "VS2",
												"table": "55",
												"z": "3.52"
											},
											{
												"x": "5.8",
												"y": "5.87",
												"cut": "Very Good",
												"price": "2767",
												"color": "F",
												"carat": "0.7",
												"depth": "60",
												"clarity": "VS1",
												"table": "57",
												"z": "3.5"
											},
											{
												"x": "5.74",
												"y": "5.76",
												"cut": "Ideal",
												"price": "2767",
												"color": "D",
												"carat": "0.71",
												"depth": "61.6",
												"clarity": "SI2",
												"table": "55",
												"z": "3.54"
											},
											{
												"x": "5.62",
												"y": "5.65",
												"cut": "Good",
												"price": "2767",
												"color": "H",
												"carat": "0.7",
												"depth": "62.1",
												"clarity": "VVS2",
												"table": "64",
												"z": "3.5"
											},
											{
												"x": "5.52",
												"y": "5.61",
												"cut": "Very Good",
												"price": "2768",
												"color": "G",
												"carat": "0.71",
												"depth": "63.3",
												"clarity": "VS1",
												"table": "59",
												"z": "3.52"
											},
											{
												"x": "5.83",
												"y": "5.87",
												"cut": "Very Good",
												"price": "2768",
												"color": "D",
												"carat": "0.73",
												"depth": "60.2",
												"clarity": "SI1",
												"table": "56",
												"z": "3.52"
											},
											{
												"x": "5.66",
												"y": "5.73",
												"cut": "Very Good",
												"price": "2768",
												"color": "D",
												"carat": "0.7",
												"depth": "61.1",
												"clarity": "SI1",
												"table": "58",
												"z": "3.48"
											},
											{
												"x": "5.73",
												"y": "5.76",
												"cut": "Ideal",
												"price": "2768",
												"color": "E",
												"carat": "0.7",
												"depth": "60.9",
												"clarity": "SI1",
												"table": "57",
												"z": "3.5"
											},
											{
												"x": "5.71",
												"y": "5.67",
												"cut": "Premium",
												"price": "2768",
												"color": "D",
												"carat": "0.71",
												"depth": "61.7",
												"clarity": "SI2",
												"table": "59",
												"z": "3.51"
											},
											{
												"x": "5.82",
												"y": "5.86",
												"cut": "Ideal",
												"price": "2769",
												"color": "I",
												"carat": "0.74",
												"depth": "61.3",
												"clarity": "SI1",
												"table": "56",
												"z": "3.57"
											},
											{
												"x": "5.65",
												"y": "5.61",
												"cut": "Premium",
												"price": "2770",
												"color": "D",
												"carat": "0.71",
												"depth": "62.5",
												"clarity": "VS2",
												"table": "60",
												"z": "3.52"
											},
											{
												"x": "5.83",
												"y": "5.76",
												"cut": "Premium",
												"price": "2770",
												"color": "G",
												"carat": "0.73",
												"depth": "61.4",
												"clarity": "VS2",
												"table": "59",
												"z": "3.56"
											},
											{
												"x": "5.79",
												"y": "5.81",
												"cut": "Very Good",
												"price": "2770",
												"color": "F",
												"carat": "0.76",
												"depth": "62.9",
												"clarity": "SI1",
												"table": "57",
												"z": "3.65"
											},
											{
												"x": "5.78",
												"y": "5.83",
												"cut": "Ideal",
												"price": "2770",
												"color": "D",
												"carat": "0.76",
												"depth": "62.4",
												"clarity": "SI2",
												"table": "57",
												"z": "3.62"
											},
											{
												"x": "5.77",
												"y": "5.8",
												"cut": "Ideal",
												"price": "2770",
												"color": "F",
												"carat": "0.71",
												"depth": "60.7",
												"clarity": "SI1",
												"table": "56",
												"z": "3.51"
											},
											{
												"x": "5.87",
												"y": "5.82",
												"cut": "Premium",
												"price": "2770",
												"color": "G",
												"carat": "0.73",
												"depth": "60.7",
												"clarity": "VS2",
												"table": "58",
												"z": "3.55"
											},
											{
												"x": "5.79",
												"y": "5.75",
												"cut": "Premium",
												"price": "2770",
												"color": "G",
												"carat": "0.73",
												"depth": "61.5",
												"clarity": "VS1",
												"table": "58",
												"z": "3.55"
											},
											{
												"x": "5.92",
												"y": "5.89",
												"cut": "Ideal",
												"price": "2770",
												"color": "D",
												"carat": "0.73",
												"depth": "59.9",
												"clarity": "SI2",
												"table": "57",
												"z": "3.54"
											},
											{
												"x": "5.92",
												"y": "5.87",
												"cut": "Premium",
												"price": "2770",
												"color": "G",
												"carat": "0.73",
												"depth": "59.2",
												"clarity": "VS2",
												"table": "59",
												"z": "3.49"
											},
											{
												"x": "5.81",
												"y": "5.83",
												"cut": "Very Good",
												"price": "2771",
												"color": "H",
												"carat": "0.72",
												"depth": "60.3",
												"clarity": "VVS2",
												"table": "56",
												"z": "3.51"
											},
											{
												"x": "5.79",
												"y": "5.82",
												"cut": "Very Good",
												"price": "2771",
												"color": "F",
												"carat": "0.73",
												"depth": "61.7",
												"clarity": "SI1",
												"table": "60",
												"z": "3.58"
											},
											{
												"x": "5.73",
												"y": "5.77",
												"cut": "Ideal",
												"price": "2771",
												"color": "G",
												"carat": "0.71",
												"depth": "61.9",
												"clarity": "VS2",
												"table": "57",
												"z": "3.56"
											},
											{
												"x": "5.97",
												"y": "5.92",
												"cut": "Ideal",
												"price": "2771",
												"color": "F",
												"carat": "0.79",
												"depth": "61.9",
												"clarity": "SI2",
												"table": "55",
												"z": "3.68"
											},
											{
												"x": "5.83",
												"y": "5.89",
												"cut": "Very Good",
												"price": "2772",
												"color": "H",
												"carat": "0.73",
												"depth": "60.4",
												"clarity": "VVS1",
												"table": "59",
												"z": "3.54"
											},
											{
												"x": "6.01",
												"y": "6.03",
												"cut": "Very Good",
												"price": "2772",
												"color": "F",
												"carat": "0.8",
												"depth": "61",
												"clarity": "SI2",
												"table": "57",
												"z": "3.67"
											},
											{
												"x": "5.39",
												"y": "5.44",
												"cut": "Ideal",
												"price": "2772",
												"color": "G",
												"carat": "0.58",
												"depth": "61.5",
												"clarity": "VVS1",
												"table": "55",
												"z": "3.33"
											},
											{
												"x": "5.33",
												"y": "5.37",
												"cut": "Ideal",
												"price": "2772",
												"color": "F",
												"carat": "0.58",
												"depth": "61.7",
												"clarity": "VVS1",
												"table": "56",
												"z": "3.3"
											},
											{
												"x": "5.8",
												"y": "5.88",
												"cut": "Good",
												"price": "2772",
												"color": "E",
												"carat": "0.71",
												"depth": "59.2",
												"clarity": "VS2",
												"table": "61",
												"z": "3.46"
											},
											{
												"x": "5.85",
												"y": "5.89",
												"cut": "Ideal",
												"price": "2773",
												"color": "D",
												"carat": "0.75",
												"depth": "61.3",
												"clarity": "SI2",
												"table": "56",
												"z": "3.6"
											},
											{
												"x": "5.87",
												"y": "5.78",
												"cut": "Premium",
												"price": "2773",
												"color": "D",
												"carat": "0.7",
												"depth": "58",
												"clarity": "VS2",
												"table": "62",
												"z": "3.38"
											},
											{
												"x": "6.83",
												"y": "6.9",
												"cut": "Very Good",
												"price": "2774",
												"color": "J",
												"carat": "1.17",
												"depth": "60.2",
												"clarity": "I1",
												"table": "61",
												"z": "4.13"
											},
											{
												"x": "5.41",
												"y": "5.44",
												"cut": "Ideal",
												"price": "2774",
												"color": "E",
												"carat": "0.6",
												"depth": "61.7",
												"clarity": "VS1",
												"table": "55",
												"z": "3.35"
											},
											{
												"x": "5.68",
												"y": "5.74",
												"cut": "Ideal",
												"price": "2774",
												"color": "E",
												"carat": "0.7",
												"depth": "62.7",
												"clarity": "SI1",
												"table": "55",
												"z": "3.58"
											},
											{
												"x": "5.85",
												"y": "5.88",
												"cut": "Good",
												"price": "2774",
												"color": "I",
												"carat": "0.83",
												"depth": "64.6",
												"clarity": "VS2",
												"table": "54",
												"z": "3.79"
											},
											{
												"x": "5.8",
												"y": "5.84",
												"cut": "Very Good",
												"price": "2775",
												"color": "F",
												"carat": "0.74",
												"depth": "61.3",
												"clarity": "VS2",
												"table": "61",
												"z": "3.57"
											},
											{
												"x": "5.62",
												"y": "5.69",
												"cut": "Very Good",
												"price": "2776",
												"color": "G",
												"carat": "0.72",
												"depth": "63.7",
												"clarity": "VS2",
												"table": "56.4",
												"z": "3.61"
											},
											{
												"x": "5.74",
												"y": "5.68",
												"cut": "Premium",
												"price": "2776",
												"color": "E",
												"carat": "0.71",
												"depth": "62.7",
												"clarity": "VS2",
												"table": "58",
												"z": "3.58"
											},
											{
												"x": "5.79",
												"y": "5.62",
												"cut": "Ideal",
												"price": "2776",
												"color": "E",
												"carat": "0.71",
												"depth": "62.2",
												"clarity": "VS2",
												"table": "57",
												"z": "3.55"
											},
											{
												"x": "5.25",
												"y": "5.27",
												"cut": "Ideal",
												"price": "2776",
												"color": "E",
												"carat": "0.54",
												"depth": "61.6",
												"clarity": "VVS2",
												"table": "56",
												"z": "3.24"
											},
											{
												"x": "5.24",
												"y": "5.26",
												"cut": "Ideal",
												"price": "2776",
												"color": "E",
												"carat": "0.54",
												"depth": "61.5",
												"clarity": "VVS2",
												"table": "57",
												"z": "3.23"
											},
											{
												"x": "5.72",
												"y": "5.75",
												"cut": "Ideal",
												"price": "2776",
												"color": "G",
												"carat": "0.72",
												"depth": "61.8",
												"clarity": "SI1",
												"table": "56",
												"z": "3.55"
											},
											{
												"x": "5.79",
												"y": "5.82",
												"cut": "Ideal",
												"price": "2776",
												"color": "G",
												"carat": "0.72",
												"depth": "60.7",
												"clarity": "SI1",
												"table": "56",
												"z": "3.53"
											},
											{
												"x": "5.8",
												"y": "5.84",
												"cut": "Good",
												"price": "2776",
												"color": "G",
												"carat": "0.72",
												"depth": "59.7",
												"clarity": "VS2",
												"table": "60.5",
												"z": "3.47"
											},
											{
												"x": "5.8",
												"y": "5.76",
												"cut": "Ideal",
												"price": "2776",
												"color": "G",
												"carat": "0.71",
												"depth": "60.5",
												"clarity": "SI1",
												"table": "56",
												"z": "3.5"
											},
											{
												"x": "5.66",
												"y": "5.73",
												"cut": "Very Good",
												"price": "2777",
												"color": "D",
												"carat": "0.7",
												"depth": "62.7",
												"clarity": "VS1",
												"table": "58",
												"z": "3.57"
											},
											{
												"x": "5.67",
												"y": "5.7",
												"cut": "Premium",
												"price": "2777",
												"color": "F",
												"carat": "0.71",
												"depth": "62.1",
												"clarity": "VS2",
												"table": "58",
												"z": "3.53"
											},
											{
												"x": "5.64",
												"y": "5.69",
												"cut": "Very Good",
												"price": "2777",
												"color": "F",
												"carat": "0.71",
												"depth": "62.8",
												"clarity": "VS2",
												"table": "57",
												"z": "3.56"
											},
											{
												"x": "5.61",
												"y": "5.64",
												"cut": "Good",
												"price": "2777",
												"color": "F",
												"carat": "0.71",
												"depth": "63.8",
												"clarity": "VS2",
												"table": "58",
												"z": "3.59"
											},
											{
												"x": "5.87",
												"y": "5.9",
												"cut": "Good",
												"price": "2777",
												"color": "F",
												"carat": "0.71",
												"depth": "57.8",
												"clarity": "VS2",
												"table": "60",
												"z": "3.4"
											},
											{
												"x": "5.7",
												"y": "5.67",
												"cut": "Ideal",
												"price": "2777",
												"color": "E",
												"carat": "0.7",
												"depth": "62.1",
												"clarity": "VS2",
												"table": "55",
												"z": "3.53"
											},
											{
												"x": "5.71",
												"y": "5.64",
												"cut": "Premium",
												"price": "2777",
												"color": "E",
												"carat": "0.7",
												"depth": "61.1",
												"clarity": "VS2",
												"table": "60",
												"z": "3.47"
											},
											{
												"x": "5.79",
												"y": "5.75",
												"cut": "Premium",
												"price": "2777",
												"color": "E",
												"carat": "0.7",
												"depth": "60",
												"clarity": "SI1",
												"table": "59",
												"z": "3.46"
											},
											{
												"x": "5.73",
												"y": "5.68",
												"cut": "Premium",
												"price": "2777",
												"color": "E",
												"carat": "0.7",
												"depth": "61.2",
												"clarity": "SI1",
												"table": "57",
												"z": "3.49"
											},
											{
												"x": "5.67",
												"y": "5.63",
												"cut": "Premium",
												"price": "2777",
												"color": "E",
												"carat": "0.7",
												"depth": "62.7",
												"clarity": "SI1",
												"table": "59",
												"z": "3.54"
											},
											{
												"x": "5.73",
												"y": "5.68",
												"cut": "Premium",
												"price": "2777",
												"color": "E",
												"carat": "0.7",
												"depth": "61",
												"clarity": "SI1",
												"table": "57",
												"z": "3.48"
											},
											{
												"x": "5.78",
												"y": "5.72",
												"cut": "Premium",
												"price": "2777",
												"color": "E",
												"carat": "0.7",
												"depth": "61",
												"clarity": "SI1",
												"table": "58",
												"z": "3.51"
											},
											{
												"x": "5.76",
												"y": "5.7",
												"cut": "Ideal",
												"price": "2777",
												"color": "E",
												"carat": "0.7",
												"depth": "61.4",
												"clarity": "SI1",
												"table": "57",
												"z": "3.52"
											},
											{
												"x": "5.82",
												"y": "5.71",
												"cut": "Premium",
												"price": "2777",
												"color": "F",
												"carat": "0.72",
												"depth": "61.8",
												"clarity": "SI1",
												"table": "61",
												"z": "3.56"
											},
											{
												"x": "5.76",
												"y": "5.7",
												"cut": "Very Good",
												"price": "2777",
												"color": "E",
												"carat": "0.7",
												"depth": "59.9",
												"clarity": "SI1",
												"table": "63",
												"z": "3.43"
											},
											{
												"x": "5.71",
												"y": "5.68",
												"cut": "Premium",
												"price": "2777",
												"color": "E",
												"carat": "0.7",
												"depth": "61.3",
												"clarity": "SI1",
												"table": "58",
												"z": "3.49"
											},
											{
												"x": "5.77",
												"y": "5.74",
												"cut": "Premium",
												"price": "2777",
												"color": "E",
												"carat": "0.7",
												"depth": "60.5",
												"clarity": "SI1",
												"table": "58",
												"z": "3.48"
											},
											{
												"x": "5.64",
												"y": "5.59",
												"cut": "Good",
												"price": "2777",
												"color": "E",
												"carat": "0.7",
												"depth": "64.1",
												"clarity": "VS2",
												"table": "59",
												"z": "3.6"
											},
											{
												"x": "6.05",
												"y": "5.97",
												"cut": "Fair",
												"price": "2777",
												"color": "H",
												"carat": "0.98",
												"depth": "67.9",
												"clarity": "SI2",
												"table": "60",
												"z": "4.08"
											},
											{
												"x": "5.83",
												"y": "5.8",
												"cut": "Premium",
												"price": "2777",
												"color": "F",
												"carat": "0.78",
												"depth": "62.4",
												"clarity": "SI1",
												"table": "58",
												"z": "3.63"
											},
											{
												"x": "5.6",
												"y": "5.51",
												"cut": "Very Good",
												"price": "2777",
												"color": "E",
												"carat": "0.7",
												"depth": "63.2",
												"clarity": "SI1",
												"table": "60",
												"z": "3.51"
											},
											{
												"x": "5.19",
												"y": "5.22",
												"cut": "Ideal",
												"price": "2778",
												"color": "F",
												"carat": "0.52",
												"depth": "61.3",
												"clarity": "VVS1",
												"table": "55",
												"z": "3.19"
											},
											{
												"x": "5.82",
												"y": "5.84",
												"cut": "Very Good",
												"price": "2779",
												"color": "H",
												"carat": "0.73",
												"depth": "60.8",
												"clarity": "VS2",
												"table": "56",
												"z": "3.55"
											},
											{
												"x": "5.84",
												"y": "5.8",
												"cut": "Ideal",
												"price": "2779",
												"color": "E",
												"carat": "0.74",
												"depth": "61.7",
												"clarity": "SI1",
												"table": "56",
												"z": "3.59"
											},
											{
												"x": "5.61",
												"y": "5.65",
												"cut": "Very Good",
												"price": "2780",
												"color": "F",
												"carat": "0.7",
												"depth": "63.6",
												"clarity": "VS2",
												"table": "57",
												"z": "3.58"
											},
											{
												"x": "5.9",
												"y": "5.93",
												"cut": "Premium",
												"price": "2780",
												"color": "G",
												"carat": "0.77",
												"depth": "61.2",
												"clarity": "VS2",
												"table": "58",
												"z": "3.62"
											},
											{
												"x": "5.68",
												"y": "5.72",
												"cut": "Ideal",
												"price": "2780",
												"color": "F",
												"carat": "0.71",
												"depth": "62.1",
												"clarity": "VS2",
												"table": "54",
												"z": "3.54"
											},
											{
												"x": "5.81",
												"y": "5.86",
												"cut": "Ideal",
												"price": "2780",
												"color": "G",
												"carat": "0.74",
												"depth": "61.5",
												"clarity": "VS1",
												"table": "55",
												"z": "3.59"
											},
											{
												"x": "5.64",
												"y": "5.73",
												"cut": "Ideal",
												"price": "2780",
												"color": "G",
												"carat": "0.7",
												"depth": "61.4",
												"clarity": "VS1",
												"table": "59",
												"z": "3.49"
											},
											{
												"x": "6.39",
												"y": "6.36",
												"cut": "Premium",
												"price": "2781",
												"color": "F",
												"carat": "1.01",
												"depth": "61.8",
												"clarity": "I1",
												"table": "60",
												"z": "3.94"
											},
											{
												"x": "5.83",
												"y": "5.88",
												"cut": "Ideal",
												"price": "2781",
												"color": "H",
												"carat": "0.77",
												"depth": "62.2",
												"clarity": "SI1",
												"table": "56",
												"z": "3.64"
											},
											{
												"x": "5.92",
												"y": "5.99",
												"cut": "Ideal",
												"price": "2781",
												"color": "H",
												"carat": "0.78",
												"depth": "61.2",
												"clarity": "SI1",
												"table": "56",
												"z": "3.64"
											},
											{
												"x": "5.83",
												"y": "5.76",
												"cut": "Very Good",
												"price": "2782",
												"color": "H",
												"carat": "0.72",
												"depth": "60.6",
												"clarity": "VS1",
												"table": "63",
												"z": "3.51"
											},
											{
												"x": "5.34",
												"y": "5.37",
												"cut": "Very Good",
												"price": "2782",
												"color": "D",
												"carat": "0.53",
												"depth": "57.5",
												"clarity": "VVS2",
												"table": "64",
												"z": "3.08"
											},
											{
												"x": "5.9",
												"y": "5.94",
												"cut": "Ideal",
												"price": "2782",
												"color": "G",
												"carat": "0.76",
												"depth": "61.3",
												"clarity": "VS2",
												"table": "56",
												"z": "3.63"
											},
											{
												"x": "5.81",
												"y": "5.77",
												"cut": "Good",
												"price": "2782",
												"color": "E",
												"carat": "0.7",
												"depth": "57.2",
												"clarity": "VS1",
												"table": "62",
												"z": "3.31"
											},
											{
												"x": "5.62",
												"y": "5.54",
												"cut": "Premium",
												"price": "2782",
												"color": "E",
												"carat": "0.7",
												"depth": "62.9",
												"clarity": "VS1",
												"table": "60",
												"z": "3.51"
											},
											{
												"x": "5.78",
												"y": "5.73",
												"cut": "Very Good",
												"price": "2782",
												"color": "D",
												"carat": "0.75",
												"depth": "63.1",
												"clarity": "SI2",
												"table": "58",
												"z": "3.63"
											},
											{
												"x": "5.76",
												"y": "5.75",
												"cut": "Ideal",
												"price": "2782",
												"color": "D",
												"carat": "0.72",
												"depth": "60.8",
												"clarity": "SI1",
												"table": "57",
												"z": "3.5"
											},
											{
												"x": "5.73",
												"y": "5.69",
												"cut": "Premium",
												"price": "2782",
												"color": "D",
												"carat": "0.72",
												"depth": "62.7",
												"clarity": "SI1",
												"table": "59",
												"z": "3.58"
											},
											{
												"x": "5.68",
												"y": "5.66",
												"cut": "Premium",
												"price": "2782",
												"color": "D",
												"carat": "0.7",
												"depth": "62.8",
												"clarity": "SI1",
												"table": "60",
												"z": "3.56"
											},
											{
												"x": "6.39",
												"y": "6.2",
												"cut": "Fair",
												"price": "2782",
												"color": "G",
												"carat": "0.84",
												"depth": "55.1",
												"clarity": "SI1",
												"table": "67",
												"z": "3.47"
											},
											{
												"x": "5.88",
												"y": "5.85",
												"cut": "Premium",
												"price": "2782",
												"color": "F",
												"carat": "0.75",
												"depth": "61.4",
												"clarity": "SI1",
												"table": "59",
												"z": "3.6"
											},
											{
												"x": "5.14",
												"y": "5.18",
												"cut": "Ideal",
												"price": "2783",
												"color": "F",
												"carat": "0.52",
												"depth": "62.2",
												"clarity": "IF",
												"table": "55",
												"z": "3.21"
											},
											{
												"x": "5.69",
												"y": "5.73",
												"cut": "Very Good",
												"price": "2784",
												"color": "F",
												"carat": "0.72",
												"depth": "63",
												"clarity": "VS2",
												"table": "54",
												"z": "3.6"
											},
											{
												"x": "5.85",
												"y": "5.92",
												"cut": "Very Good",
												"price": "2784",
												"color": "H",
												"carat": "0.79",
												"depth": "63.7",
												"clarity": "VS1",
												"table": "56",
												"z": "3.75"
											},
											{
												"x": "5.66",
												"y": "5.69",
												"cut": "Very Good",
												"price": "2787",
												"color": "F",
												"carat": "0.72",
												"depth": "63.6",
												"clarity": "VS2",
												"table": "58",
												"z": "3.61"
											},
											{
												"x": "5.11",
												"y": "5.15",
												"cut": "Ideal",
												"price": "2787",
												"color": "F",
												"carat": "0.51",
												"depth": "62",
												"clarity": "VVS1",
												"table": "57",
												"z": "3.18"
											},
											{
												"x": "5.54",
												"y": "5.55",
												"cut": "Ideal",
												"price": "2787",
												"color": "D",
												"carat": "0.64",
												"depth": "61.5",
												"clarity": "VS1",
												"table": "56",
												"z": "3.41"
											},
											{
												"x": "5.74",
												"y": "5.77",
												"cut": "Very Good",
												"price": "2788",
												"color": "H",
												"carat": "0.7",
												"depth": "60.5",
												"clarity": "VVS1",
												"table": "60",
												"z": "3.48"
											},
											{
												"x": "6.07",
												"y": "6.1",
												"cut": "Very Good",
												"price": "2788",
												"color": "I",
												"carat": "0.83",
												"depth": "61.1",
												"clarity": "VS1",
												"table": "60",
												"z": "3.72"
											},
											{
												"x": "5.85",
												"y": "5.87",
												"cut": "Ideal",
												"price": "2788",
												"color": "I",
												"carat": "0.76",
												"depth": "61.8",
												"clarity": "VVS2",
												"table": "56",
												"z": "3.62"
											},
											{
												"x": "5.64",
												"y": "5.68",
												"cut": "Good",
												"price": "2788",
												"color": "D",
												"carat": "0.71",
												"depth": "63.3",
												"clarity": "VS2",
												"table": "56",
												"z": "3.58"
											},
											{
												"x": "5.97",
												"y": "5.92",
												"cut": "Good",
												"price": "2788",
												"color": "G",
												"carat": "0.77",
												"depth": "59.4",
												"clarity": "VS1",
												"table": "64",
												"z": "3.53"
											},
											{
												"x": "5.71",
												"y": "5.65",
												"cut": "Ideal",
												"price": "2788",
												"color": "F",
												"carat": "0.71",
												"depth": "62.5",
												"clarity": "SI1",
												"table": "55",
												"z": "3.55"
											},
											{
												"x": "6.29",
												"y": "6.21",
												"cut": "Fair",
												"price": "2788",
												"color": "E",
												"carat": "1.01",
												"depth": "64.5",
												"clarity": "I1",
												"table": "58",
												"z": "4.03"
											},
											{
												"x": "6.31",
												"y": "6.22",
												"cut": "Premium",
												"price": "2788",
												"color": "H",
												"carat": "1.01",
												"depth": "62.7",
												"clarity": "SI2",
												"table": "59",
												"z": "3.93"
											},
											{
												"x": "5.81",
												"y": "5.77",
												"cut": "Good",
												"price": "2789",
												"color": "F",
												"carat": "0.77",
												"depth": "64.2",
												"clarity": "SI1",
												"table": "52",
												"z": "3.72"
											},
											{
												"x": "5.76",
												"y": "5.85",
												"cut": "Good",
												"price": "2789",
												"color": "E",
												"carat": "0.76",
												"depth": "63.7",
												"clarity": "SI1",
												"table": "54",
												"z": "3.7"
											},
											{
												"x": "5.92",
												"y": "5.94",
												"cut": "Premium",
												"price": "2789",
												"color": "E",
												"carat": "0.76",
												"depth": "60.4",
												"clarity": "SI1",
												"table": "58",
												"z": "3.58"
											},
											{
												"x": "5.82",
												"y": "5.86",
												"cut": "Premium",
												"price": "2789",
												"color": "E",
												"carat": "0.76",
												"depth": "61.8",
												"clarity": "SI1",
												"table": "58",
												"z": "3.61"
											},
											{
												"x": "6.49",
												"y": "6.45",
												"cut": "Very Good",
												"price": "2789",
												"color": "J",
												"carat": "1.05",
												"depth": "63.2",
												"clarity": "SI2",
												"table": "56",
												"z": "4.09"
											},
											{
												"x": "5.97",
												"y": "6.01",
												"cut": "Ideal",
												"price": "2789",
												"color": "G",
												"carat": "0.81",
												"depth": "61.6",
												"clarity": "SI2",
												"table": "56",
												"z": "3.69"
											},
											{
												"x": "5.72",
												"y": "5.75",
												"cut": "Ideal",
												"price": "2789",
												"color": "E",
												"carat": "0.7",
												"depth": "61.6",
												"clarity": "SI1",
												"table": "56",
												"z": "3.53"
											},
											{
												"x": "5.28",
												"y": "5.3",
												"cut": "Ideal",
												"price": "2789",
												"color": "G",
												"carat": "0.55",
												"depth": "60.9",
												"clarity": "IF",
												"table": "57",
												"z": "3.22"
											},
											{
												"x": "5.94",
												"y": "5.99",
												"cut": "Good",
												"price": "2789",
												"color": "G",
												"carat": "0.81",
												"depth": "61",
												"clarity": "SI2",
												"table": "61",
												"z": "3.64"
											},
											{
												"x": "5.48",
												"y": "5.41",
												"cut": "Premium",
												"price": "2789",
												"color": "E",
												"carat": "0.63",
												"depth": "62.1",
												"clarity": "VVS2",
												"table": "57",
												"z": "3.38"
											},
											{
												"x": "5.55",
												"y": "5.52",
												"cut": "Premium",
												"price": "2789",
												"color": "E",
												"carat": "0.63",
												"depth": "60.9",
												"clarity": "VVS1",
												"table": "60",
												"z": "3.37"
											},
											{
												"x": "5.9",
												"y": "5.88",
												"cut": "Premium",
												"price": "2789",
												"color": "H",
												"carat": "0.77",
												"depth": "61.3",
												"clarity": "VS1",
												"table": "60",
												"z": "3.61"
											},
											{
												"x": "6.41",
												"y": "6.27",
												"cut": "Fair",
												"price": "2789",
												"color": "J",
												"carat": "1.05",
												"depth": "65.8",
												"clarity": "SI2",
												"table": "59",
												"z": "4.18"
											},
											{
												"x": "5.54",
												"y": "5.58",
												"cut": "Ideal",
												"price": "2790",
												"color": "G",
												"carat": "0.64",
												"depth": "61.3",
												"clarity": "IF",
												"table": "56",
												"z": "3.41"
											},
											{
												"x": "6",
												"y": "5.94",
												"cut": "Premium",
												"price": "2790",
												"color": "I",
												"carat": "0.76",
												"depth": "58.8",
												"clarity": "VVS1",
												"table": "59",
												"z": "3.51"
											},
											{
												"x": "6.02",
												"y": "6.05",
												"cut": "Ideal",
												"price": "2790",
												"color": "F",
												"carat": "0.83",
												"depth": "62.3",
												"clarity": "SI2",
												"table": "55",
												"z": "3.76"
											},
											{
												"x": "5.77",
												"y": "5.74",
												"cut": "Premium",
												"price": "2790",
												"color": "F",
												"carat": "0.71",
												"depth": "60.1",
												"clarity": "VS1",
												"table": "62",
												"z": "3.46"
											},
											{
												"x": "5.73",
												"y": "5.69",
												"cut": "Premium",
												"price": "2790",
												"color": "F",
												"carat": "0.71",
												"depth": "61.8",
												"clarity": "VS1",
												"table": "59",
												"z": "3.53"
											},
											{
												"x": "6.07",
												"y": "6.1",
												"cut": "Very Good",
												"price": "2791",
												"color": "I",
												"carat": "0.87",
												"depth": "63.6",
												"clarity": "SI1",
												"table": "55.8",
												"z": "3.87"
											},
											{
												"x": "5.74",
												"y": "5.78",
												"cut": "Ideal",
												"price": "2791",
												"color": "E",
												"carat": "0.73",
												"depth": "62.2",
												"clarity": "SI1",
												"table": "56",
												"z": "3.58"
											},
											{
												"x": "5.83",
												"y": "5.86",
												"cut": "Premium",
												"price": "2792",
												"color": "E",
												"carat": "0.71",
												"depth": "59.2",
												"clarity": "SI1",
												"table": "59",
												"z": "3.46"
											},
											{
												"x": "5.7",
												"y": "5.75",
												"cut": "Premium",
												"price": "2792",
												"color": "E",
												"carat": "0.71",
												"depth": "61.8",
												"clarity": "SI1",
												"table": "59",
												"z": "3.54"
											},
											{
												"x": "5.72",
												"y": "5.77",
												"cut": "Ideal",
												"price": "2792",
												"color": "E",
												"carat": "0.71",
												"depth": "61.3",
												"clarity": "SI1",
												"table": "55",
												"z": "3.52"
											},
											{
												"x": "5.71",
												"y": "5.65",
												"cut": "Premium",
												"price": "2792",
												"color": "F",
												"carat": "0.7",
												"depth": "62.1",
												"clarity": "VS1",
												"table": "60",
												"z": "3.53"
											},
											{
												"x": "5.78",
												"y": "5.75",
												"cut": "Premium",
												"price": "2792",
												"color": "F",
												"carat": "0.7",
												"depth": "60.7",
												"clarity": "VS1",
												"table": "60",
												"z": "3.5"
											},
											{
												"x": "5.91",
												"y": "5.86",
												"cut": "Premium",
												"price": "2792",
												"color": "H",
												"carat": "0.76",
												"depth": "59.6",
												"clarity": "VVS2",
												"table": "57",
												"z": "3.51"
											},
											{
												"x": "5.73",
												"y": "5.68",
												"cut": "Ideal",
												"price": "2792",
												"color": "F",
												"carat": "0.7",
												"depth": "62.2",
												"clarity": "VS1",
												"table": "56",
												"z": "3.55"
											},
											{
												"x": "5.98",
												"y": "6.06",
												"cut": "Very Good",
												"price": "2793",
												"color": "G",
												"carat": "0.79",
												"depth": "60.6",
												"clarity": "SI1",
												"table": "57",
												"z": "3.65"
											},
											{
												"x": "5.66",
												"y": "5.69",
												"cut": "Very Good",
												"price": "2793",
												"color": "E",
												"carat": "0.7",
												"depth": "62.9",
												"clarity": "VS2",
												"table": "57",
												"z": "3.57"
											},
											{
												"x": "5.6",
												"y": "5.66",
												"cut": "Good",
												"price": "2793",
												"color": "E",
												"carat": "0.7",
												"depth": "64.1",
												"clarity": "VS2",
												"table": "55",
												"z": "3.61"
											},
											{
												"x": "5.87",
												"y": "5.91",
												"cut": "Ideal",
												"price": "2793",
												"color": "I",
												"carat": "0.76",
												"depth": "61.3",
												"clarity": "VS2",
												"table": "56",
												"z": "3.61"
											},
											{
												"x": "5.72",
												"y": "5.76",
												"cut": "Ideal",
												"price": "2793",
												"color": "H",
												"carat": "0.73",
												"depth": "62.7",
												"clarity": "VS2",
												"table": "55",
												"z": "3.6"
											},
											{
												"x": "5.91",
												"y": "5.86",
												"cut": "Very Good",
												"price": "2794",
												"color": "E",
												"carat": "0.79",
												"depth": "63.2",
												"clarity": "SI1",
												"table": "56",
												"z": "3.72"
											},
											{
												"x": "5.81",
												"y": "5.82",
												"cut": "Very Good",
												"price": "2795",
												"color": "E",
												"carat": "0.71",
												"depth": "60.7",
												"clarity": "VS2",
												"table": "56",
												"z": "3.53"
											},
											{
												"x": "5.91",
												"y": "5.86",
												"cut": "Premium",
												"price": "2795",
												"color": "I",
												"carat": "0.81",
												"depth": "61.9",
												"clarity": "VVS2",
												"table": "60",
												"z": "3.64"
											},
											{
												"x": "5.92",
												"y": "5.96",
												"cut": "Ideal",
												"price": "2795",
												"color": "F",
												"carat": "0.81",
												"depth": "62.6",
												"clarity": "SI2",
												"table": "55",
												"z": "3.72"
											},
											{
												"x": "5.74",
												"y": "5.72",
												"cut": "Good",
												"price": "2795",
												"color": "F",
												"carat": "0.72",
												"depth": "60.7",
												"clarity": "VS1",
												"table": "60",
												"z": "3.48"
											},
											{
												"x": "5.73",
												"y": "5.69",
												"cut": "Premium",
												"price": "2795",
												"color": "D",
												"carat": "0.72",
												"depth": "62",
												"clarity": "SI2",
												"table": "60",
												"z": "3.54"
											},
											{
												"x": "5.72",
												"y": "5.7",
												"cut": "Premium",
												"price": "2795",
												"color": "I",
												"carat": "0.72",
												"depth": "63",
												"clarity": "IF",
												"table": "57",
												"z": "3.6"
											},
											{
												"x": "6.17",
												"y": "6.13",
												"cut": "Premium",
												"price": "2795",
												"color": "H",
												"carat": "0.81",
												"depth": "58",
												"clarity": "VS2",
												"table": "59",
												"z": "3.57"
											},
											{
												"x": "5.73",
												"y": "5.65",
												"cut": "Premium",
												"price": "2795",
												"color": "G",
												"carat": "0.72",
												"depth": "62.9",
												"clarity": "VS2",
												"table": "57",
												"z": "3.58"
											},
											{
												"x": "6.61",
												"y": "6.55",
												"cut": "Premium",
												"price": "2795",
												"color": "I",
												"carat": "1",
												"depth": "58.2",
												"clarity": "SI2",
												"table": "60",
												"z": "3.83"
											},
											{
												"x": "5.7",
												"y": "5.76",
												"cut": "Good",
												"price": "2796",
												"color": "E",
												"carat": "0.73",
												"depth": "63.2",
												"clarity": "SI1",
												"table": "58",
												"z": "3.62"
											},
											{
												"x": "5.94",
												"y": "6.01",
												"cut": "Very Good",
												"price": "2797",
												"color": "H",
												"carat": "0.81",
												"depth": "61.3",
												"clarity": "SI2",
												"table": "59",
												"z": "3.66"
											},
											{
												"x": "6.07",
												"y": "6.1",
												"cut": "Very Good",
												"price": "2797",
												"color": "E",
												"carat": "0.81",
												"depth": "60.3",
												"clarity": "SI1",
												"table": "60",
												"z": "3.67"
											},
											{
												"x": "5.67",
												"y": "5.71",
												"cut": "Premium",
												"price": "2797",
												"color": "D",
												"carat": "0.71",
												"depth": "62.7",
												"clarity": "SI1",
												"table": "60",
												"z": "3.57"
											},
											{
												"x": "5.73",
												"y": "5.75",
												"cut": "Premium",
												"price": "2797",
												"color": "D",
												"carat": "0.71",
												"depth": "61.3",
												"clarity": "SI1",
												"table": "58",
												"z": "3.52"
											},
											{
												"x": "5.74",
												"y": "5.69",
												"cut": "Premium",
												"price": "2797",
												"color": "D",
												"carat": "0.71",
												"depth": "61.6",
												"clarity": "SI1",
												"table": "60",
												"z": "3.52"
											},
											{
												"x": "5.34",
												"y": "5.35",
												"cut": "Ideal",
												"price": "2797",
												"color": "F",
												"carat": "0.57",
												"depth": "61.9",
												"clarity": "VVS2",
												"table": "55",
												"z": "3.31"
											},
											{
												"x": "5.12",
												"y": "5.16",
												"cut": "Ideal",
												"price": "2797",
												"color": "D",
												"carat": "0.51",
												"depth": "61.7",
												"clarity": "VVS1",
												"table": "56",
												"z": "3.17"
											},
											{
												"x": "5.72",
												"y": "5.75",
												"cut": "Ideal",
												"price": "2797",
												"color": "G",
												"carat": "0.72",
												"depth": "61.9",
												"clarity": "VS2",
												"table": "58",
												"z": "3.55"
											},
											{
												"x": "5.77",
												"y": "5.81",
												"cut": "Ideal",
												"price": "2797",
												"color": "H",
												"carat": "0.74",
												"depth": "61.8",
												"clarity": "VS1",
												"table": "58",
												"z": "3.58"
											},
											{
												"x": "5.81",
												"y": "5.82",
												"cut": "Ideal",
												"price": "2797",
												"color": "H",
												"carat": "0.74",
												"depth": "61.6",
												"clarity": "VS1",
												"table": "56",
												"z": "3.58"
											},
											{
												"x": "5.81",
												"y": "5.9",
												"cut": "Fair",
												"price": "2797",
												"color": "G",
												"carat": "0.7",
												"depth": "58.8",
												"clarity": "VVS1",
												"table": "66",
												"z": "3.44"
											},
											{
												"x": "6.03",
												"y": "6.01",
												"cut": "Premium",
												"price": "2797",
												"color": "F",
												"carat": "0.8",
												"depth": "61",
												"clarity": "SI2",
												"table": "57",
												"z": "3.67"
											},
											{
												"x": "6.19",
												"y": "6.05",
												"cut": "Fair",
												"price": "2797",
												"color": "E",
												"carat": "1.01",
												"depth": "67.4",
												"clarity": "SI2",
												"table": "60",
												"z": "4.13"
											},
											{
												"x": "5.92",
												"y": "5.82",
												"cut": "Very Good",
												"price": "2797",
												"color": "H",
												"carat": "0.8",
												"depth": "63.4",
												"clarity": "VS2",
												"table": "60",
												"z": "3.72"
											},
											{
												"x": "5.87",
												"y": "5.91",
												"cut": "Ideal",
												"price": "2798",
												"color": "I",
												"carat": "0.77",
												"depth": "61.5",
												"clarity": "VS1",
												"table": "59",
												"z": "3.62"
											},
											{
												"x": "6.19",
												"y": "6.25",
												"cut": "Very Good",
												"price": "2799",
												"color": "E",
												"carat": "0.83",
												"depth": "58",
												"clarity": "SI2",
												"table": "62",
												"z": "3.61"
											},
											{
												"x": "5.97",
												"y": "6.02",
												"cut": "Ideal",
												"price": "2799",
												"color": "F",
												"carat": "0.82",
												"depth": "62.4",
												"clarity": "SI2",
												"table": "54",
												"z": "3.74"
											},
											{
												"x": "5.91",
												"y": "5.86",
												"cut": "Ideal",
												"price": "2799",
												"color": "D",
												"carat": "0.78",
												"depth": "61.9",
												"clarity": "SI1",
												"table": "57",
												"z": "3.64"
											},
											{
												"x": "5.43",
												"y": "5.46",
												"cut": "Very Good",
												"price": "2800",
												"color": "G",
												"carat": "0.6",
												"depth": "61.6",
												"clarity": "IF",
												"table": "56",
												"z": "3.35"
											},
											{
												"x": "6.07",
												"y": "6.11",
												"cut": "Good",
												"price": "2800",
												"color": "I",
												"carat": "0.9",
												"depth": "62.2",
												"clarity": "SI2",
												"table": "59",
												"z": "3.79"
											},
											{
												"x": "5.6",
												"y": "5.66",
												"cut": "Premium",
												"price": "2800",
												"color": "E",
												"carat": "0.7",
												"depth": "62.2",
												"clarity": "VS1",
												"table": "58",
												"z": "3.5"
											},
											{
												"x": "6.17",
												"y": "6.23",
												"cut": "Very Good",
												"price": "2800",
												"color": "I",
												"carat": "0.9",
												"depth": "61.3",
												"clarity": "SI2",
												"table": "56",
												"z": "3.8"
											},
											{
												"x": "5.99",
												"y": "6.08",
												"cut": "Ideal",
												"price": "2800",
												"color": "G",
												"carat": "0.83",
												"depth": "62.3",
												"clarity": "SI1",
												"table": "57",
												"z": "3.76"
											},
											{
												"x": "6.03",
												"y": "6.07",
												"cut": "Ideal",
												"price": "2800",
												"color": "G",
												"carat": "0.83",
												"depth": "61.8",
												"clarity": "SI1",
												"table": "57",
												"z": "3.74"
											},
											{
												"x": "5.95",
												"y": "6.02",
												"cut": "Very Good",
												"price": "2800",
												"color": "H",
												"carat": "0.83",
												"depth": "62.5",
												"clarity": "SI1",
												"table": "59",
												"z": "3.74"
											},
											{
												"x": "5.74",
												"y": "5.68",
												"cut": "Premium",
												"price": "2800",
												"color": "G",
												"carat": "0.74",
												"depth": "62.9",
												"clarity": "VS1",
												"table": "60",
												"z": "3.59"
											},
											{
												"x": "5.92",
												"y": "5.95",
												"cut": "Ideal",
												"price": "2800",
												"color": "I",
												"carat": "0.79",
												"depth": "61.8",
												"clarity": "VS1",
												"table": "59",
												"z": "3.67"
											},
											{
												"x": "5.43",
												"y": "5.45",
												"cut": "Ideal",
												"price": "2800",
												"color": "G",
												"carat": "0.61",
												"depth": "62.3",
												"clarity": "IF",
												"table": "56",
												"z": "3.39"
											},
											{
												"x": "5.89",
												"y": "5.8",
												"cut": "Fair",
												"price": "2800",
												"color": "G",
												"carat": "0.76",
												"depth": "59",
												"clarity": "VS1",
												"table": "70",
												"z": "3.46"
											},
											{
												"x": "6.37",
												"y": "6.41",
												"cut": "Ideal",
												"price": "2801",
												"color": "F",
												"carat": "0.96",
												"depth": "60.7",
												"clarity": "I1",
												"table": "55",
												"z": "3.88"
											},
											{
												"x": "5.8",
												"y": "5.76",
												"cut": "Ideal",
												"price": "2801",
												"color": "F",
												"carat": "0.73",
												"depth": "62.5",
												"clarity": "VS2",
												"table": "55",
												"z": "3.61"
											},
											{
												"x": "5.76",
												"y": "5.7",
												"cut": "Premium",
												"price": "2801",
												"color": "F",
												"carat": "0.73",
												"depth": "62.7",
												"clarity": "VS2",
												"table": "58",
												"z": "3.59"
											},
											{
												"x": "5.93",
												"y": "5.96",
												"cut": "Ideal",
												"price": "2801",
												"color": "H",
												"carat": "0.75",
												"depth": "60.4",
												"clarity": "SI1",
												"table": "57",
												"z": "3.59"
											},
											{
												"x": "5.7",
												"y": "5.67",
												"cut": "Premium",
												"price": "2801",
												"color": "F",
												"carat": "0.71",
												"depth": "62.1",
												"clarity": "VS2",
												"table": "58",
												"z": "3.53"
											},
											{
												"x": "5.9",
												"y": "5.87",
												"cut": "Good",
												"price": "2801",
												"color": "F",
												"carat": "0.71",
												"depth": "57.8",
												"clarity": "VS2",
												"table": "60",
												"z": "3.4"
											},
											{
												"x": "5.64",
												"y": "5.61",
												"cut": "Good",
												"price": "2801",
												"color": "F",
												"carat": "0.71",
												"depth": "63.8",
												"clarity": "VS2",
												"table": "58",
												"z": "3.59"
											},
											{
												"x": "5.69",
												"y": "5.64",
												"cut": "Premium",
												"price": "2801",
												"color": "F",
												"carat": "0.71",
												"depth": "62.8",
												"clarity": "VS2",
												"table": "57",
												"z": "3.56"
											},
											{
												"x": "6.46",
												"y": "6.41",
												"cut": "Premium",
												"price": "2801",
												"color": "G",
												"carat": "1.04",
												"depth": "62.2",
												"clarity": "I1",
												"table": "58",
												"z": "4"
											},
											{
												"x": "6.45",
												"y": "6.34",
												"cut": "Premium",
												"price": "2801",
												"color": "J",
												"carat": "1",
												"depth": "62.3",
												"clarity": "SI2",
												"table": "58",
												"z": "3.98"
											},
											{
												"x": "6.19",
												"y": "6.23",
												"cut": "Very Good",
												"price": "2802",
												"color": "G",
												"carat": "0.87",
												"depth": "59.9",
												"clarity": "SI2",
												"table": "58",
												"z": "3.72"
											},
											{
												"x": "5.22",
												"y": "5.25",
												"cut": "Ideal",
												"price": "2802",
												"color": "F",
												"carat": "0.53",
												"depth": "61.9",
												"clarity": "IF",
												"table": "54",
												"z": "3.24"
											},
											{
												"x": "5.79",
												"y": "5.61",
												"cut": "Premium",
												"price": "2802",
												"color": "E",
												"carat": "0.72",
												"depth": "63",
												"clarity": "VS2",
												"table": "55",
												"z": "3.59"
											},
											{
												"x": "5.83",
												"y": "5.7",
												"cut": "Premium",
												"price": "2802",
												"color": "F",
												"carat": "0.72",
												"depth": "62.4",
												"clarity": "VS1",
												"table": "58",
												"z": "3.6"
											},
											{
												"x": "5.63",
												"y": "5.65",
												"cut": "Very Good",
												"price": "2803",
												"color": "F",
												"carat": "0.7",
												"depth": "62.9",
												"clarity": "VS2",
												"table": "58",
												"z": "3.55"
											},
											{
												"x": "5.74",
												"y": "5.79",
												"cut": "Very Good",
												"price": "2803",
												"color": "E",
												"carat": "0.74",
												"depth": "63.5",
												"clarity": "SI1",
												"table": "56",
												"z": "3.66"
											},
											{
												"x": "5.75",
												"y": "5.71",
												"cut": "Ideal",
												"price": "2803",
												"color": "G",
												"carat": "0.71",
												"depth": "61.3",
												"clarity": "VS2",
												"table": "56",
												"z": "3.51"
											},
											{
												"x": "5.84",
												"y": "5.89",
												"cut": "Ideal",
												"price": "2803",
												"color": "E",
												"carat": "0.73",
												"depth": "60.6",
												"clarity": "SI1",
												"table": "54",
												"z": "3.55"
											},
											{
												"x": "5.56",
												"y": "5.59",
												"cut": "Good",
												"price": "2803",
												"color": "G",
												"carat": "0.7",
												"depth": "65.1",
												"clarity": "VS1",
												"table": "58",
												"z": "3.63"
											},
											{
												"x": "5.7",
												"y": "5.67",
												"cut": "Premium",
												"price": "2803",
												"color": "F",
												"carat": "0.71",
												"depth": "62.6",
												"clarity": "VS2",
												"table": "58",
												"z": "3.56"
											},
											{
												"x": "5.85",
												"y": "5.81",
												"cut": "Premium",
												"price": "2803",
												"color": "F",
												"carat": "0.71",
												"depth": "58",
												"clarity": "VS2",
												"table": "62",
												"z": "3.38"
											},
											{
												"x": "5.7",
												"y": "5.65",
												"cut": "Premium",
												"price": "2803",
												"color": "G",
												"carat": "0.71",
												"depth": "62.4",
												"clarity": "VS1",
												"table": "61",
												"z": "3.54"
											},
											{
												"x": "5.93",
												"y": "5.88",
												"cut": "Premium",
												"price": "2803",
												"color": "G",
												"carat": "0.77",
												"depth": "61.3",
												"clarity": "VS2",
												"table": "57",
												"z": "3.62"
											},
											{
												"x": "5.81",
												"y": "5.77",
												"cut": "Premium",
												"price": "2803",
												"color": "G",
												"carat": "0.71",
												"depth": "59.9",
												"clarity": "VS2",
												"table": "60",
												"z": "3.47"
											},
											{
												"x": "6.03",
												"y": "5.95",
												"cut": "Premium",
												"price": "2803",
												"color": "G",
												"carat": "0.78",
												"depth": "60.8",
												"clarity": "VS2",
												"table": "58",
												"z": "3.64"
											},
											{
												"x": "5.66",
												"y": "5.64",
												"cut": "Very Good",
												"price": "2803",
												"color": "G",
												"carat": "0.71",
												"depth": "63.5",
												"clarity": "VS1",
												"table": "55",
												"z": "3.59"
											},
											{
												"x": "6.21",
												"y": "6.15",
												"cut": "Ideal",
												"price": "2803",
												"color": "D",
												"carat": "0.91",
												"depth": "62.2",
												"clarity": "SI2",
												"table": "57",
												"z": "3.85"
											},
											{
												"x": "5.62",
												"y": "5.66",
												"cut": "Very Good",
												"price": "2804",
												"color": "E",
												"carat": "0.71",
												"depth": "63.8",
												"clarity": "VS2",
												"table": "58",
												"z": "3.6"
											},
											{
												"x": "5.66",
												"y": "5.68",
												"cut": "Very Good",
												"price": "2804",
												"color": "E",
												"carat": "0.71",
												"depth": "64",
												"clarity": "VS2",
												"table": "57",
												"z": "3.63"
											},
											{
												"x": "5.88",
												"y": "5.96",
												"cut": "Very Good",
												"price": "2804",
												"color": "E",
												"carat": "0.8",
												"depth": "62.5",
												"clarity": "SI2",
												"table": "56",
												"z": "3.7"
											},
											{
												"x": "5.69",
												"y": "5.73",
												"cut": "Very Good",
												"price": "2804",
												"color": "D",
												"carat": "0.7",
												"depth": "62.3",
												"clarity": "SI1",
												"table": "58",
												"z": "3.56"
											},
											{
												"x": "5.74",
												"y": "5.77",
												"cut": "Ideal",
												"price": "2804",
												"color": "F",
												"carat": "0.72",
												"depth": "61.7",
												"clarity": "VS1",
												"table": "57",
												"z": "3.55"
											},
											{
												"x": "5.75",
												"y": "5.7",
												"cut": "Very Good",
												"price": "2804",
												"color": "F",
												"carat": "0.72",
												"depth": "62.2",
												"clarity": "VS1",
												"table": "58",
												"z": "3.56"
											},
											{
												"x": "6.01",
												"y": "6.08",
												"cut": "Ideal",
												"price": "2804",
												"color": "H",
												"carat": "0.82",
												"depth": "61.5",
												"clarity": "VS2",
												"table": "56",
												"z": "3.72"
											},
											{
												"x": "5.68",
												"y": "5.7",
												"cut": "Ideal",
												"price": "2804",
												"color": "D",
												"carat": "0.7",
												"depth": "61",
												"clarity": "SI1",
												"table": "59",
												"z": "3.47"
											},
											{
												"x": "5.74",
												"y": "5.77",
												"cut": "Ideal",
												"price": "2804",
												"color": "D",
												"carat": "0.72",
												"depth": "62.2",
												"clarity": "SI1",
												"table": "56",
												"z": "3.58"
											},
											{
												"x": "5.77",
												"y": "5.8",
												"cut": "Ideal",
												"price": "2804",
												"color": "D",
												"carat": "0.72",
												"depth": "61.5",
												"clarity": "SI1",
												"table": "54",
												"z": "3.56"
											},
											{
												"x": "5.93",
												"y": "5.84",
												"cut": "Fair",
												"price": "2804",
												"color": "I",
												"carat": "0.9",
												"depth": "67.3",
												"clarity": "SI1",
												"table": "59",
												"z": "3.96"
											},
											{
												"x": "5.85",
												"y": "5.78",
												"cut": "Premium",
												"price": "2805",
												"color": "F",
												"carat": "0.74",
												"depth": "61.7",
												"clarity": "VS2",
												"table": "58",
												"z": "3.59"
											},
											{
												"x": "5.8",
												"y": "5.77",
												"cut": "Premium",
												"price": "2805",
												"color": "F",
												"carat": "0.74",
												"depth": "61.9",
												"clarity": "VS2",
												"table": "56",
												"z": "3.58"
											},
											{
												"x": "5.77",
												"y": "5.81",
												"cut": "Ideal",
												"price": "2805",
												"color": "E",
												"carat": "0.73",
												"depth": "61.8",
												"clarity": "SI2",
												"table": "58",
												"z": "3.58"
											},
											{
												"x": "5.34",
												"y": "5.43",
												"cut": "Fair",
												"price": "2805",
												"color": "E",
												"carat": "0.57",
												"depth": "58.7",
												"clarity": "VVS1",
												"table": "66",
												"z": "3.16"
											},
											{
												"x": "5.75",
												"y": "5.7",
												"cut": "Premium",
												"price": "2805",
												"color": "F",
												"carat": "0.73",
												"depth": "62.5",
												"clarity": "VS2",
												"table": "57",
												"z": "3.58"
											},
											{
												"x": "5.74",
												"y": "5.7",
												"cut": "Ideal",
												"price": "2805",
												"color": "G",
												"carat": "0.72",
												"depth": "62.8",
												"clarity": "VS2",
												"table": "56",
												"z": "3.59"
											},
											{
												"x": "5.82",
												"y": "5.75",
												"cut": "Fair",
												"price": "2805",
												"color": "F",
												"carat": "0.74",
												"depth": "61.1",
												"clarity": "VS2",
												"table": "68",
												"z": "3.53"
											},
											{
												"x": "5.92",
												"y": "5.89",
												"cut": "Good",
												"price": "2805",
												"color": "G",
												"carat": "0.82",
												"depth": "64",
												"clarity": "VS2",
												"table": "57",
												"z": "3.78"
											},
											{
												"x": "5.89",
												"y": "5.94",
												"cut": "Very Good",
												"price": "2806",
												"color": "G",
												"carat": "0.81",
												"depth": "62.5",
												"clarity": "SI1",
												"table": "60",
												"z": "3.69"
											},
											{
												"x": "5.85",
												"y": "5.9",
												"cut": "Very Good",
												"price": "2806",
												"color": "H",
												"carat": "0.75",
												"depth": "60.6",
												"clarity": "VVS1",
												"table": "58",
												"z": "3.56"
											},
											{
												"x": "5.72",
												"y": "5.74",
												"cut": "Ideal",
												"price": "2806",
												"color": "F",
												"carat": "0.7",
												"depth": "61.6",
												"clarity": "SI1",
												"table": "55",
												"z": "3.53"
											},
											{
												"x": "5.66",
												"y": "5.72",
												"cut": "Very Good",
												"price": "2807",
												"color": "F",
												"carat": "0.71",
												"depth": "62.2",
												"clarity": "VS1",
												"table": "58",
												"z": "3.54"
											},
											{
												"x": "5.84",
												"y": "5.9",
												"cut": "Very Good",
												"price": "2807",
												"color": "F",
												"carat": "0.71",
												"depth": "60",
												"clarity": "VS1",
												"table": "57",
												"z": "3.52"
											},
											{
												"x": "6.21",
												"y": "6.19",
												"cut": "Premium",
												"price": "2807",
												"color": "J",
												"carat": "0.93",
												"depth": "61.9",
												"clarity": "SI2",
												"table": "57",
												"z": "3.84"
											},
											{
												"x": "5.87",
												"y": "5.91",
												"cut": "Very Good",
												"price": "2808",
												"color": "H",
												"carat": "0.8",
												"depth": "62.8",
												"clarity": "VS2",
												"table": "57",
												"z": "3.7"
											},
											{
												"x": "5.64",
												"y": "5.71",
												"cut": "Very Good",
												"price": "2808",
												"color": "F",
												"carat": "0.7",
												"depth": "62",
												"clarity": "VS1",
												"table": "57",
												"z": "3.52"
											},
											{
												"x": "6.16",
												"y": "6.09",
												"cut": "Fair",
												"price": "2808",
												"color": "G",
												"carat": "1",
												"depth": "66.4",
												"clarity": "I1",
												"table": "59",
												"z": "4.07"
											},
											{
												"x": "5.78",
												"y": "5.74",
												"cut": "Very Good",
												"price": "2808",
												"color": "G",
												"carat": "0.75",
												"depth": "63.4",
												"clarity": "VS2",
												"table": "56",
												"z": "3.65"
											},
											{
												"x": "5.41",
												"y": "5.43",
												"cut": "Ideal",
												"price": "2808",
												"color": "E",
												"carat": "0.58",
												"depth": "60.9",
												"clarity": "VVS2",
												"table": "56",
												"z": "3.3"
											},
											{
												"x": "5.74",
												"y": "5.7",
												"cut": "Very Good",
												"price": "2808",
												"color": "D",
												"carat": "0.73",
												"depth": "63.1",
												"clarity": "SI1",
												"table": "57",
												"z": "3.61"
											},
											{
												"x": "5.85",
												"y": "5.79",
												"cut": "Very Good",
												"price": "2809",
												"color": "F",
												"carat": "0.81",
												"depth": "63.1",
												"clarity": "SI1",
												"table": "59",
												"z": "3.67"
											},
											{
												"x": "6.15",
												"y": "6.05",
												"cut": "Premium",
												"price": "2809",
												"color": "D",
												"carat": "0.81",
												"depth": "59.2",
												"clarity": "SI2",
												"table": "57",
												"z": "3.61"
											},
											{
												"x": "5.84",
												"y": "5.8",
												"cut": "Premium",
												"price": "2809",
												"color": "F",
												"carat": "0.71",
												"depth": "60.7",
												"clarity": "SI1",
												"table": "54",
												"z": "3.53"
											},
											{
												"x": "6.73",
												"y": "6.66",
												"cut": "Fair",
												"price": "2809",
												"color": "F",
												"carat": "1.2",
												"depth": "64.6",
												"clarity": "I1",
												"table": "56",
												"z": "4.33"
											},
											{
												"x": "5.63",
												"y": "5.7",
												"cut": "Very Good",
												"price": "2810",
												"color": "F",
												"carat": "0.7",
												"depth": "61.8",
												"clarity": "VS1",
												"table": "56",
												"z": "3.5"
											},
											{
												"x": "5.77",
												"y": "5.84",
												"cut": "Very Good",
												"price": "2810",
												"color": "F",
												"carat": "0.7",
												"depth": "59.9",
												"clarity": "VS1",
												"table": "60",
												"z": "3.48"
											},
											{
												"x": "5.81",
												"y": "5.85",
												"cut": "Ideal",
												"price": "2810",
												"color": "D",
												"carat": "0.74",
												"depth": "61.7",
												"clarity": "SI2",
												"table": "55",
												"z": "3.6"
											},
											{
												"x": "5.57",
												"y": "5.61",
												"cut": "Good",
												"price": "2810",
												"color": "F",
												"carat": "0.7",
												"depth": "62.8",
												"clarity": "VS1",
												"table": "61",
												"z": "3.51"
											},
											{
												"x": "5.84",
												"y": "5.93",
												"cut": "Good",
												"price": "2810",
												"color": "G",
												"carat": "0.8",
												"depth": "62.7",
												"clarity": "SI1",
												"table": "57",
												"z": "3.69"
											},
											{
												"x": "5.72",
												"y": "5.76",
												"cut": "Very Good",
												"price": "2811",
												"color": "F",
												"carat": "0.75",
												"depth": "63.4",
												"clarity": "SI1",
												"table": "58",
												"z": "3.64"
											},
											{
												"x": "5.98",
												"y": "5.95",
												"cut": "Very Good",
												"price": "2811",
												"color": "D",
												"carat": "0.83",
												"depth": "63.5",
												"clarity": "SI1",
												"table": "54",
												"z": "3.79"
											},
											{
												"x": "6.14",
												"y": "6.07",
												"cut": "Fair",
												"price": "2811",
												"color": "J",
												"carat": "1",
												"depth": "65.7",
												"clarity": "VS2",
												"table": "59",
												"z": "4.01"
											},
											{
												"x": "6.21",
												"y": "6.06",
												"cut": "Fair",
												"price": "2811",
												"color": "I",
												"carat": "0.99",
												"depth": "68.1",
												"clarity": "SI2",
												"table": "56",
												"z": "4.18"
											},
											{
												"x": "5.57",
												"y": "5.64",
												"cut": "Very Good",
												"price": "2812",
												"color": "G",
												"carat": "0.7",
												"depth": "63",
												"clarity": "VS1",
												"table": "60",
												"z": "3.53"
											},
											{
												"x": "5.75",
												"y": "5.85",
												"cut": "Very Good",
												"price": "2812",
												"color": "F",
												"carat": "0.7",
												"depth": "59.5",
												"clarity": "VS2",
												"table": "58",
												"z": "3.45"
											},
											{
												"x": "5.49",
												"y": "5.53",
												"cut": "Good",
												"price": "2812",
												"color": "E",
												"carat": "0.7",
												"depth": "63.5",
												"clarity": "SI1",
												"table": "59",
												"z": "3.5"
											},
											{
												"x": "5.63",
												"y": "5.69",
												"cut": "Very Good",
												"price": "2812",
												"color": "F",
												"carat": "0.7",
												"depth": "61.7",
												"clarity": "VS2",
												"table": "58",
												"z": "3.49"
											},
											{
												"x": "4.37",
												"y": "4.34",
												"cut": "Premium",
												"price": "554",
												"color": "I",
												"carat": "0.32",
												"depth": "62.7",
												"clarity": "SI1",
												"table": "58",
												"z": "2.73"
											},
											{
												"x": "4.39",
												"y": "4.34",
												"cut": "Premium",
												"price": "554",
												"color": "I",
												"carat": "0.32",
												"depth": "62.8",
												"clarity": "SI1",
												"table": "58",
												"z": "2.74"
											},
											{
												"x": "4.37",
												"y": "4.35",
												"cut": "Ideal",
												"price": "554",
												"color": "I",
												"carat": "0.32",
												"depth": "62.4",
												"clarity": "SI1",
												"table": "57",
												"z": "2.72"
											},
											{
												"x": "4.39",
												"y": "4.36",
												"cut": "Premium",
												"price": "554",
												"color": "I",
												"carat": "0.32",
												"depth": "61",
												"clarity": "SI1",
												"table": "59",
												"z": "2.67"
											},
											{
												"x": "4.39",
												"y": "4.36",
												"cut": "Very Good",
												"price": "554",
												"color": "I",
												"carat": "0.32",
												"depth": "63.1",
												"clarity": "SI1",
												"table": "56",
												"z": "2.76"
											},
											{
												"x": "4.47",
												"y": "4.42",
												"cut": "Ideal",
												"price": "554",
												"color": "I",
												"carat": "0.32",
												"depth": "60.7",
												"clarity": "SI1",
												"table": "57",
												"z": "2.7"
											},
											{
												"x": "4.31",
												"y": "4.29",
												"cut": "Premium",
												"price": "554",
												"color": "H",
												"carat": "0.3",
												"depth": "60.9",
												"clarity": "SI1",
												"table": "59",
												"z": "2.62"
											},
											{
												"x": "4.41",
												"y": "4.38",
												"cut": "Premium",
												"price": "554",
												"color": "H",
												"carat": "0.3",
												"depth": "60.1",
												"clarity": "SI1",
												"table": "55",
												"z": "2.64"
											},
											{
												"x": "4.28",
												"y": "4.24",
												"cut": "Premium",
												"price": "554",
												"color": "H",
												"carat": "0.3",
												"depth": "62.9",
												"clarity": "SI1",
												"table": "58",
												"z": "2.68"
											},
											{
												"x": "4.29",
												"y": "4.27",
												"cut": "Very Good",
												"price": "554",
												"color": "H",
												"carat": "0.3",
												"depth": "63.3",
												"clarity": "SI1",
												"table": "56",
												"z": "2.71"
											},
											{
												"x": "4.26",
												"y": "4.2",
												"cut": "Good",
												"price": "554",
												"color": "H",
												"carat": "0.3",
												"depth": "63.8",
												"clarity": "SI1",
												"table": "55",
												"z": "2.7"
											},
											{
												"x": "4.27",
												"y": "4.22",
												"cut": "Ideal",
												"price": "554",
												"color": "H",
												"carat": "0.3",
												"depth": "62.9",
												"clarity": "SI1",
												"table": "57",
												"z": "2.67"
											},
											{
												"x": "4.25",
												"y": "4.23",
												"cut": "Very Good",
												"price": "554",
												"color": "H",
												"carat": "0.3",
												"depth": "63.4",
												"clarity": "SI1",
												"table": "60",
												"z": "2.69"
											},
											{
												"x": "4.36",
												"y": "4.34",
												"cut": "Good",
												"price": "554",
												"color": "I",
												"carat": "0.32",
												"depth": "63.9",
												"clarity": "SI1",
												"table": "55",
												"z": "2.78"
											},
											{
												"x": "4.85",
												"y": "4.79",
												"cut": "Ideal",
												"price": "554",
												"color": "H",
												"carat": "0.33",
												"depth": "61.4",
												"clarity": "SI2",
												"table": "56",
												"z": "2.95"
											},
											{
												"x": "4.28",
												"y": "4.33",
												"cut": "Very Good",
												"price": "555",
												"color": "E",
												"carat": "0.29",
												"depth": "61.9",
												"clarity": "VS1",
												"table": "55",
												"z": "2.66"
											},
											{
												"x": "4.2",
												"y": "4.25",
												"cut": "Very Good",
												"price": "555",
												"color": "E",
												"carat": "0.29",
												"depth": "62.4",
												"clarity": "VS1",
												"table": "55",
												"z": "2.63"
											},
											{
												"x": "4.32",
												"y": "4.35",
												"cut": "Very Good",
												"price": "555",
												"color": "F",
												"carat": "0.31",
												"depth": "61.8",
												"clarity": "SI1",
												"table": "58",
												"z": "2.68"
											},
											{
												"x": "4.47",
												"y": "4.5",
												"cut": "Ideal",
												"price": "555",
												"color": "H",
												"carat": "0.34",
												"depth": "61.5",
												"clarity": "VS2",
												"table": "56",
												"z": "2.76"
											},
											{
												"x": "4.54",
												"y": "4.57",
												"cut": "Ideal",
												"price": "555",
												"color": "H",
												"carat": "0.34",
												"depth": "60.4",
												"clarity": "VS2",
												"table": "57",
												"z": "2.75"
											},
											{
												"x": "4.48",
												"y": "4.52",
												"cut": "Ideal",
												"price": "555",
												"color": "I",
												"carat": "0.34",
												"depth": "61.8",
												"clarity": "VS1",
												"table": "55",
												"z": "2.78"
											},
											{
												"x": "4.5",
												"y": "4.53",
												"cut": "Ideal",
												"price": "555",
												"color": "I",
												"carat": "0.34",
												"depth": "62",
												"clarity": "VS1",
												"table": "56",
												"z": "2.8"
											},
											{
												"x": "4.29",
												"y": "4.31",
												"cut": "Ideal",
												"price": "555",
												"color": "G",
												"carat": "0.3",
												"depth": "62.3",
												"clarity": "VS1",
												"table": "56",
												"z": "2.68"
											},
											{
												"x": "4.26",
												"y": "4.31",
												"cut": "Ideal",
												"price": "555",
												"color": "F",
												"carat": "0.29",
												"depth": "61.6",
												"clarity": "VS1",
												"table": "56",
												"z": "2.64"
											},
											{
												"x": "4.56",
												"y": "4.58",
												"cut": "Ideal",
												"price": "555",
												"color": "G",
												"carat": "0.35",
												"depth": "60.6",
												"clarity": "SI1",
												"table": "56",
												"z": "2.77"
											},
											{
												"x": "4.94",
												"y": "5",
												"cut": "Very Good",
												"price": "555",
												"color": "E",
												"carat": "0.43",
												"depth": "58.4",
												"clarity": "I1",
												"table": "62",
												"z": "2.9"
											},
											{
												"x": "4.37",
												"y": "4.42",
												"cut": "Very Good",
												"price": "556",
												"color": "F",
												"carat": "0.32",
												"depth": "61.4",
												"clarity": "VS2",
												"table": "58",
												"z": "2.7"
											},
											{
												"x": "4.54",
												"y": "4.57",
												"cut": "Ideal",
												"price": "556",
												"color": "I",
												"carat": "0.36",
												"depth": "61.9",
												"clarity": "VS2",
												"table": "56",
												"z": "2.82"
											},
											{
												"x": "4.28",
												"y": "4.3",
												"cut": "Ideal",
												"price": "556",
												"color": "G",
												"carat": "0.3",
												"depth": "62",
												"clarity": "VS2",
												"table": "56",
												"z": "2.66"
											},
											{
												"x": "4.09",
												"y": "4.12",
												"cut": "Ideal",
												"price": "556",
												"color": "E",
												"carat": "0.26",
												"depth": "61.5",
												"clarity": "VS1",
												"table": "57",
												"z": "2.52"
											},
											{
												"x": "5.64",
												"y": "5.72",
												"cut": "Very Good",
												"price": "2812",
												"color": "F",
												"carat": "0.7",
												"depth": "62.3",
												"clarity": "VS2",
												"table": "58",
												"z": "3.54"
											},
											{
												"x": "5.66",
												"y": "5.71",
												"cut": "Very Good",
												"price": "2812",
												"color": "F",
												"carat": "0.7",
												"depth": "60.9",
												"clarity": "VS2",
												"table": "61",
												"z": "3.46"
											},
											{
												"x": "5.69",
												"y": "5.72",
												"cut": "Ideal",
												"price": "2812",
												"color": "D",
												"carat": "0.71",
												"depth": "62.4",
												"clarity": "SI1",
												"table": "57",
												"z": "3.56"
											},
											{
												"x": "6.72",
												"y": "6.67",
												"cut": "Fair",
												"price": "2812",
												"color": "J",
												"carat": "0.99",
												"depth": "55",
												"clarity": "SI1",
												"table": "61",
												"z": "3.68"
											},
											{
												"x": "5.92",
												"y": "5.89",
												"cut": "Premium",
												"price": "2812",
												"color": "E",
												"carat": "0.73",
												"depth": "58.6",
												"clarity": "VS2",
												"table": "60",
												"z": "3.46"
											},
											{
												"x": "5.15",
												"y": "5.11",
												"cut": "Ideal",
												"price": "2812",
												"color": "F",
												"carat": "0.51",
												"depth": "62",
												"clarity": "VVS1",
												"table": "57",
												"z": "3.18"
											},
											{
												"x": "6.3",
												"y": "6.29",
												"cut": "Premium",
												"price": "2813",
												"color": "G",
												"carat": "0.91",
												"depth": "59.8",
												"clarity": "SI2",
												"table": "58",
												"z": "3.77"
											},
											{
												"x": "6",
												"y": "5.95",
												"cut": "Very Good",
												"price": "2813",
												"color": "E",
												"carat": "0.84",
												"depth": "63.4",
												"clarity": "SI1",
												"table": "55",
												"z": "3.79"
											},
											{
												"x": "6.09",
												"y": "6.05",
												"cut": "Good",
												"price": "2813",
												"color": "I",
												"carat": "0.91",
												"depth": "64.3",
												"clarity": "VS2",
												"table": "58",
												"z": "3.9"
											},
											{
												"x": "5.86",
												"y": "5.81",
												"cut": "Premium",
												"price": "2814",
												"color": "E",
												"carat": "0.76",
												"depth": "62.2",
												"clarity": "SI1",
												"table": "59",
												"z": "3.63"
											},
											{
												"x": "5.88",
												"y": "5.85",
												"cut": "Ideal",
												"price": "2814",
												"color": "E",
												"carat": "0.76",
												"depth": "61.7",
												"clarity": "SI1",
												"table": "57",
												"z": "3.62"
											},
											{
												"x": "5.86",
												"y": "5.83",
												"cut": "Premium",
												"price": "2814",
												"color": "E",
												"carat": "0.75",
												"depth": "61.1",
												"clarity": "SI1",
												"table": "59",
												"z": "3.57"
											},
											{
												"x": "5.23",
												"y": "5.27",
												"cut": "Very Good",
												"price": "2815",
												"color": "D",
												"carat": "0.55",
												"depth": "61.5",
												"clarity": "VVS1",
												"table": "56",
												"z": "3.23"
											},
											{
												"x": "5.93",
												"y": "6.01",
												"cut": "Very Good",
												"price": "2815",
												"color": "F",
												"carat": "0.76",
												"depth": "58.5",
												"clarity": "SI2",
												"table": "62",
												"z": "3.49"
											},
											{
												"x": "5.79",
												"y": "5.81",
												"cut": "Premium",
												"price": "2815",
												"color": "G",
												"carat": "0.74",
												"depth": "61.7",
												"clarity": "VS1",
												"table": "58",
												"z": "3.58"
											},
											{
												"x": "5.75",
												"y": "5.81",
												"cut": "Ideal",
												"price": "2815",
												"color": "H",
												"carat": "0.7",
												"depth": "60.4",
												"clarity": "SI1",
												"table": "56",
												"z": "3.49"
											},
											{
												"x": "5.7",
												"y": "5.76",
												"cut": "Ideal",
												"price": "2815",
												"color": "H",
												"carat": "0.7",
												"depth": "61.4",
												"clarity": "SI1",
												"table": "56",
												"z": "3.52"
											},
											{
												"x": "5.73",
												"y": "5.79",
												"cut": "Ideal",
												"price": "2815",
												"color": "H",
												"carat": "0.7",
												"depth": "61.5",
												"clarity": "SI1",
												"table": "55",
												"z": "3.54"
											},
											{
												"x": "5.72",
												"y": "5.77",
												"cut": "Ideal",
												"price": "2815",
												"color": "H",
												"carat": "0.7",
												"depth": "61.4",
												"clarity": "SI1",
												"table": "56",
												"z": "3.53"
											},
											{
												"x": "6.08",
												"y": "6.04",
												"cut": "Fair",
												"price": "2815",
												"color": "J",
												"carat": "0.9",
												"depth": "65",
												"clarity": "VS2",
												"table": "56",
												"z": "3.94"
											},
											{
												"x": "6.62",
												"y": "6.53",
												"cut": "Fair",
												"price": "2815",
												"color": "F",
												"carat": "0.95",
												"depth": "56",
												"clarity": "SI2",
												"table": "60",
												"z": "3.68"
											},
											{
												"x": "6.26",
												"y": "6.23",
												"cut": "Premium",
												"price": "2815",
												"color": "H",
												"carat": "0.89",
												"depth": "60.2",
												"clarity": "SI2",
												"table": "59",
												"z": "3.76"
											},
											{
												"x": "5.99",
												"y": "5.92",
												"cut": "Premium",
												"price": "2815",
												"color": "E",
												"carat": "0.72",
												"depth": "58.3",
												"clarity": "VS2",
												"table": "58",
												"z": "3.47"
											},
											{
												"x": "6.73",
												"y": "6.65",
												"cut": "Fair",
												"price": "2815",
												"color": "E",
												"carat": "0.96",
												"depth": "53.1",
												"clarity": "SI2",
												"table": "63",
												"z": "3.55"
											},
											{
												"x": "6.55",
												"y": "6.5",
												"cut": "Premium",
												"price": "2815",
												"color": "G",
												"carat": "1.02",
												"depth": "60.3",
												"clarity": "I1",
												"table": "58",
												"z": "3.94"
											},
											{
												"x": "5.91",
												"y": "5.95",
												"cut": "Very Good",
												"price": "2816",
												"color": "I",
												"carat": "0.78",
												"depth": "61.4",
												"clarity": "VVS2",
												"table": "56",
												"z": "3.64"
											},
											{
												"x": "5.52",
												"y": "5.54",
												"cut": "Ideal",
												"price": "2816",
												"color": "G",
												"carat": "0.61",
												"depth": "60.1",
												"clarity": "VVS2",
												"table": "57",
												"z": "3.32"
											},
											{
												"x": "5.61",
												"y": "5.69",
												"cut": "Good",
												"price": "2816",
												"color": "D",
												"carat": "0.71",
												"depth": "63.4",
												"clarity": "VS1",
												"table": "55",
												"z": "3.58"
											},
											{
												"x": "5.96",
												"y": "5.88",
												"cut": "Premium",
												"price": "2816",
												"color": "F",
												"carat": "0.78",
												"depth": "61.5",
												"clarity": "SI1",
												"table": "59",
												"z": "3.64"
											},
											{
												"x": "6.16",
												"y": "6.13",
												"cut": "Ideal",
												"price": "2816",
												"color": "H",
												"carat": "0.87",
												"depth": "62.7",
												"clarity": "SI2",
												"table": "56",
												"z": "3.85"
											},
											{
												"x": "6.04",
												"y": "6",
												"cut": "Ideal",
												"price": "2816",
												"color": "H",
												"carat": "0.83",
												"depth": "62.5",
												"clarity": "SI1",
												"table": "55",
												"z": "3.76"
											},
											{
												"x": "5.78",
												"y": "5.73",
												"cut": "Premium",
												"price": "2817",
												"color": "E",
												"carat": "0.71",
												"depth": "61.3",
												"clarity": "SI1",
												"table": "56",
												"z": "3.53"
											},
											{
												"x": "5.84",
												"y": "5.89",
												"cut": "Ideal",
												"price": "2817",
												"color": "I",
												"carat": "0.71",
												"depth": "60.2",
												"clarity": "VVS2",
												"table": "56",
												"z": "3.53"
											},
											{
												"x": "5.66",
												"y": "5.64",
												"cut": "Ideal",
												"price": "2817",
												"color": "E",
												"carat": "0.71",
												"depth": "62.7",
												"clarity": "VS2",
												"table": "57",
												"z": "3.54"
											},
											{
												"x": "5.69",
												"y": "5.65",
												"cut": "Premium",
												"price": "2817",
												"color": "E",
												"carat": "0.71",
												"depth": "62.3",
												"clarity": "VS2",
												"table": "58",
												"z": "3.53"
											},
											{
												"x": "5.48",
												"y": "5.52",
												"cut": "Ideal",
												"price": "2817",
												"color": "F",
												"carat": "0.63",
												"depth": "61.5",
												"clarity": "VVS2",
												"table": "56",
												"z": "3.38"
											},
											{
												"x": "5.86",
												"y": "5.83",
												"cut": "Premium",
												"price": "2817",
												"color": "E",
												"carat": "0.71",
												"depth": "59.2",
												"clarity": "SI1",
												"table": "59",
												"z": "3.46"
											},
											{
												"x": "5.75",
												"y": "5.7",
												"cut": "Premium",
												"price": "2817",
												"color": "E",
												"carat": "0.71",
												"depth": "61.8",
												"clarity": "SI1",
												"table": "59",
												"z": "3.54"
											},
											{
												"x": "5.77",
												"y": "5.72",
												"cut": "Ideal",
												"price": "2817",
												"color": "E",
												"carat": "0.71",
												"depth": "61.3",
												"clarity": "SI1",
												"table": "55",
												"z": "3.52"
											},
											{
												"x": "5.77",
												"y": "5.73",
												"cut": "Premium",
												"price": "2817",
												"color": "E",
												"carat": "0.71",
												"depth": "61.4",
												"clarity": "SI1",
												"table": "58",
												"z": "3.53"
											},
											{
												"x": "6.2",
												"y": "6.16",
												"cut": "Ideal",
												"price": "2817",
												"color": "J",
												"carat": "0.9",
												"depth": "62.8",
												"clarity": "VS2",
												"table": "55",
												"z": "3.88"
											},
											{
												"x": "5.6",
												"y": "5.54",
												"cut": "Good",
												"price": "2817",
												"color": "E",
												"carat": "0.71",
												"depth": "62.8",
												"clarity": "SI1",
												"table": "64",
												"z": "3.5"
											},
											{
												"x": "5.66",
												"y": "5.63",
												"cut": "Premium",
												"price": "2818",
												"color": "E",
												"carat": "0.7",
												"depth": "62.4",
												"clarity": "VS2",
												"table": "61",
												"z": "3.52"
											},
											{
												"x": "5.78",
												"y": "5.73",
												"cut": "Premium",
												"price": "2818",
												"color": "E",
												"carat": "0.7",
												"depth": "59.3",
												"clarity": "VS2",
												"table": "60",
												"z": "3.41"
											},
											{
												"x": "5.64",
												"y": "5.6",
												"cut": "Premium",
												"price": "2818",
												"color": "E",
												"carat": "0.7",
												"depth": "63",
												"clarity": "VS2",
												"table": "60",
												"z": "3.54"
											},
											{
												"x": "6.43",
												"y": "6.39",
												"cut": "Premium",
												"price": "2818",
												"color": "H",
												"carat": "1",
												"depth": "61.3",
												"clarity": "I1",
												"table": "60",
												"z": "3.93"
											},
											{
												"x": "6.36",
												"y": "6.22",
												"cut": "Premium",
												"price": "2818",
												"color": "F",
												"carat": "0.86",
												"depth": "59.3",
												"clarity": "SI2",
												"table": "62",
												"z": "3.73"
											},
											{
												"x": "6.07",
												"y": "6",
												"cut": "Ideal",
												"price": "2818",
												"color": "H",
												"carat": "0.8",
												"depth": "61",
												"clarity": "SI1",
												"table": "57",
												"z": "3.68"
											},
											{
												"x": "5.66",
												"y": "5.61",
												"cut": "Ideal",
												"price": "2818",
												"color": "E",
												"carat": "0.7",
												"depth": "62.9",
												"clarity": "VS1",
												"table": "57",
												"z": "3.54"
											},
											{
												"x": "5.91",
												"y": "5.83",
												"cut": "Premium",
												"price": "2818",
												"color": "E",
												"carat": "0.7",
												"depth": "59.6",
												"clarity": "VS1",
												"table": "57",
												"z": "3.5"
											},
											{
												"x": "5.69",
												"y": "5.64",
												"cut": "Premium",
												"price": "2818",
												"color": "F",
												"carat": "0.7",
												"depth": "61.8",
												"clarity": "VS2",
												"table": "60",
												"z": "3.5"
											},
											{
												"x": "5.68",
												"y": "5.64",
												"cut": "Premium",
												"price": "2818",
												"color": "E",
												"carat": "0.7",
												"depth": "62.7",
												"clarity": "VS1",
												"table": "57",
												"z": "3.55"
											},
											{
												"x": "6.34",
												"y": "6.12",
												"cut": "Fair",
												"price": "2818",
												"color": "H",
												"carat": "1",
												"depth": "65.3",
												"clarity": "SI2",
												"table": "62",
												"z": "4.08"
											},
											{
												"x": "5.64",
												"y": "5.68",
												"cut": "Very Good",
												"price": "2819",
												"color": "G",
												"carat": "0.72",
												"depth": "63.8",
												"clarity": "VS1",
												"table": "58",
												"z": "3.61"
											},
											{
												"x": "5.73",
												"y": "5.77",
												"cut": "Ideal",
												"price": "2819",
												"color": "H",
												"carat": "0.72",
												"depth": "62.3",
												"clarity": "VS1",
												"table": "56",
												"z": "3.58"
											},
											{
												"x": "5.76",
												"y": "5.79",
												"cut": "Good",
												"price": "2819",
												"color": "F",
												"carat": "0.7",
												"depth": "59.7",
												"clarity": "VS1",
												"table": "63",
												"z": "3.45"
											},
											{
												"x": "5.97",
												"y": "5.95",
												"cut": "Good",
												"price": "2819",
												"color": "F",
												"carat": "0.86",
												"depth": "64.3",
												"clarity": "SI2",
												"table": "60",
												"z": "3.83"
											},
											{
												"x": "5.66",
												"y": "5.69",
												"cut": "Ideal",
												"price": "2820",
												"color": "G",
												"carat": "0.71",
												"depth": "62.9",
												"clarity": "VS1",
												"table": "58",
												"z": "3.57"
											},
											{
												"x": "5.8",
												"y": "5.78",
												"cut": "Ideal",
												"price": "2821",
												"color": "E",
												"carat": "0.75",
												"depth": "62",
												"clarity": "SI1",
												"table": "57",
												"z": "3.59"
											},
											{
												"x": "5.77",
												"y": "5.73",
												"cut": "Premium",
												"price": "2821",
												"color": "E",
												"carat": "0.73",
												"depth": "61.6",
												"clarity": "VS2",
												"table": "59",
												"z": "3.54"
											},
											{
												"x": "5.2",
												"y": "5.21",
												"cut": "Ideal",
												"price": "2821",
												"color": "E",
												"carat": "0.53",
												"depth": "61.9",
												"clarity": "VVS1",
												"table": "55",
												"z": "3.22"
											},
											{
												"x": "5.83",
												"y": "5.76",
												"cut": "Premium",
												"price": "2821",
												"color": "E",
												"carat": "0.73",
												"depth": "61.3",
												"clarity": "SI1",
												"table": "58",
												"z": "3.55"
											},
											{
												"x": "5.72",
												"y": "5.7",
												"cut": "Good",
												"price": "2821",
												"color": "E",
												"carat": "0.73",
												"depth": "63.6",
												"clarity": "SI1",
												"table": "57",
												"z": "3.63"
											},
											{
												"x": "5.92",
												"y": "5.85",
												"cut": "Premium",
												"price": "2821",
												"color": "E",
												"carat": "0.73",
												"depth": "59.6",
												"clarity": "SI1",
												"table": "61",
												"z": "3.51"
											},
											{
												"x": "5.77",
												"y": "5.68",
												"cut": "Premium",
												"price": "2821",
												"color": "E",
												"carat": "0.73",
												"depth": "62.2",
												"clarity": "SI1",
												"table": "59",
												"z": "3.56"
											},
											{
												"x": "5.84",
												"y": "5.82",
												"cut": "Premium",
												"price": "2821",
												"color": "D",
												"carat": "0.73",
												"depth": "61.7",
												"clarity": "SI1",
												"table": "55",
												"z": "3.6"
											},
											{
												"x": "5.76",
												"y": "5.7",
												"cut": "Very Good",
												"price": "2821",
												"color": "E",
												"carat": "0.73",
												"depth": "63.2",
												"clarity": "SI1",
												"table": "58",
												"z": "3.62"
											},
											{
												"x": "5.74",
												"y": "5.71",
												"cut": "Premium",
												"price": "2822",
												"color": "E",
												"carat": "0.7",
												"depth": "60.8",
												"clarity": "VS1",
												"table": "60",
												"z": "3.48"
											},
											{
												"x": "5.84",
												"y": "5.8",
												"cut": "Premium",
												"price": "2822",
												"color": "E",
												"carat": "0.72",
												"depth": "60.3",
												"clarity": "VS2",
												"table": "59",
												"z": "3.51"
											},
											{
												"x": "5.8",
												"y": "5.76",
												"cut": "Premium",
												"price": "2822",
												"color": "E",
												"carat": "0.72",
												"depth": "60.9",
												"clarity": "VS2",
												"table": "60",
												"z": "3.52"
											},
											{
												"x": "5.77",
												"y": "5.7",
												"cut": "Premium",
												"price": "2822",
												"color": "E",
												"carat": "0.72",
												"depth": "62.4",
												"clarity": "VS2",
												"table": "59",
												"z": "3.58"
											},
											{
												"x": "5.73",
												"y": "5.7",
												"cut": "Premium",
												"price": "2822",
												"color": "E",
												"carat": "0.7",
												"depth": "60.2",
												"clarity": "VS2",
												"table": "60",
												"z": "3.44"
											},
											{
												"x": "5.37",
												"y": "5.4",
												"cut": "Ideal",
												"price": "2822",
												"color": "F",
												"carat": "0.6",
												"depth": "62",
												"clarity": "VVS2",
												"table": "55",
												"z": "3.34"
											},
											{
												"x": "5.85",
												"y": "5.89",
												"cut": "Ideal",
												"price": "2822",
												"color": "I",
												"carat": "0.74",
												"depth": "60.8",
												"clarity": "VVS1",
												"table": "57",
												"z": "3.57"
											},
											{
												"x": "5.75",
												"y": "5.78",
												"cut": "Ideal",
												"price": "2822",
												"color": "F",
												"carat": "0.73",
												"depth": "62.1",
												"clarity": "SI1",
												"table": "55",
												"z": "3.58"
											},
											{
												"x": "5.71",
												"y": "5.67",
												"cut": "Premium",
												"price": "2822",
												"color": "D",
												"carat": "0.71",
												"depth": "62.7",
												"clarity": "SI1",
												"table": "60",
												"z": "3.57"
											},
											{
												"x": "5.75",
												"y": "5.73",
												"cut": "Premium",
												"price": "2822",
												"color": "D",
												"carat": "0.71",
												"depth": "61.3",
												"clarity": "SI1",
												"table": "58",
												"z": "3.52"
											},
											{
												"x": "5.82",
												"y": "5.75",
												"cut": "Premium",
												"price": "2822",
												"color": "D",
												"carat": "0.7",
												"depth": "60.2",
												"clarity": "SI1",
												"table": "60",
												"z": "3.48"
											},
											{
												"x": "5.75",
												"y": "5.72",
												"cut": "Ideal",
												"price": "2822",
												"color": "D",
												"carat": "0.7",
												"depth": "60.7",
												"clarity": "SI1",
												"table": "56",
												"z": "3.48"
											},
											{
												"x": "6.04",
												"y": "6.03",
												"cut": "Good",
												"price": "2822",
												"color": "J",
												"carat": "0.9",
												"depth": "64",
												"clarity": "VS2",
												"table": "61",
												"z": "3.86"
											},
											{
												"x": "5.86",
												"y": "5.83",
												"cut": "Ideal",
												"price": "2822",
												"color": "D",
												"carat": "0.71",
												"depth": "60.2",
												"clarity": "SI1",
												"table": "56",
												"z": "3.52"
											},
											{
												"x": "5.73",
												"y": "5.68",
												"cut": "Premium",
												"price": "2822",
												"color": "E",
												"carat": "0.7",
												"depth": "61.5",
												"clarity": "VS2",
												"table": "59",
												"z": "3.51"
											},
											{
												"x": "5.71",
												"y": "5.66",
												"cut": "Premium",
												"price": "2822",
												"color": "E",
												"carat": "0.7",
												"depth": "62.6",
												"clarity": "VS2",
												"table": "56",
												"z": "3.56"
											},
											{
												"x": "5.82",
												"y": "5.77",
												"cut": "Ideal",
												"price": "2822",
												"color": "D",
												"carat": "0.7",
												"depth": "59.7",
												"clarity": "SI1",
												"table": "58",
												"z": "3.46"
											},
											{
												"x": "5.71",
												"y": "5.66",
												"cut": "Good",
												"price": "2822",
												"color": "E",
												"carat": "0.7",
												"depth": "61.4",
												"clarity": "SI1",
												"table": "64",
												"z": "3.49"
											},
											{
												"x": "5.62",
												"y": "5.59",
												"cut": "Ideal",
												"price": "2822",
												"color": "D",
												"carat": "0.7",
												"depth": "62.5",
												"clarity": "SI1",
												"table": "57",
												"z": "3.51"
											},
											{
												"x": "5.73",
												"y": "5.63",
												"cut": "Ideal",
												"price": "2822",
												"color": "D",
												"carat": "0.7",
												"depth": "61.8",
												"clarity": "SI1",
												"table": "56",
												"z": "3.51"
											},
											{
												"x": "5.72",
												"y": "5.68",
												"cut": "Premium",
												"price": "2822",
												"color": "E",
												"carat": "0.7",
												"depth": "60.7",
												"clarity": "VS2",
												"table": "62",
												"z": "3.46"
											},
											{
												"x": "5.8",
												"y": "5.72",
												"cut": "Premium",
												"price": "2822",
												"color": "F",
												"carat": "0.7",
												"depth": "60.6",
												"clarity": "VS2",
												"table": "58",
												"z": "3.49"
											},
											{
												"x": "5.75",
												"y": "5.71",
												"cut": "Ideal",
												"price": "2822",
												"color": "D",
												"carat": "0.7",
												"depth": "61.4",
												"clarity": "SI1",
												"table": "54",
												"z": "3.52"
											},
											{
												"x": "5.86",
												"y": "5.9",
												"cut": "Very Good",
												"price": "2823",
												"color": "D",
												"carat": "0.79",
												"depth": "62.8",
												"clarity": "SI2",
												"table": "59",
												"z": "3.69"
											},
											{
												"x": "6.06",
												"y": "6.13",
												"cut": "Good",
												"price": "2823",
												"color": "I",
												"carat": "0.9",
												"depth": "63.8",
												"clarity": "SI1",
												"table": "57",
												"z": "3.89"
											},
											{
												"x": "5.71",
												"y": "5.66",
												"cut": "Premium",
												"price": "2823",
												"color": "E",
												"carat": "0.71",
												"depth": "62.3",
												"clarity": "VS2",
												"table": "58",
												"z": "3.54"
											},
											{
												"x": "5.51",
												"y": "5.59",
												"cut": "Ideal",
												"price": "2823",
												"color": "E",
												"carat": "0.61",
												"depth": "61.3",
												"clarity": "VVS2",
												"table": "54",
												"z": "3.4"
											},
											{
												"x": "6.05",
												"y": "5.98",
												"cut": "Fair",
												"price": "2823",
												"color": "H",
												"carat": "0.9",
												"depth": "65.8",
												"clarity": "SI2",
												"table": "54",
												"z": "3.96"
											},
											{
												"x": "5.77",
												"y": "5.73",
												"cut": "Ideal",
												"price": "2823",
												"color": "E",
												"carat": "0.71",
												"depth": "60.5",
												"clarity": "SI1",
												"table": "56",
												"z": "3.47"
											},
											{
												"x": "5.74",
												"y": "5.69",
												"cut": "Premium",
												"price": "2824",
												"color": "D",
												"carat": "0.71",
												"depth": "61.2",
												"clarity": "VS2",
												"table": "59",
												"z": "3.5"
											},
											{
												"x": "5.84",
												"y": "5.86",
												"cut": "Ideal",
												"price": "2824",
												"color": "I",
												"carat": "0.77",
												"depth": "62.1",
												"clarity": "VVS2",
												"table": "57",
												"z": "3.63"
											},
											{
												"x": "5.73",
												"y": "5.75",
												"cut": "Good",
												"price": "2824",
												"color": "E",
												"carat": "0.74",
												"depth": "63.1",
												"clarity": "VS1",
												"table": "58",
												"z": "3.62"
											},
											{
												"x": "6.02",
												"y": "5.97",
												"cut": "Ideal",
												"price": "2824",
												"color": "F",
												"carat": "0.82",
												"depth": "62.4",
												"clarity": "SI2",
												"table": "54",
												"z": "3.74"
											},
											{
												"x": "6.05",
												"y": "6.03",
												"cut": "Premium",
												"price": "2824",
												"color": "E",
												"carat": "0.82",
												"depth": "60.8",
												"clarity": "SI2",
												"table": "60",
												"z": "3.67"
											},
											{
												"x": "5.73",
												"y": "5.66",
												"cut": "Premium",
												"price": "2825",
												"color": "G",
												"carat": "0.71",
												"depth": "62.2",
												"clarity": "VS1",
												"table": "59",
												"z": "3.54"
											},
											{
												"x": "6.08",
												"y": "6.05",
												"cut": "Premium",
												"price": "2825",
												"color": "H",
												"carat": "0.83",
												"depth": "60",
												"clarity": "SI1",
												"table": "59",
												"z": "3.64"
											},
											{
												"x": "5.75",
												"y": "5.8",
												"cut": "Very Good",
												"price": "2825",
												"color": "G",
												"carat": "0.73",
												"depth": "62",
												"clarity": "VS1",
												"table": "57",
												"z": "3.58"
											},
											{
												"x": "6.02",
												"y": "5.95",
												"cut": "Premium",
												"price": "2825",
												"color": "H",
												"carat": "0.83",
												"depth": "62.5",
												"clarity": "SI1",
												"table": "59",
												"z": "3.74"
											},
											{
												"x": "6.9",
												"y": "6.83",
												"cut": "Premium",
												"price": "2825",
												"color": "J",
												"carat": "1.17",
												"depth": "60.2",
												"clarity": "I1",
												"table": "61",
												"z": "4.13"
											},
											{
												"x": "6.24",
												"y": "6.19",
												"cut": "Fair",
												"price": "2825",
												"color": "H",
												"carat": "0.91",
												"depth": "61.3",
												"clarity": "SI2",
												"table": "67",
												"z": "3.81"
											},
											{
												"x": "5.75",
												"y": "5.68",
												"cut": "Premium",
												"price": "2826",
												"color": "E",
												"carat": "0.73",
												"depth": "62.6",
												"clarity": "VS1",
												"table": "60",
												"z": "3.58"
											},
											{
												"x": "5.94",
												"y": "5.88",
												"cut": "Good",
												"price": "2826",
												"color": "E",
												"carat": "0.7",
												"depth": "57.2",
												"clarity": "VS1",
												"table": "59",
												"z": "3.38"
											},
											{
												"x": "6.11",
												"y": "6.07",
												"cut": "Premium",
												"price": "2826",
												"color": "I",
												"carat": "0.9",
												"depth": "62.2",
												"clarity": "SI2",
												"table": "59",
												"z": "3.79"
											},
											{
												"x": "5.66",
												"y": "5.6",
												"cut": "Premium",
												"price": "2826",
												"color": "E",
												"carat": "0.7",
												"depth": "62.2",
												"clarity": "VS1",
												"table": "58",
												"z": "3.5"
											},
											{
												"x": "5.6",
												"y": "5.58",
												"cut": "Very Good",
												"price": "2826",
												"color": "D",
												"carat": "0.7",
												"depth": "63.3",
												"clarity": "VS2",
												"table": "56",
												"z": "3.54"
											},
											{
												"x": "5.78",
												"y": "5.74",
												"cut": "Premium",
												"price": "2826",
												"color": "E",
												"carat": "0.7",
												"depth": "59.4",
												"clarity": "VS1",
												"table": "61",
												"z": "3.42"
											},
											{
												"x": "6.17",
												"y": "6.07",
												"cut": "Very Good",
												"price": "2826",
												"color": "I",
												"carat": "0.9",
												"depth": "63.5",
												"clarity": "SI2",
												"table": "56",
												"z": "3.88"
											},
											{
												"x": "5.97",
												"y": "5.94",
												"cut": "Premium",
												"price": "2826",
												"color": "F",
												"carat": "0.78",
												"depth": "60.8",
												"clarity": "SI1",
												"table": "60",
												"z": "3.62"
											},
											{
												"x": "6.41",
												"y": "6.37",
												"cut": "Ideal",
												"price": "2826",
												"color": "F",
												"carat": "0.96",
												"depth": "60.7",
												"clarity": "I1",
												"table": "55",
												"z": "3.88"
											},
											{
												"x": "5.67",
												"y": "5.7",
												"cut": "Very Good",
												"price": "2827",
												"color": "D",
												"carat": "0.7",
												"depth": "62.3",
												"clarity": "SI1",
												"table": "59",
												"z": "3.54"
											},
											{
												"x": "5.68",
												"y": "5.7",
												"cut": "Good",
												"price": "2827",
												"color": "D",
												"carat": "0.72",
												"depth": "64",
												"clarity": "VS2",
												"table": "54",
												"z": "3.64"
											},
											{
												"x": "5.96",
												"y": "5.9",
												"cut": "Premium",
												"price": "2827",
												"color": "H",
												"carat": "0.79",
												"depth": "62.6",
												"clarity": "VVS2",
												"table": "58",
												"z": "3.71"
											},
											{
												"x": "5.69",
												"y": "5.74",
												"cut": "Ideal",
												"price": "2827",
												"color": "H",
												"carat": "0.7",
												"depth": "61.6",
												"clarity": "VVS1",
												"table": "57",
												"z": "3.52"
											},
											{
												"x": "5.66",
												"y": "5.7",
												"cut": "Ideal",
												"price": "2827",
												"color": "H",
												"carat": "0.7",
												"depth": "62.3",
												"clarity": "VVS1",
												"table": "55",
												"z": "3.54"
											},
											{
												"x": "5.74",
												"y": "5.77",
												"cut": "Ideal",
												"price": "2828",
												"color": "D",
												"carat": "0.7",
												"depth": "60.6",
												"clarity": "SI2",
												"table": "57",
												"z": "3.49"
											},
											{
												"x": "6.39",
												"y": "6.31",
												"cut": "Premium",
												"price": "2828",
												"color": "H",
												"carat": "1.01",
												"depth": "61.6",
												"clarity": "SI2",
												"table": "61",
												"z": "3.91"
											},
											{
												"x": "5.75",
												"y": "5.7",
												"cut": "Premium",
												"price": "2829",
												"color": "F",
												"carat": "0.72",
												"depth": "62.2",
												"clarity": "VS1",
												"table": "58",
												"z": "3.56"
											},
											{
												"x": "5.91",
												"y": "5.87",
												"cut": "Good",
												"price": "2829",
												"color": "E",
												"carat": "0.8",
												"depth": "63.7",
												"clarity": "SI2",
												"table": "54",
												"z": "3.75"
											},
											{
												"x": "5.36",
												"y": "5.38",
												"cut": "Ideal",
												"price": "2829",
												"color": "E",
												"carat": "0.59",
												"depth": "62",
												"clarity": "VVS1",
												"table": "56",
												"z": "3.33"
											},
											{
												"x": "5.77",
												"y": "5.74",
												"cut": "Ideal",
												"price": "2829",
												"color": "F",
												"carat": "0.72",
												"depth": "61.7",
												"clarity": "VS1",
												"table": "57",
												"z": "3.55"
											},
											{
												"x": "5.88",
												"y": "5.82",
												"cut": "Premium",
												"price": "2829",
												"color": "E",
												"carat": "0.75",
												"depth": "61.9",
												"clarity": "SI2",
												"table": "57",
												"z": "3.62"
											},
											{
												"x": "6.05",
												"y": "6.01",
												"cut": "Premium",
												"price": "2829",
												"color": "E",
												"carat": "0.8",
												"depth": "60.2",
												"clarity": "SI2",
												"table": "57",
												"z": "3.63"
											},
											{
												"x": "5.65",
												"y": "5.7",
												"cut": "Very Good",
												"price": "2830",
												"color": "E",
												"carat": "0.71",
												"depth": "62.7",
												"clarity": "VS2",
												"table": "59",
												"z": "3.56"
											},
											{
												"x": "5.84",
												"y": "5.89",
												"cut": "Very Good",
												"price": "2830",
												"color": "H",
												"carat": "0.77",
												"depth": "61.7",
												"clarity": "SI1",
												"table": "56",
												"z": "3.62"
											},
											{
												"x": "6.41",
												"y": "6.43",
												"cut": "Ideal",
												"price": "2830",
												"color": "F",
												"carat": "0.97",
												"depth": "60.7",
												"clarity": "I1",
												"table": "56",
												"z": "3.9"
											},
											{
												"x": "5.23",
												"y": "5.29",
												"cut": "Ideal",
												"price": "2830",
												"color": "F",
												"carat": "0.53",
												"depth": "60.9",
												"clarity": "VVS1",
												"table": "57",
												"z": "3.19"
											},
											{
												"x": "5.16",
												"y": "5.19",
												"cut": "Ideal",
												"price": "2830",
												"color": "F",
												"carat": "0.53",
												"depth": "61.8",
												"clarity": "VVS1",
												"table": "57",
												"z": "3.2"
											},
											{
												"x": "5.94",
												"y": "5.99",
												"cut": "Ideal",
												"price": "2830",
												"color": "I",
												"carat": "0.8",
												"depth": "62.1",
												"clarity": "VS2",
												"table": "54.4",
												"z": "3.7"
											},
											{
												"x": "6.21",
												"y": "6.13",
												"cut": "Premium",
												"price": "2830",
												"color": "G",
												"carat": "0.9",
												"depth": "60.6",
												"clarity": "SI1",
												"table": "62",
												"z": "3.74"
											},
											{
												"x": "5.89",
												"y": "5.98",
												"cut": "Very Good",
												"price": "2831",
												"color": "E",
												"carat": "0.76",
												"depth": "60.8",
												"clarity": "SI2",
												"table": "60",
												"z": "3.61"
											},
											{
												"x": "5.7",
												"y": "5.76",
												"cut": "Ideal",
												"price": "2831",
												"color": "E",
												"carat": "0.72",
												"depth": "62.3",
												"clarity": "SI1",
												"table": "57",
												"z": "3.57"
											},
											{
												"x": "5.82",
												"y": "5.87",
												"cut": "Ideal",
												"price": "2831",
												"color": "E",
												"carat": "0.75",
												"depth": "61.4",
												"clarity": "SI1",
												"table": "57",
												"z": "3.59"
											},
											{
												"x": "5.73",
												"y": "5.76",
												"cut": "Premium",
												"price": "2831",
												"color": "E",
												"carat": "0.72",
												"depth": "62.1",
												"clarity": "SI1",
												"table": "58",
												"z": "3.57"
											},
											{
												"x": "5.93",
												"y": "5.91",
												"cut": "Ideal",
												"price": "2831",
												"color": "G",
												"carat": "0.79",
												"depth": "61.8",
												"clarity": "SI1",
												"table": "56",
												"z": "3.66"
											},
											{
												"x": "5.71",
												"y": "5.75",
												"cut": "Very Good",
												"price": "2832",
												"color": "F",
												"carat": "0.72",
												"depth": "62.5",
												"clarity": "VS2",
												"table": "58",
												"z": "3.58"
											},
											{
												"x": "6.15",
												"y": "6.18",
												"cut": "Very Good",
												"price": "2832",
												"color": "I",
												"carat": "0.91",
												"depth": "62.8",
												"clarity": "SI2",
												"table": "61",
												"z": "3.87"
											},
											{
												"x": "5.75",
												"y": "5.65",
												"cut": "Premium",
												"price": "2832",
												"color": "G",
												"carat": "0.71",
												"depth": "62.1",
												"clarity": "VVS2",
												"table": "57",
												"z": "3.54"
											},
											{
												"x": "5.87",
												"y": "5.81",
												"cut": "Premium",
												"price": "2832",
												"color": "G",
												"carat": "0.81",
												"depth": "63",
												"clarity": "SI1",
												"table": "60",
												"z": "3.68"
											},
											{
												"x": "5.91",
												"y": "5.97",
												"cut": "Ideal",
												"price": "2832",
												"color": "H",
												"carat": "0.82",
												"depth": "62.5",
												"clarity": "SI1",
												"table": "57",
												"z": "3.71"
											},
											{
												"x": "5.72",
												"y": "5.66",
												"cut": "Premium",
												"price": "2832",
												"color": "F",
												"carat": "0.71",
												"depth": "62.2",
												"clarity": "VS1",
												"table": "58",
												"z": "3.54"
											},
											{
												"x": "6.05",
												"y": "6.01",
												"cut": "Good",
												"price": "2832",
												"color": "J",
												"carat": "0.9",
												"depth": "64.3",
												"clarity": "SI1",
												"table": "63",
												"z": "3.88"
											},
											{
												"x": "5.86",
												"y": "5.95",
												"cut": "Very Good",
												"price": "2833",
												"color": "I",
												"carat": "0.8",
												"depth": "62",
												"clarity": "VS2",
												"table": "58",
												"z": "3.66"
											},
											{
												"x": "5.28",
												"y": "5.34",
												"cut": "Very Good",
												"price": "2833",
												"color": "E",
												"carat": "0.56",
												"depth": "61",
												"clarity": "IF",
												"table": "59",
												"z": "3.24"
											},
											{
												"x": "5.77",
												"y": "5.8",
												"cut": "Very Good",
												"price": "2833",
												"color": "D",
												"carat": "0.7",
												"depth": "59.6",
												"clarity": "VS2",
												"table": "61",
												"z": "3.45"
											},
											{
												"x": "5.74",
												"y": "5.76",
												"cut": "Ideal",
												"price": "2833",
												"color": "D",
												"carat": "0.7",
												"depth": "61",
												"clarity": "VS2",
												"table": "57",
												"z": "3.51"
											},
											{
												"x": "5.45",
												"y": "5.48",
												"cut": "Ideal",
												"price": "2833",
												"color": "F",
												"carat": "0.61",
												"depth": "61.7",
												"clarity": "VVS2",
												"table": "55",
												"z": "3.37"
											},
											{
												"x": "6.02",
												"y": "6.07",
												"cut": "Ideal",
												"price": "2833",
												"color": "H",
												"carat": "0.85",
												"depth": "62.5",
												"clarity": "SI2",
												"table": "57",
												"z": "3.78"
											},
											{
												"x": "5.73",
												"y": "5.75",
												"cut": "Ideal",
												"price": "2833",
												"color": "F",
												"carat": "0.7",
												"depth": "60.7",
												"clarity": "SI1",
												"table": "57",
												"z": "3.49"
											},
											{
												"x": "5.94",
												"y": "5.87",
												"cut": "Ideal",
												"price": "2834",
												"color": "G",
												"carat": "0.8",
												"depth": "62.2",
												"clarity": "VS2",
												"table": "56",
												"z": "3.67"
											},
											{
												"x": "5.91",
												"y": "5.87",
												"cut": "Ideal",
												"price": "2834",
												"color": "H",
												"carat": "0.8",
												"depth": "62.8",
												"clarity": "VS2",
												"table": "57",
												"z": "3.7"
											},
											{
												"x": "5.16",
												"y": "5.19",
												"cut": "Very Good",
												"price": "2834",
												"color": "D",
												"carat": "0.51",
												"depth": "59.9",
												"clarity": "VVS1",
												"table": "58",
												"z": "3.1"
											},
											{
												"x": "5.2",
												"y": "5.23",
												"cut": "Ideal",
												"price": "2834",
												"color": "F",
												"carat": "0.53",
												"depth": "61.4",
												"clarity": "VVS1",
												"table": "57",
												"z": "3.2"
											},
											{
												"x": "5.92",
												"y": "5.95",
												"cut": "Ideal",
												"price": "2834",
												"color": "I",
												"carat": "0.78",
												"depth": "61.8",
												"clarity": "VS2",
												"table": "55",
												"z": "3.67"
											},
											{
												"x": "6.17",
												"y": "6.14",
												"cut": "Very Good",
												"price": "2834",
												"color": "J",
												"carat": "0.9",
												"depth": "63.4",
												"clarity": "SI1",
												"table": "54",
												"z": "3.9"
											},
											{
												"x": "6.07",
												"y": "6",
												"cut": "Fair",
												"price": "2834",
												"color": "G",
												"carat": "0.9",
												"depth": "65.3",
												"clarity": "SI2",
												"table": "59",
												"z": "3.94"
											},
											{
												"x": "6.01",
												"y": "5.95",
												"cut": "Ideal",
												"price": "2834",
												"color": "E",
												"carat": "0.77",
												"depth": "60.7",
												"clarity": "SI2",
												"table": "55",
												"z": "3.63"
											},
											{
												"x": "5.89",
												"y": "5.81",
												"cut": "Ideal",
												"price": "2835",
												"color": "F",
												"carat": "0.73",
												"depth": "61.2",
												"clarity": "VS1",
												"table": "56",
												"z": "3.58"
											},
											{
												"x": "5.47",
												"y": "5.51",
												"cut": "Ideal",
												"price": "2835",
												"color": "F",
												"carat": "0.63",
												"depth": "61.9",
												"clarity": "VVS2",
												"table": "57",
												"z": "3.4"
											},
											{
												"x": "5.7",
												"y": "5.75",
												"cut": "Ideal",
												"price": "2835",
												"color": "E",
												"carat": "0.7",
												"depth": "61.5",
												"clarity": "VS2",
												"table": "54",
												"z": "3.52"
											},
											{
												"x": "5.71",
												"y": "5.73",
												"cut": "Ideal",
												"price": "2835",
												"color": "E",
												"carat": "0.72",
												"depth": "62.8",
												"clarity": "VS2",
												"table": "57",
												"z": "3.59"
											},
											{
												"x": "5.78",
												"y": "5.8",
												"cut": "Ideal",
												"price": "2835",
												"color": "E",
												"carat": "0.72",
												"depth": "61",
												"clarity": "SI1",
												"table": "57",
												"z": "3.53"
											},
											{
												"x": "6.04",
												"y": "5.94",
												"cut": "Premium",
												"price": "2835",
												"color": "F",
												"carat": "0.75",
												"depth": "59.6",
												"clarity": "VS2",
												"table": "59",
												"z": "3.57"
											},
											{
												"x": "6.04",
												"y": "6.06",
												"cut": "Very Good",
												"price": "2836",
												"color": "H",
												"carat": "0.82",
												"depth": "60.7",
												"clarity": "SI1",
												"table": "56",
												"z": "3.67"
											},
											{
												"x": "5.6",
												"y": "5.65",
												"cut": "Good",
												"price": "2836",
												"color": "E",
												"carat": "0.71",
												"depth": "62.8",
												"clarity": "VS2",
												"table": "60",
												"z": "3.53"
											},
											{
												"x": "5.69",
												"y": "5.66",
												"cut": "Premium",
												"price": "2837",
												"color": "E",
												"carat": "0.7",
												"depth": "62.6",
												"clarity": "VS1",
												"table": "59",
												"z": "3.55"
											},
											{
												"x": "5.74",
												"y": "5.69",
												"cut": "Ideal",
												"price": "2837",
												"color": "E",
												"carat": "0.7",
												"depth": "61.8",
												"clarity": "VS1",
												"table": "56",
												"z": "3.53"
											},
											{
												"x": "5.86",
												"y": "5.82",
												"cut": "Ideal",
												"price": "2838",
												"color": "F",
												"carat": "0.71",
												"depth": "59.8",
												"clarity": "SI1",
												"table": "53",
												"z": "3.49"
											},
											{
												"x": "5.92",
												"y": "5.94",
												"cut": "Very Good",
												"price": "2838",
												"color": "H",
												"carat": "0.76",
												"depth": "60.9",
												"clarity": "SI1",
												"table": "55",
												"z": "3.61"
											},
											{
												"x": "5.83",
												"y": "5.79",
												"cut": "Fair",
												"price": "2838",
												"color": "F",
												"carat": "0.82",
												"depth": "64.9",
												"clarity": "SI1",
												"table": "58",
												"z": "3.77"
											},
											{
												"x": "5.91",
												"y": "5.89",
												"cut": "Premium",
												"price": "2838",
												"color": "F",
												"carat": "0.72",
												"depth": "58.8",
												"clarity": "VS1",
												"table": "60",
												"z": "3.47"
											},
											{
												"x": "5.72",
												"y": "5.64",
												"cut": "Premium",
												"price": "2838",
												"color": "F",
												"carat": "0.7",
												"depth": "62.3",
												"clarity": "VS2",
												"table": "58",
												"z": "3.54"
											},
											{
												"x": "5.69",
												"y": "5.63",
												"cut": "Premium",
												"price": "2838",
												"color": "F",
												"carat": "0.7",
												"depth": "61.7",
												"clarity": "VS2",
												"table": "58",
												"z": "3.49"
											},
											{
												"x": "5.73",
												"y": "5.64",
												"cut": "Premium",
												"price": "2838",
												"color": "G",
												"carat": "0.7",
												"depth": "62.6",
												"clarity": "VS1",
												"table": "55",
												"z": "3.56"
											},
											{
												"x": "5.83",
												"y": "5.79",
												"cut": "Premium",
												"price": "2838",
												"color": "F",
												"carat": "0.7",
												"depth": "59.4",
												"clarity": "VS2",
												"table": "61",
												"z": "3.45"
											},
											{
												"x": "5.53",
												"y": "5.49",
												"cut": "Very Good",
												"price": "2838",
												"color": "E",
												"carat": "0.7",
												"depth": "63.5",
												"clarity": "SI1",
												"table": "59",
												"z": "3.5"
											},
											{
												"x": "5.71",
												"y": "5.66",
												"cut": "Premium",
												"price": "2838",
												"color": "F",
												"carat": "0.7",
												"depth": "60.9",
												"clarity": "VS2",
												"table": "61",
												"z": "3.46"
											},
											{
												"x": "5.85",
												"y": "5.75",
												"cut": "Premium",
												"price": "2838",
												"color": "F",
												"carat": "0.7",
												"depth": "59.5",
												"clarity": "VS2",
												"table": "58",
												"z": "3.45"
											},
											{
												"x": "5.64",
												"y": "5.57",
												"cut": "Premium",
												"price": "2838",
												"color": "G",
												"carat": "0.7",
												"depth": "63",
												"clarity": "VS1",
												"table": "60",
												"z": "3.53"
											},
											{
												"x": "5.85",
												"y": "5.89",
												"cut": "Very Good",
												"price": "2839",
												"color": "E",
												"carat": "0.74",
												"depth": "60",
												"clarity": "SI1",
												"table": "57",
												"z": "3.52"
											},
											{
												"x": "5.74",
												"y": "5.71",
												"cut": "Ideal",
												"price": "2839",
												"color": "F",
												"carat": "0.71",
												"depth": "61.5",
												"clarity": "VS1",
												"table": "57",
												"z": "3.52"
											},
											{
												"x": "5.75",
												"y": "5.72",
												"cut": "Ideal",
												"price": "2839",
												"color": "F",
												"carat": "0.7",
												"depth": "61.6",
												"clarity": "VS1",
												"table": "54",
												"z": "3.53"
											},
											{
												"x": "5.82",
												"y": "5.68",
												"cut": "Ideal",
												"price": "2839",
												"color": "F",
												"carat": "0.71",
												"depth": "62.1",
												"clarity": "VS1",
												"table": "55",
												"z": "3.57"
											},
											{
												"x": "5.84",
												"y": "5.81",
												"cut": "Premium",
												"price": "2839",
												"color": "F",
												"carat": "0.71",
												"depth": "59.1",
												"clarity": "VS1",
												"table": "61",
												"z": "3.44"
											},
											{
												"x": "5.82",
												"y": "5.8",
												"cut": "Premium",
												"price": "2839",
												"color": "F",
												"carat": "0.71",
												"depth": "59",
												"clarity": "VS1",
												"table": "60",
												"z": "3.43"
											},
											{
												"x": "5.75",
												"y": "5.72",
												"cut": "Premium",
												"price": "2839",
												"color": "F",
												"carat": "0.71",
												"depth": "60.5",
												"clarity": "VS1",
												"table": "58",
												"z": "3.47"
											},
											{
												"x": "5.73",
												"y": "5.71",
												"cut": "Ideal",
												"price": "2839",
												"color": "F",
												"carat": "0.7",
												"depth": "62.4",
												"clarity": "VS1",
												"table": "53",
												"z": "3.57"
											},
											{
												"x": "5.8",
												"y": "5.82",
												"cut": "Ideal",
												"price": "2839",
												"color": "G",
												"carat": "0.73",
												"depth": "61.8",
												"clarity": "VS2",
												"table": "54",
												"z": "3.59"
											},
											{
												"x": "5.69",
												"y": "5.72",
												"cut": "Ideal",
												"price": "2839",
												"color": "E",
												"carat": "0.7",
												"depth": "62.1",
												"clarity": "VS2",
												"table": "54",
												"z": "3.54"
											},
											{
												"x": "5.71",
												"y": "5.74",
												"cut": "Ideal",
												"price": "2839",
												"color": "G",
												"carat": "0.7",
												"depth": "61.3",
												"clarity": "VS1",
												"table": "57",
												"z": "3.51"
											},
											{
												"x": "5.82",
												"y": "5.78",
												"cut": "Premium",
												"price": "2839",
												"color": "G",
												"carat": "0.71",
												"depth": "60.3",
												"clarity": "VVS2",
												"table": "58",
												"z": "3.5"
											},
											{
												"x": "5.87",
												"y": "5.82",
												"cut": "Premium",
												"price": "2839",
												"color": "F",
												"carat": "0.71",
												"depth": "59.2",
												"clarity": "VS1",
												"table": "58",
												"z": "3.46"
											},
											{
												"x": "6.09",
												"y": "6.01",
												"cut": "Premium",
												"price": "2839",
												"color": "G",
												"carat": "0.79",
												"depth": "59.3",
												"clarity": "VS2",
												"table": "62",
												"z": "3.59"
											},
											{
												"x": "5.7",
												"y": "5.62",
												"cut": "Premium",
												"price": "2839",
												"color": "F",
												"carat": "0.71",
												"depth": "62.7",
												"clarity": "VS1",
												"table": "59",
												"z": "3.55"
											},
											{
												"x": "5.9",
												"y": "5.87",
												"cut": "Very Good",
												"price": "2840",
												"color": "H",
												"carat": "0.77",
												"depth": "61",
												"clarity": "VS1",
												"table": "60",
												"z": "3.59"
											},
											{
												"x": "5.85",
												"y": "5.92",
												"cut": "Very Good",
												"price": "2840",
												"color": "F",
												"carat": "0.75",
												"depth": "59.8",
												"clarity": "SI2",
												"table": "56",
												"z": "3.52"
											},
											{
												"x": "5.75",
												"y": "5.8",
												"cut": "Ideal",
												"price": "2840",
												"color": "F",
												"carat": "0.7",
												"depth": "61",
												"clarity": "SI1",
												"table": "56",
												"z": "3.52"
											},
											{
												"x": "5.88",
												"y": "5.82",
												"cut": "Premium",
												"price": "2840",
												"color": "F",
												"carat": "0.71",
												"depth": "59.3",
												"clarity": "VS2",
												"table": "56",
												"z": "3.47"
											},
											{
												"x": "6.27",
												"y": "6.2",
												"cut": "Ideal",
												"price": "2840",
												"color": "D",
												"carat": "0.92",
												"depth": "61.9",
												"clarity": "SI2",
												"table": "56",
												"z": "3.86"
											},
											{
												"x": "6.08",
												"y": "6.04",
												"cut": "Premium",
												"price": "2840",
												"color": "F",
												"carat": "0.83",
												"depth": "61.4",
												"clarity": "SI2",
												"table": "59",
												"z": "3.72"
											},
											{
												"x": "5.87",
												"y": "5.78",
												"cut": "Premium",
												"price": "2840",
												"color": "H",
												"carat": "0.7",
												"depth": "59.2",
												"clarity": "VVS1",
												"table": "60",
												"z": "3.45"
											},
											{
												"x": "5.9",
												"y": "5.87",
												"cut": "Premium",
												"price": "2841",
												"color": "F",
												"carat": "0.73",
												"depth": "60.3",
												"clarity": "VS2",
												"table": "59",
												"z": "3.55"
											},
											{
												"x": "5.69",
												"y": "5.61",
												"cut": "Very Good",
												"price": "2841",
												"color": "D",
												"carat": "0.71",
												"depth": "63.4",
												"clarity": "VS1",
												"table": "55",
												"z": "3.58"
											},
											{
												"x": "5.66",
												"y": "5.71",
												"cut": "Very Good",
												"price": "2841",
												"color": "D",
												"carat": "0.73",
												"depth": "63.9",
												"clarity": "SI1",
												"table": "57",
												"z": "3.63"
											},
											{
												"x": "6",
												"y": "6.12",
												"cut": "Ideal",
												"price": "2841",
												"color": "F",
												"carat": "0.82",
												"depth": "61.7",
												"clarity": "SI2",
												"table": "53",
												"z": "3.74"
											},
											{
												"x": "5.96",
												"y": "6.02",
												"cut": "Ideal",
												"price": "2841",
												"color": "F",
												"carat": "0.82",
												"depth": "62.3",
												"clarity": "SI2",
												"table": "56",
												"z": "3.73"
											},
											{
												"x": "6.12",
												"y": "6.14",
												"cut": "Very Good",
												"price": "2841",
												"color": "F",
												"carat": "0.82",
												"depth": "59.7",
												"clarity": "SI2",
												"table": "57",
												"z": "3.66"
											},
											{
												"x": "5.19",
												"y": "5.21",
												"cut": "Ideal",
												"price": "2841",
												"color": "F",
												"carat": "0.52",
												"depth": "61.2",
												"clarity": "VVS1",
												"table": "56",
												"z": "3.18"
											},
											{
												"x": "6.6",
												"y": "6.55",
												"cut": "Premium",
												"price": "2841",
												"color": "F",
												"carat": "1",
												"depth": "58.9",
												"clarity": "I1",
												"table": "60",
												"z": "3.87"
											},
											{
												"x": "6.16",
												"y": "6.03",
												"cut": "Fair",
												"price": "2841",
												"color": "G",
												"carat": "0.95",
												"depth": "66.7",
												"clarity": "SI1",
												"table": "56",
												"z": "4.06"
											},
											{
												"x": "5.76",
												"y": "5.8",
												"cut": "Ideal",
												"price": "2841",
												"color": "D",
												"carat": "0.73",
												"depth": "61.4",
												"clarity": "SI1",
												"table": "57",
												"z": "3.55"
											},
											{
												"x": "5.87",
												"y": "5.77",
												"cut": "Premium",
												"price": "2841",
												"color": "F",
												"carat": "0.73",
												"depth": "59.9",
												"clarity": "VS2",
												"table": "59",
												"z": "3.5"
											},
											{
												"x": "5.82",
												"y": "5.77",
												"cut": "Premium",
												"price": "2841",
												"color": "G",
												"carat": "0.73",
												"depth": "61.4",
												"clarity": "VS1",
												"table": "58",
												"z": "3.56"
											},
											{
												"x": "5.92",
												"y": "5.96",
												"cut": "Ideal",
												"price": "2842",
												"color": "I",
												"carat": "0.8",
												"depth": "62.6",
												"clarity": "VS1",
												"table": "54",
												"z": "3.72"
											},
											{
												"x": "5.8",
												"y": "5.72",
												"cut": "Premium",
												"price": "2842",
												"color": "F",
												"carat": "0.7",
												"depth": "58.7",
												"clarity": "VS2",
												"table": "61",
												"z": "3.38"
											},
											{
												"x": "5.71",
												"y": "5.75",
												"cut": "Very Good",
												"price": "2843",
												"color": "E",
												"carat": "0.7",
												"depth": "60.2",
												"clarity": "VS2",
												"table": "62",
												"z": "3.45"
											},
											{
												"x": "5.65",
												"y": "5.67",
												"cut": "Very Good",
												"price": "2843",
												"color": "E",
												"carat": "0.7",
												"depth": "62.7",
												"clarity": "VS2",
												"table": "58",
												"z": "3.55"
											},
											{
												"x": "5.76",
												"y": "5.82",
												"cut": "Very Good",
												"price": "2843",
												"color": "E",
												"carat": "0.71",
												"depth": "59.4",
												"clarity": "VS2",
												"table": "58",
												"z": "3.44"
											},
											{
												"x": "5.91",
												"y": "5.92",
												"cut": "Very Good",
												"price": "2843",
												"color": "F",
												"carat": "0.81",
												"depth": "63.2",
												"clarity": "SI2",
												"table": "58",
												"z": "3.74"
											},
											{
												"x": "5.73",
												"y": "5.79",
												"cut": "Very Good",
												"price": "2843",
												"color": "D",
												"carat": "0.71",
												"depth": "61.5",
												"clarity": "SI1",
												"table": "58",
												"z": "3.54"
											},
											{
												"x": "5.81",
												"y": "5.84",
												"cut": "Ideal",
												"price": "2843",
												"color": "G",
												"carat": "0.73",
												"depth": "61.3",
												"clarity": "VVS2",
												"table": "57",
												"z": "3.57"
											},
											{
												"x": "5.73",
												"y": "5.79",
												"cut": "Very Good",
												"price": "2843",
												"color": "F",
												"carat": "0.73",
												"depth": "61.8",
												"clarity": "VS1",
												"table": "59",
												"z": "3.56"
											},
											{
												"x": "5.71",
												"y": "5.74",
												"cut": "Ideal",
												"price": "2843",
												"color": "E",
												"carat": "0.72",
												"depth": "62",
												"clarity": "VS2",
												"table": "57",
												"z": "3.55"
											},
											{
												"x": "5.91",
												"y": "5.95",
												"cut": "Ideal",
												"price": "2843",
												"color": "F",
												"carat": "0.81",
												"depth": "62.1",
												"clarity": "SI2",
												"table": "57",
												"z": "3.68"
											},
											{
												"x": "5.81",
												"y": "5.78",
												"cut": "Ideal",
												"price": "2843",
												"color": "G",
												"carat": "0.71",
												"depth": "60.7",
												"clarity": "VVS2",
												"table": "57",
												"z": "3.52"
											},
											{
												"x": "5.92",
												"y": "5.96",
												"cut": "Very Good",
												"price": "2844",
												"color": "E",
												"carat": "0.73",
												"depth": "57.7",
												"clarity": "SI1",
												"table": "61",
												"z": "3.43"
											},
											{
												"x": "5.65",
												"y": "5.68",
												"cut": "Very Good",
												"price": "2844",
												"color": "E",
												"carat": "0.7",
												"depth": "62",
												"clarity": "VS1",
												"table": "59",
												"z": "3.51"
											},
											{
												"x": "6.45",
												"y": "6.46",
												"cut": "Ideal",
												"price": "2844",
												"color": "I",
												"carat": "1.01",
												"depth": "61.5",
												"clarity": "I1",
												"table": "57",
												"z": "3.97"
											},
											{
												"x": "6.35",
												"y": "6.39",
												"cut": "Good",
												"price": "2844",
												"color": "I",
												"carat": "1.01",
												"depth": "63.1",
												"clarity": "I1",
												"table": "57",
												"z": "4.02"
											},
											{
												"x": "5.91",
												"y": "5.93",
												"cut": "Ideal",
												"price": "2844",
												"color": "H",
												"carat": "0.79",
												"depth": "62.5",
												"clarity": "VS2",
												"table": "57",
												"z": "3.7"
											},
											{
												"x": "5.65",
												"y": "5.68",
												"cut": "Very Good",
												"price": "2845",
												"color": "E",
												"carat": "0.7",
												"depth": "61.8",
												"clarity": "VS2",
												"table": "59",
												"z": "3.5"
											},
											{
												"x": "5.83",
												"y": "5.85",
												"cut": "Very Good",
												"price": "2845",
												"color": "E",
												"carat": "0.7",
												"depth": "58.9",
												"clarity": "VS2",
												"table": "60",
												"z": "3.44"
											},
											{
												"x": "5.92",
												"y": "5.82",
												"cut": "Good",
												"price": "2845",
												"color": "H",
												"carat": "0.8",
												"depth": "63.4",
												"clarity": "VS2",
												"table": "60",
												"z": "3.72"
											},
											{
												"x": "7.12",
												"y": "7.05",
												"cut": "Premium",
												"price": "2845",
												"color": "H",
												"carat": "1.27",
												"depth": "59.3",
												"clarity": "SI2",
												"table": "61",
												"z": "4.2"
											},
											{
												"x": "5.96",
												"y": "5.91",
												"cut": "Ideal",
												"price": "2846",
												"color": "D",
												"carat": "0.79",
												"depth": "61.5",
												"clarity": "SI1",
												"table": "56",
												"z": "3.65"
											},
											{
												"x": "5.79",
												"y": "5.84",
												"cut": "Very Good",
												"price": "2846",
												"color": "F",
												"carat": "0.72",
												"depth": "60.2",
												"clarity": "VS1",
												"table": "59",
												"z": "3.5"
											},
											{
												"x": "5.79",
												"y": "5.84",
												"cut": "Ideal",
												"price": "2846",
												"color": "H",
												"carat": "0.73",
												"depth": "61.6",
												"clarity": "VVS2",
												"table": "56",
												"z": "3.58"
											},
											{
												"x": "6.3",
												"y": "6.26",
												"cut": "Fair",
												"price": "2846",
												"color": "H",
												"carat": "1.01",
												"depth": "65.4",
												"clarity": "SI2",
												"table": "59",
												"z": "4.11"
											},
											{
												"x": "6.25",
												"y": "6.18",
												"cut": "Good",
												"price": "2846",
												"color": "H",
												"carat": "1.01",
												"depth": "64.2",
												"clarity": "I1",
												"table": "61",
												"z": "3.99"
											},
											{
												"x": "5.92",
												"y": "5.95",
												"cut": "Ideal",
												"price": "2846",
												"color": "E",
												"carat": "0.73",
												"depth": "59.1",
												"clarity": "SI1",
												"table": "59",
												"z": "3.51"
											},
											{
												"x": "5.71",
												"y": "5.76",
												"cut": "Ideal",
												"price": "2846",
												"color": "E",
												"carat": "0.7",
												"depth": "61.6",
												"clarity": "SI1",
												"table": "57",
												"z": "3.53"
											},
											{
												"x": "5.76",
												"y": "5.84",
												"cut": "Good",
												"price": "2846",
												"color": "F",
												"carat": "0.7",
												"depth": "59.1",
												"clarity": "VS2",
												"table": "61",
												"z": "3.43"
											},
											{
												"x": "5.84",
												"y": "5.79",
												"cut": "Premium",
												"price": "2846",
												"color": "E",
												"carat": "0.77",
												"depth": "62.9",
												"clarity": "SI1",
												"table": "59",
												"z": "3.66"
											},
											{
												"x": "5.91",
												"y": "5.81",
												"cut": "Premium",
												"price": "2846",
												"color": "G",
												"carat": "0.77",
												"depth": "61.3",
												"clarity": "VS2",
												"table": "60",
												"z": "3.59"
											},
											{
												"x": "5.94",
												"y": "5.89",
												"cut": "Premium",
												"price": "2846",
												"color": "G",
												"carat": "0.77",
												"depth": "61.4",
												"clarity": "VS1",
												"table": "58",
												"z": "3.63"
											},
											{
												"x": "6.1",
												"y": "6.12",
												"cut": "Very Good",
												"price": "2847",
												"color": "H",
												"carat": "0.84",
												"depth": "61.2",
												"clarity": "SI1",
												"table": "57",
												"z": "3.74"
											},
											{
												"x": "5.83",
												"y": "5.85",
												"cut": "Ideal",
												"price": "2847",
												"color": "E",
												"carat": "0.72",
												"depth": "60.3",
												"clarity": "SI1",
												"table": "57",
												"z": "3.52"
											},
											{
												"x": "5.93",
												"y": "5.88",
												"cut": "Premium",
												"price": "2847",
												"color": "D",
												"carat": "0.76",
												"depth": "61.1",
												"clarity": "SI1",
												"table": "59",
												"z": "3.61"
											},
											{
												"x": "5.61",
												"y": "5.68",
												"cut": "Very Good",
												"price": "2848",
												"color": "G",
												"carat": "0.7",
												"depth": "62.9",
												"clarity": "VVS2",
												"table": "59",
												"z": "3.55"
											},
											{
												"x": "5.25",
												"y": "5.29",
												"cut": "Ideal",
												"price": "2848",
												"color": "D",
												"carat": "0.54",
												"depth": "61.5",
												"clarity": "VVS2",
												"table": "55",
												"z": "3.24"
											},
											{
												"x": "5.74",
												"y": "5.72",
												"cut": "Fair",
												"price": "2848",
												"color": "D",
												"carat": "0.75",
												"depth": "64.6",
												"clarity": "SI2",
												"table": "57",
												"z": "3.7"
											},
											{
												"x": "5.86",
												"y": "5.84",
												"cut": "Good",
												"price": "2849",
												"color": "E",
												"carat": "0.79",
												"depth": "64.1",
												"clarity": "SI1",
												"table": "54",
												"z": "3.75"
											},
											{
												"x": "5.75",
												"y": "5.73",
												"cut": "Very Good",
												"price": "2849",
												"color": "E",
												"carat": "0.74",
												"depth": "63.1",
												"clarity": "VS1",
												"table": "58",
												"z": "3.62"
											},
											{
												"x": "5.74",
												"y": "5.77",
												"cut": "Very Good",
												"price": "2850",
												"color": "E",
												"carat": "0.7",
												"depth": "61",
												"clarity": "VS2",
												"table": "60",
												"z": "3.51"
											},
											{
												"x": "5.69",
												"y": "5.79",
												"cut": "Ideal",
												"price": "2850",
												"color": "F",
												"carat": "0.7",
												"depth": "60.8",
												"clarity": "VS2",
												"table": "59",
												"z": "3.49"
											},
											{
												"x": "5.83",
												"y": "5.87",
												"cut": "Ideal",
												"price": "2850",
												"color": "J",
												"carat": "0.75",
												"depth": "61.5",
												"clarity": "SI1",
												"table": "56",
												"z": "3.6"
											},
											{
												"x": "6.75",
												"y": "6.67",
												"cut": "Very Good",
												"price": "2850",
												"color": "H",
												"carat": "1.2",
												"depth": "63.1",
												"clarity": "I1",
												"table": "60",
												"z": "4.23"
											},
											{
												"x": "5.89",
												"y": "5.82",
												"cut": "Very Good",
												"price": "2851",
												"color": "F",
												"carat": "0.8",
												"depth": "63.4",
												"clarity": "SI1",
												"table": "57",
												"z": "3.71"
											},
											{
												"x": "5.54",
												"y": "5.57",
												"cut": "Ideal",
												"price": "2851",
												"color": "D",
												"carat": "0.66",
												"depth": "62.1",
												"clarity": "VS1",
												"table": "56",
												"z": "3.45"
											},
											{
												"x": "6.22",
												"y": "6.07",
												"cut": "Very Good",
												"price": "2851",
												"color": "F",
												"carat": "0.87",
												"depth": "61",
												"clarity": "SI2",
												"table": "63",
												"z": "3.75"
											},
											{
												"x": "6.04",
												"y": "5.98",
												"cut": "Premium",
												"price": "2851",
												"color": "H",
												"carat": "0.86",
												"depth": "62.7",
												"clarity": "SI1",
												"table": "59",
												"z": "3.77"
											},
											{
												"x": "5.85",
												"y": "5.81",
												"cut": "Ideal",
												"price": "2851",
												"color": "F",
												"carat": "0.74",
												"depth": "61",
												"clarity": "SI1",
												"table": "57",
												"z": "3.56"
											},
											{
												"x": "5.37",
												"y": "5.43",
												"cut": "Very Good",
												"price": "2852",
												"color": "E",
												"carat": "0.58",
												"depth": "60.6",
												"clarity": "IF",
												"table": "59",
												"z": "3.27"
											},
											{
												"x": "5.88",
												"y": "5.92",
												"cut": "Ideal",
												"price": "2852",
												"color": "I",
												"carat": "0.78",
												"depth": "61.5",
												"clarity": "VS1",
												"table": "57",
												"z": "3.63"
											},
											{
												"x": "5.85",
												"y": "5.86",
												"cut": "Ideal",
												"price": "2852",
												"color": "G",
												"carat": "0.74",
												"depth": "61.3",
												"clarity": "SI1",
												"table": "55",
												"z": "3.59"
											},
											{
												"x": "5.7",
												"y": "5.79",
												"cut": "Ideal",
												"price": "2852",
												"color": "E",
												"carat": "0.73",
												"depth": "62.7",
												"clarity": "SI1",
												"table": "55",
												"z": "3.6"
											},
											{
												"x": "6.12",
												"y": "6.07",
												"cut": "Very Good",
												"price": "2852",
												"color": "I",
												"carat": "0.91",
												"depth": "63.5",
												"clarity": "SI1",
												"table": "57",
												"z": "3.87"
											},
											{
												"x": "5.67",
												"y": "5.7",
												"cut": "Premium",
												"price": "2853",
												"color": "F",
												"carat": "0.71",
												"depth": "62.6",
												"clarity": "VS2",
												"table": "58",
												"z": "3.56"
											},
											{
												"x": "5.64",
												"y": "5.66",
												"cut": "Good",
												"price": "2853",
												"color": "G",
												"carat": "0.71",
												"depth": "63.5",
												"clarity": "VS1",
												"table": "55",
												"z": "3.59"
											},
											{
												"x": "5.9",
												"y": "5.85",
												"cut": "Ideal",
												"price": "2853",
												"color": "D",
												"carat": "0.79",
												"depth": "62.8",
												"clarity": "SI2",
												"table": "57",
												"z": "3.69"
											},
											{
												"x": "6.07",
												"y": "6.03",
												"cut": "Premium",
												"price": "2853",
												"color": "D",
												"carat": "0.79",
												"depth": "60",
												"clarity": "SI2",
												"table": "60",
												"z": "3.63"
											},
											{
												"x": "5.73",
												"y": "5.66",
												"cut": "Premium",
												"price": "2853",
												"color": "E",
												"carat": "0.71",
												"depth": "62.7",
												"clarity": "SI1",
												"table": "58",
												"z": "3.57"
											},
											{
												"x": "5.99",
												"y": "5.97",
												"cut": "Premium",
												"price": "2853",
												"color": "I",
												"carat": "0.82",
												"depth": "61.9",
												"clarity": "VS1",
												"table": "58",
												"z": "3.7"
											},
											{
												"x": "5.87",
												"y": "5.95",
												"cut": "Very Good",
												"price": "2854",
												"color": "H",
												"carat": "0.78",
												"depth": "61.9",
												"clarity": "VS1",
												"table": "57.1",
												"z": "3.66"
											},
											{
												"x": "5.64",
												"y": "5.7",
												"cut": "Very Good",
												"price": "2854",
												"color": "E",
												"carat": "0.7",
												"depth": "62.4",
												"clarity": "VS1",
												"table": "56",
												"z": "3.54"
											},
											{
												"x": "6.78",
												"y": "6.75",
												"cut": "Premium",
												"price": "2854",
												"color": "H",
												"carat": "1.12",
												"depth": "59.1",
												"clarity": "I1",
												"table": "61",
												"z": "4"
											},
											{
												"x": "5.86",
												"y": "5.76",
												"cut": "Premium",
												"price": "2854",
												"color": "E",
												"carat": "0.73",
												"depth": "62",
												"clarity": "VS2",
												"table": "57",
												"z": "3.6"
											},
											{
												"x": "6.06",
												"y": "6.03",
												"cut": "Fair",
												"price": "2854",
												"color": "J",
												"carat": "0.91",
												"depth": "64.4",
												"clarity": "VS2",
												"table": "62",
												"z": "3.89"
											},
											{
												"x": "6.04",
												"y": "6",
												"cut": "Fair",
												"price": "2854",
												"color": "J",
												"carat": "0.91",
												"depth": "65.4",
												"clarity": "VS2",
												"table": "60",
												"z": "3.94"
											},
											{
												"x": "6.12",
												"y": "6.09",
												"cut": "Good",
												"price": "2854",
												"color": "J",
												"carat": "0.91",
												"depth": "64.2",
												"clarity": "VS2",
												"table": "58",
												"z": "3.92"
											},
											{
												"x": "6.04",
												"y": "6.01",
												"cut": "Fair",
												"price": "2854",
												"color": "H",
												"carat": "0.91",
												"depth": "65.8",
												"clarity": "SI1",
												"table": "58",
												"z": "3.96"
											},
											{
												"x": "5.91",
												"y": "5.83",
												"cut": "Premium",
												"price": "2854",
												"color": "E",
												"carat": "0.7",
												"depth": "58.4",
												"clarity": "VS1",
												"table": "59",
												"z": "3.43"
											},
											{
												"x": "5.67",
												"y": "5.64",
												"cut": "Premium",
												"price": "2854",
												"color": "F",
												"carat": "0.68",
												"depth": "61.7",
												"clarity": "VVS2",
												"table": "57",
												"z": "3.49"
											},
											{
												"x": "5.7",
												"y": "5.75",
												"cut": "Very Good",
												"price": "2855",
												"color": "F",
												"carat": "0.73",
												"depth": "62.5",
												"clarity": "VS2",
												"table": "57",
												"z": "3.58"
											},
											{
												"x": "6.38",
												"y": "6.29",
												"cut": "Good",
												"price": "2855",
												"color": "J",
												"carat": "1.03",
												"depth": "63.6",
												"clarity": "SI1",
												"table": "57",
												"z": "4.03"
											},
											{
												"x": "5.8",
												"y": "5.74",
												"cut": "Premium",
												"price": "2855",
												"color": "D",
												"carat": "0.74",
												"depth": "62.4",
												"clarity": "VS2",
												"table": "57",
												"z": "3.6"
											},
											{
												"x": "6.82",
												"y": "6.74",
												"cut": "Fair",
												"price": "2855",
												"color": "E",
												"carat": "0.98",
												"depth": "53.3",
												"clarity": "SI2",
												"table": "67",
												"z": "3.61"
											},
											{
												"x": "6.84",
												"y": "6.77",
												"cut": "Fair",
												"price": "2856",
												"color": "I",
												"carat": "1.02",
												"depth": "53",
												"clarity": "SI1",
												"table": "63",
												"z": "3.66"
											},
											{
												"x": "5.96",
												"y": "5.9",
												"cut": "Fair",
												"price": "2856",
												"color": "G",
												"carat": "1",
												"depth": "67.8",
												"clarity": "SI2",
												"table": "61",
												"z": "4.02"
											},
											{
												"x": "6.49",
												"y": "6.43",
												"cut": "Ideal",
												"price": "2856",
												"color": "H",
												"carat": "1.02",
												"depth": "61.6",
												"clarity": "SI2",
												"table": "55",
												"z": "3.98"
											},
											{
												"x": "5.44",
												"y": "5.49",
												"cut": "Ideal",
												"price": "2856",
												"color": "F",
												"carat": "0.6",
												"depth": "60.8",
												"clarity": "VVS2",
												"table": "57",
												"z": "3.32"
											},
											{
												"x": "5.97",
												"y": "6.01",
												"cut": "Ideal",
												"price": "2856",
												"color": "G",
												"carat": "0.8",
												"depth": "61.6",
												"clarity": "SI2",
												"table": "56",
												"z": "3.69"
											},
											{
												"x": "6.43",
												"y": "6.41",
												"cut": "Ideal",
												"price": "2856",
												"color": "F",
												"carat": "0.97",
												"depth": "60.7",
												"clarity": "I1",
												"table": "56",
												"z": "3.9"
											},
											{
												"x": "6.19",
												"y": "6.03",
												"cut": "Fair",
												"price": "2856",
												"color": "I",
												"carat": "1",
												"depth": "67.9",
												"clarity": "SI1",
												"table": "62",
												"z": "4.15"
											},
											{
												"x": "4.05",
												"y": "4.08",
												"cut": "Ideal",
												"price": "556",
												"color": "E",
												"carat": "0.26",
												"depth": "62.3",
												"clarity": "VS1",
												"table": "57",
												"z": "2.53"
											},
											{
												"x": "4.09",
												"y": "4.12",
												"cut": "Ideal",
												"price": "556",
												"color": "E",
												"carat": "0.26",
												"depth": "62.1",
												"clarity": "VS1",
												"table": "56",
												"z": "2.55"
											},
											{
												"x": "4.57",
												"y": "4.59",
												"cut": "Ideal",
												"price": "556",
												"color": "H",
												"carat": "0.36",
												"depth": "61.9",
												"clarity": "SI1",
												"table": "55",
												"z": "2.83"
											},
											{
												"x": "4.6",
												"y": "4.66",
												"cut": "Good",
												"price": "556",
												"color": "G",
												"carat": "0.34",
												"depth": "57.5",
												"clarity": "VS2",
												"table": "61",
												"z": "2.66"
											},
											{
												"x": "4.44",
												"y": "4.47",
												"cut": "Good",
												"price": "556",
												"color": "E",
												"carat": "0.34",
												"depth": "63.3",
												"clarity": "SI1",
												"table": "57",
												"z": "2.82"
											},
											{
												"x": "4.44",
												"y": "4.47",
												"cut": "Good",
												"price": "556",
												"color": "E",
												"carat": "0.34",
												"depth": "63.5",
												"clarity": "SI1",
												"table": "55",
												"z": "2.83"
											},
											{
												"x": "4.44",
												"y": "4.46",
												"cut": "Good",
												"price": "556",
												"color": "E",
												"carat": "0.34",
												"depth": "63.4",
												"clarity": "SI1",
												"table": "55",
												"z": "2.82"
											},
											{
												"x": "4.54",
												"y": "4.56",
												"cut": "Very Good",
												"price": "556",
												"color": "G",
												"carat": "0.34",
												"depth": "59.6",
												"clarity": "VS2",
												"table": "62",
												"z": "2.71"
											},
											{
												"x": "4.47",
												"y": "4.5",
												"cut": "Ideal",
												"price": "556",
												"color": "E",
												"carat": "0.34",
												"depth": "62.2",
												"clarity": "SI1",
												"table": "54",
												"z": "2.79"
											},
											{
												"x": "4.34",
												"y": "4.37",
												"cut": "Good",
												"price": "556",
												"color": "E",
												"carat": "0.32",
												"depth": "64.1",
												"clarity": "VS2",
												"table": "54",
												"z": "2.79"
											},
											{
												"x": "4.36",
												"y": "4.41",
												"cut": "Ideal",
												"price": "557",
												"color": "I",
												"carat": "0.31",
												"depth": "61.6",
												"clarity": "VVS1",
												"table": "55",
												"z": "2.7"
											},
											{
												"x": "4.36",
												"y": "4.38",
												"cut": "Ideal",
												"price": "557",
												"color": "I",
												"carat": "0.31",
												"depth": "61.3",
												"clarity": "VVS1",
												"table": "56",
												"z": "2.68"
											},
											{
												"x": "4.37",
												"y": "4.4",
												"cut": "Ideal",
												"price": "557",
												"color": "I",
												"carat": "0.31",
												"depth": "62.3",
												"clarity": "VVS1",
												"table": "54",
												"z": "2.73"
											},
											{
												"x": "4.37",
												"y": "4.4",
												"cut": "Ideal",
												"price": "557",
												"color": "I",
												"carat": "0.31",
												"depth": "62",
												"clarity": "VVS1",
												"table": "54",
												"z": "2.72"
											},
											{
												"x": "4.33",
												"y": "4.35",
												"cut": "Ideal",
												"price": "557",
												"color": "I",
												"carat": "0.31",
												"depth": "62.7",
												"clarity": "VVS1",
												"table": "53",
												"z": "2.72"
											},
											{
												"x": "4.36",
												"y": "4.38",
												"cut": "Ideal",
												"price": "557",
												"color": "I",
												"carat": "0.31",
												"depth": "62.2",
												"clarity": "VVS1",
												"table": "53",
												"z": "2.72"
											},
											{
												"x": "4.32",
												"y": "4.35",
												"cut": "Ideal",
												"price": "557",
												"color": "G",
												"carat": "0.31",
												"depth": "62.2",
												"clarity": "VS2",
												"table": "53.6",
												"z": "2.7"
											},
											{
												"x": "4.35",
												"y": "4.37",
												"cut": "Ideal",
												"price": "557",
												"color": "H",
												"carat": "0.31",
												"depth": "61.6",
												"clarity": "VS1",
												"table": "54.8",
												"z": "2.69"
											},
											{
												"x": "4.33",
												"y": "4.37",
												"cut": "Ideal",
												"price": "557",
												"color": "H",
												"carat": "0.31",
												"depth": "61.8",
												"clarity": "VS1",
												"table": "54.2",
												"z": "2.69"
											},
											{
												"x": "4.52",
												"y": "4.5",
												"cut": "Premium",
												"price": "557",
												"color": "G",
												"carat": "0.33",
												"depth": "59.4",
												"clarity": "SI2",
												"table": "59",
												"z": "2.68"
											},
											{
												"x": "4.43",
												"y": "4.4",
												"cut": "Premium",
												"price": "557",
												"color": "F",
												"carat": "0.33",
												"depth": "62.3",
												"clarity": "SI2",
												"table": "58",
												"z": "2.75"
											},
											{
												"x": "4.42",
												"y": "4.4",
												"cut": "Premium",
												"price": "557",
												"color": "G",
												"carat": "0.33",
												"depth": "62.6",
												"clarity": "SI2",
												"table": "58",
												"z": "2.76"
											},
											{
												"x": "4.45",
												"y": "4.41",
												"cut": "Ideal",
												"price": "557",
												"color": "G",
												"carat": "0.33",
												"depth": "61.9",
												"clarity": "SI2",
												"table": "56",
												"z": "2.74"
											},
											{
												"x": "4.42",
												"y": "4.4",
												"cut": "Premium",
												"price": "557",
												"color": "F",
												"carat": "0.33",
												"depth": "63",
												"clarity": "SI2",
												"table": "58",
												"z": "2.78"
											},
											{
												"x": "4.41",
												"y": "4.38",
												"cut": "Premium",
												"price": "557",
												"color": "J",
												"carat": "0.33",
												"depth": "62.8",
												"clarity": "VS1",
												"table": "58",
												"z": "2.76"
											},
											{
												"x": "4.46",
												"y": "4.39",
												"cut": "Premium",
												"price": "557",
												"color": "J",
												"carat": "0.33",
												"depth": "61.5",
												"clarity": "VS1",
												"table": "61",
												"z": "2.72"
											},
											{
												"x": "4.44",
												"y": "4.41",
												"cut": "Ideal",
												"price": "557",
												"color": "J",
												"carat": "0.33",
												"depth": "62.1",
												"clarity": "VS1",
												"table": "55",
												"z": "2.75"
											},
											{
												"x": "4.39",
												"y": "4.37",
												"cut": "Ideal",
												"price": "557",
												"color": "I",
												"carat": "0.33",
												"depth": "63",
												"clarity": "SI1",
												"table": "57",
												"z": "2.76"
											},
											{
												"x": "4.43",
												"y": "4.4",
												"cut": "Good",
												"price": "557",
												"color": "I",
												"carat": "0.33",
												"depth": "63.6",
												"clarity": "SI1",
												"table": "53",
												"z": "2.81"
											},
											{
												"x": "4.54",
												"y": "4.5",
												"cut": "Premium",
												"price": "557",
												"color": "I",
												"carat": "0.33",
												"depth": "60.4",
												"clarity": "SI1",
												"table": "59",
												"z": "2.73"
											},
											{
												"x": "6.21",
												"y": "5.97",
												"cut": "Fair",
												"price": "2856",
												"color": "H",
												"carat": "1",
												"depth": "66.1",
												"clarity": "SI2",
												"table": "56",
												"z": "4.04"
											},
											{
												"x": "5.92",
												"y": "5.86",
												"cut": "Premium",
												"price": "2856",
												"color": "F",
												"carat": "0.77",
												"depth": "60.8",
												"clarity": "SI1",
												"table": "59",
												"z": "3.58"
											},
											{
												"x": "5.94",
												"y": "5.9",
												"cut": "Premium",
												"price": "2856",
												"color": "F",
												"carat": "0.77",
												"depth": "61",
												"clarity": "SI1",
												"table": "58",
												"z": "3.61"
											},
											{
												"x": "5.68",
												"y": "5.71",
												"cut": "Good",
												"price": "2857",
												"color": "E",
												"carat": "0.7",
												"depth": "60.1",
												"clarity": "VVS2",
												"table": "63",
												"z": "3.42"
											},
											{
												"x": "6.08",
												"y": "6.02",
												"cut": "Very Good",
												"price": "2857",
												"color": "G",
												"carat": "0.9",
												"depth": "63.1",
												"clarity": "SI2",
												"table": "58",
												"z": "3.82"
											},
											{
												"x": "5.76",
												"y": "5.7",
												"cut": "Ideal",
												"price": "2857",
												"color": "E",
												"carat": "0.72",
												"depth": "62.3",
												"clarity": "SI1",
												"table": "57",
												"z": "3.57"
											},
											{
												"x": "6.2",
												"y": "6.14",
												"cut": "Premium",
												"price": "2857",
												"color": "I",
												"carat": "0.9",
												"depth": "61.9",
												"clarity": "VS2",
												"table": "59",
												"z": "3.82"
											},
											{
												"x": "5.76",
												"y": "5.73",
												"cut": "Premium",
												"price": "2857",
												"color": "E",
												"carat": "0.72",
												"depth": "62.1",
												"clarity": "SI1",
												"table": "58",
												"z": "3.57"
											},
											{
												"x": "5.63",
												"y": "5.71",
												"cut": "Ideal",
												"price": "2858",
												"color": "G",
												"carat": "0.7",
												"depth": "62.1",
												"clarity": "VVS2",
												"table": "56",
												"z": "3.52"
											},
											{
												"x": "6.02",
												"y": "6.05",
												"cut": "Very Good",
												"price": "2858",
												"color": "F",
												"carat": "0.81",
												"depth": "61.3",
												"clarity": "SI1",
												"table": "57",
												"z": "3.7"
											},
											{
												"x": "6",
												"y": "6.05",
												"cut": "Very Good",
												"price": "2858",
												"color": "F",
												"carat": "0.81",
												"depth": "61.7",
												"clarity": "SI1",
												"table": "57",
												"z": "3.72"
											},
											{
												"x": "5.76",
												"y": "5.69",
												"cut": "Premium",
												"price": "2858",
												"color": "E",
												"carat": "0.71",
												"depth": "61",
												"clarity": "VS2",
												"table": "60",
												"z": "3.49"
											},
											{
												"x": "5.73",
												"y": "5.7",
												"cut": "Premium",
												"price": "2858",
												"color": "E",
												"carat": "0.7",
												"depth": "61.4",
												"clarity": "VS2",
												"table": "59",
												"z": "3.51"
											},
											{
												"x": "5.76",
												"y": "5.68",
												"cut": "Premium",
												"price": "2858",
												"color": "E",
												"carat": "0.71",
												"depth": "61.5",
												"clarity": "VS2",
												"table": "60",
												"z": "3.52"
											},
											{
												"x": "5.68",
												"y": "5.59",
												"cut": "Very Good",
												"price": "2858",
												"color": "E",
												"carat": "0.71",
												"depth": "63.5",
												"clarity": "VS2",
												"table": "59",
												"z": "3.58"
											},
											{
												"x": "6.22",
												"y": "6.18",
												"cut": "Premium",
												"price": "2858",
												"color": "J",
												"carat": "0.92",
												"depth": "62.9",
												"clarity": "SI1",
												"table": "58",
												"z": "3.9"
											},
											{
												"x": "5.88",
												"y": "5.83",
												"cut": "Ideal",
												"price": "2858",
												"color": "E",
												"carat": "0.76",
												"depth": "62.7",
												"clarity": "SI1",
												"table": "54",
												"z": "3.67"
											},
											{
												"x": "5.84",
												"y": "5.8",
												"cut": "Ideal",
												"price": "2858",
												"color": "D",
												"carat": "0.73",
												"depth": "61.5",
												"clarity": "SI1",
												"table": "56",
												"z": "3.58"
											},
											{
												"x": "5.74",
												"y": "5.72",
												"cut": "Premium",
												"price": "2858",
												"color": "D",
												"carat": "0.71",
												"depth": "60.4",
												"clarity": "VS2",
												"table": "62",
												"z": "3.46"
											},
											{
												"x": "5.61",
												"y": "5.58",
												"cut": "Good",
												"price": "2858",
												"color": "E",
												"carat": "0.7",
												"depth": "63.6",
												"clarity": "VVS2",
												"table": "62",
												"z": "3.56"
											},
											{
												"x": "6.06",
												"y": "6",
												"cut": "Fair",
												"price": "2858",
												"color": "G",
												"carat": "0.9",
												"depth": "64.5",
												"clarity": "SI2",
												"table": "56",
												"z": "3.89"
											},
											{
												"x": "5.89",
												"y": "5.84",
												"cut": "Fair",
												"price": "2858",
												"color": "D",
												"carat": "0.71",
												"depth": "56.9",
												"clarity": "VS2",
												"table": "65",
												"z": "3.34"
											},
											{
												"x": "5.76",
												"y": "5.74",
												"cut": "Ideal",
												"price": "2859",
												"color": "D",
												"carat": "0.7",
												"depth": "61",
												"clarity": "VS2",
												"table": "57",
												"z": "3.51"
											},
											{
												"x": "5.72",
												"y": "5.66",
												"cut": "Premium",
												"price": "2859",
												"color": "D",
												"carat": "0.7",
												"depth": "62.4",
												"clarity": "VS2",
												"table": "56",
												"z": "3.55"
											},
											{
												"x": "5.91",
												"y": "5.88",
												"cut": "Premium",
												"price": "2859",
												"color": "F",
												"carat": "0.77",
												"depth": "60.9",
												"clarity": "VS1",
												"table": "60",
												"z": "3.59"
											},
											{
												"x": "5.74",
												"y": "5.78",
												"cut": "Ideal",
												"price": "2859",
												"color": "G",
												"carat": "0.71",
												"depth": "61.5",
												"clarity": "VS1",
												"table": "56",
												"z": "3.54"
											},
											{
												"x": "5.8",
												"y": "5.77",
												"cut": "Premium",
												"price": "2859",
												"color": "D",
												"carat": "0.7",
												"depth": "59.6",
												"clarity": "VS2",
												"table": "61",
												"z": "3.45"
											},
											{
												"x": "6.09",
												"y": "5.98",
												"cut": "Fair",
												"price": "2859",
												"color": "F",
												"carat": "0.75",
												"depth": "55.8",
												"clarity": "VS1",
												"table": "70",
												"z": "3.37"
											},
											{
												"x": "6.17",
												"y": "6.12",
												"cut": "Premium",
												"price": "2859",
												"color": "E",
												"carat": "0.83",
												"depth": "59.2",
												"clarity": "SI2",
												"table": "60",
												"z": "3.64"
											},
											{
												"x": "5.68",
												"y": "5.73",
												"cut": "Very Good",
												"price": "2860",
												"color": "F",
												"carat": "0.71",
												"depth": "61.3",
												"clarity": "VS2",
												"table": "61",
												"z": "3.5"
											},
											{
												"x": "6.07",
												"y": "6.1",
												"cut": "Very Good",
												"price": "2860",
												"color": "J",
												"carat": "0.9",
												"depth": "63.6",
												"clarity": "SI2",
												"table": "58",
												"z": "3.87"
											},
											{
												"x": "5.41",
												"y": "5.44",
												"cut": "Ideal",
												"price": "2860",
												"color": "E",
												"carat": "0.6",
												"depth": "61.9",
												"clarity": "VVS2",
												"table": "54.9",
												"z": "3.35"
											},
											{
												"x": "5.66",
												"y": "5.6",
												"cut": "Premium",
												"price": "2860",
												"color": "D",
												"carat": "0.71",
												"depth": "62.9",
												"clarity": "VS1",
												"table": "57",
												"z": "3.54"
											},
											{
												"x": "5.23",
												"y": "5.2",
												"cut": "Ideal",
												"price": "2860",
												"color": "F",
												"carat": "0.53",
												"depth": "61.4",
												"clarity": "VVS1",
												"table": "57",
												"z": "3.2"
											},
											{
												"x": "5.95",
												"y": "5.78",
												"cut": "Premium",
												"price": "2861",
												"color": "D",
												"carat": "0.71",
												"depth": "60.7",
												"clarity": "SI1",
												"table": "58",
												"z": "3.56"
											},
											{
												"x": "5.45",
												"y": "5.48",
												"cut": "Ideal",
												"price": "2861",
												"color": "G",
												"carat": "0.62",
												"depth": "61.6",
												"clarity": "VVS2",
												"table": "56",
												"z": "3.37"
											},
											{
												"x": "5.48",
												"y": "5.51",
												"cut": "Ideal",
												"price": "2861",
												"color": "G",
												"carat": "0.62",
												"depth": "61.6",
												"clarity": "VVS2",
												"table": "56",
												"z": "3.38"
											},
											{
												"x": "6.09",
												"y": "6.01",
												"cut": "Premium",
												"price": "2861",
												"color": "I",
												"carat": "0.9",
												"depth": "63",
												"clarity": "SI1",
												"table": "58",
												"z": "3.81"
											},
											{
												"x": "5.53",
												"y": "5.56",
												"cut": "Fair",
												"price": "2861",
												"color": "F",
												"carat": "0.62",
												"depth": "60.1",
												"clarity": "IF",
												"table": "61",
												"z": "3.33"
											},
											{
												"x": "6.01",
												"y": "5.98",
												"cut": "Premium",
												"price": "2861",
												"color": "E",
												"carat": "0.82",
												"depth": "61.7",
												"clarity": "SI2",
												"table": "59",
												"z": "3.7"
											},
											{
												"x": "5.67",
												"y": "5.57",
												"cut": "Premium",
												"price": "2861",
												"color": "D",
												"carat": "0.66",
												"depth": "61",
												"clarity": "VS1",
												"table": "58",
												"z": "3.43"
											},
											{
												"x": "5.67",
												"y": "5.72",
												"cut": "Very Good",
												"price": "2862",
												"color": "D",
												"carat": "0.7",
												"depth": "62.5",
												"clarity": "SI1",
												"table": "55",
												"z": "3.56"
											},
											{
												"x": "5.9",
												"y": "5.92",
												"cut": "Very Good",
												"price": "2862",
												"color": "F",
												"carat": "0.8",
												"depth": "62.6",
												"clarity": "SI1",
												"table": "58",
												"z": "3.7"
											},
											{
												"x": "5.88",
												"y": "5.92",
												"cut": "Very Good",
												"price": "2862",
												"color": "D",
												"carat": "0.8",
												"depth": "62.5",
												"clarity": "SI2",
												"table": "59",
												"z": "3.69"
											},
											{
												"x": "5.97",
												"y": "5.91",
												"cut": "Premium",
												"price": "2862",
												"color": "F",
												"carat": "0.79",
												"depth": "62.3",
												"clarity": "SI1",
												"table": "54",
												"z": "3.7"
											},
											{
												"x": "5.65",
												"y": "5.61",
												"cut": "Very Good",
												"price": "2862",
												"color": "F",
												"carat": "0.71",
												"depth": "63.2",
												"clarity": "VVS1",
												"table": "60",
												"z": "3.56"
											},
											{
												"x": "5.71",
												"y": "5.74",
												"cut": "Ideal",
												"price": "2862",
												"color": "H",
												"carat": "0.7",
												"depth": "61.1",
												"clarity": "VS2",
												"table": "57",
												"z": "3.5"
											},
											{
												"x": "5.73",
												"y": "5.69",
												"cut": "Very Good",
												"price": "2862",
												"color": "E",
												"carat": "0.7",
												"depth": "58.7",
												"clarity": "VS2",
												"table": "63",
												"z": "3.35"
											},
											{
												"x": "6.07",
												"y": "5.99",
												"cut": "Premium",
												"price": "2862",
												"color": "H",
												"carat": "0.79",
												"depth": "60",
												"clarity": "VS1",
												"table": "60",
												"z": "3.64"
											},
											{
												"x": "5.82",
												"y": "5.77",
												"cut": "Premium",
												"price": "2862",
												"color": "E",
												"carat": "0.7",
												"depth": "59.5",
												"clarity": "VS2",
												"table": "59",
												"z": "3.45"
											},
											{
												"x": "6.93",
												"y": "6.88",
												"cut": "Premium",
												"price": "2862",
												"color": "E",
												"carat": "1.22",
												"depth": "60.9",
												"clarity": "I1",
												"table": "57",
												"z": "4.21"
											},
											{
												"x": "6.21",
												"y": "6.11",
												"cut": "Fair",
												"price": "2862",
												"color": "E",
												"carat": "1.01",
												"depth": "67.6",
												"clarity": "SI2",
												"table": "57",
												"z": "4.18"
											},
											{
												"x": "5.78",
												"y": "5.64",
												"cut": "Premium",
												"price": "2862",
												"color": "E",
												"carat": "0.73",
												"depth": "62.5",
												"clarity": "VS2",
												"table": "61",
												"z": "3.59"
											},
											{
												"x": "6.05",
												"y": "6.09",
												"cut": "Good",
												"price": "2863",
												"color": "I",
												"carat": "0.91",
												"depth": "64.3",
												"clarity": "VS2",
												"table": "58",
												"z": "3.9"
											},
											{
												"x": "5.8",
												"y": "5.77",
												"cut": "Ideal",
												"price": "2863",
												"color": "D",
												"carat": "0.71",
												"depth": "60.8",
												"clarity": "SI1",
												"table": "56",
												"z": "3.52"
											},
											{
												"x": "6.01",
												"y": "5.97",
												"cut": "Premium",
												"price": "2863",
												"color": "G",
												"carat": "0.83",
												"depth": "62.3",
												"clarity": "SI1",
												"table": "58",
												"z": "3.73"
											},
											{
												"x": "6.06",
												"y": "6.01",
												"cut": "Premium",
												"price": "2863",
												"color": "F",
												"carat": "0.84",
												"depth": "62.3",
												"clarity": "SI2",
												"table": "59",
												"z": "3.76"
											},
											{
												"x": "5.82",
												"y": "5.75",
												"cut": "Premium",
												"price": "2863",
												"color": "D",
												"carat": "0.71",
												"depth": "61",
												"clarity": "SI1",
												"table": "61",
												"z": "3.53"
											},
											{
												"x": "5.82",
												"y": "5.8",
												"cut": "Premium",
												"price": "2863",
												"color": "D",
												"carat": "0.71",
												"depth": "59.7",
												"clarity": "SI1",
												"table": "59",
												"z": "3.47"
											},
											{
												"x": "5.8",
												"y": "5.68",
												"cut": "Premium",
												"price": "2863",
												"color": "D",
												"carat": "0.71",
												"depth": "61.7",
												"clarity": "SI1",
												"table": "56",
												"z": "3.54"
											},
											{
												"x": "5.75",
												"y": "5.7",
												"cut": "Ideal",
												"price": "2863",
												"color": "D",
												"carat": "0.71",
												"depth": "61.7",
												"clarity": "SI1",
												"table": "57",
												"z": "3.53"
											},
											{
												"x": "5.79",
												"y": "5.75",
												"cut": "Premium",
												"price": "2863",
												"color": "D",
												"carat": "0.71",
												"depth": "61.4",
												"clarity": "SI1",
												"table": "58",
												"z": "3.54"
											},
											{
												"x": "5.79",
												"y": "5.77",
												"cut": "Premium",
												"price": "2863",
												"color": "D",
												"carat": "0.71",
												"depth": "60.6",
												"clarity": "SI1",
												"table": "58",
												"z": "3.5"
											},
											{
												"x": "6.4",
												"y": "6.18",
												"cut": "Premium",
												"price": "2863",
												"color": "J",
												"carat": "0.91",
												"depth": "59.5",
												"clarity": "SI1",
												"table": "62",
												"z": "3.74"
											},
											{
												"x": "6.24",
												"y": "6.21",
												"cut": "Premium",
												"price": "2863",
												"color": "J",
												"carat": "0.9",
												"depth": "59.8",
												"clarity": "VS2",
												"table": "62",
												"z": "3.72"
											},
											{
												"x": "5.74",
												"y": "5.68",
												"cut": "Premium",
												"price": "2863",
												"color": "H",
												"carat": "0.71",
												"depth": "61.5",
												"clarity": "VVS2",
												"table": "62",
												"z": "3.51"
											},
											{
												"x": "5.84",
												"y": "5.8",
												"cut": "Premium",
												"price": "2863",
												"color": "E",
												"carat": "0.71",
												"depth": "59.1",
												"clarity": "SI1",
												"table": "61",
												"z": "3.44"
											},
											{
												"x": "5.91",
												"y": "5.86",
												"cut": "Ideal",
												"price": "2863",
												"color": "F",
												"carat": "0.72",
												"depth": "59.5",
												"clarity": "VS2",
												"table": "57",
												"z": "3.5"
											},
											{
												"x": "5.78",
												"y": "5.74",
												"cut": "Premium",
												"price": "2863",
												"color": "E",
												"carat": "0.72",
												"depth": "60.9",
												"clarity": "SI1",
												"table": "60",
												"z": "3.51"
											},
											{
												"x": "5.79",
												"y": "5.75",
												"cut": "Ideal",
												"price": "2863",
												"color": "E",
												"carat": "0.71",
												"depth": "61",
												"clarity": "VS2",
												"table": "55",
												"z": "3.52"
											},
											{
												"x": "6.07",
												"y": "6.04",
												"cut": "Ideal",
												"price": "2864",
												"color": "E",
												"carat": "0.81",
												"depth": "60.3",
												"clarity": "SI2",
												"table": "57",
												"z": "3.65"
											},
											{
												"x": "6.05",
												"y": "6.07",
												"cut": "Very Good",
												"price": "2865",
												"color": "I",
												"carat": "0.83",
												"depth": "61.6",
												"clarity": "VS2",
												"table": "58",
												"z": "3.73"
											},
											{
												"x": "5.87",
												"y": "5.81",
												"cut": "Premium",
												"price": "2865",
												"color": "D",
												"carat": "0.73",
												"depth": "60.8",
												"clarity": "SI1",
												"table": "55",
												"z": "3.55"
											},
											{
												"x": "5.25",
												"y": "5.3",
												"cut": "Very Good",
												"price": "2866",
												"color": "D",
												"carat": "0.56",
												"depth": "62",
												"clarity": "VVS1",
												"table": "56",
												"z": "3.27"
											},
											{
												"x": "5.27",
												"y": "5.31",
												"cut": "Very Good",
												"price": "2866",
												"color": "D",
												"carat": "0.56",
												"depth": "61.8",
												"clarity": "VVS1",
												"table": "55",
												"z": "3.27"
											},
											{
												"x": "5.74",
												"y": "5.7",
												"cut": "Ideal",
												"price": "2866",
												"color": "E",
												"carat": "0.71",
												"depth": "62.2",
												"clarity": "VS1",
												"table": "55",
												"z": "3.56"
											},
											{
												"x": "5.66",
												"y": "5.7",
												"cut": "Ideal",
												"price": "2866",
												"color": "H",
												"carat": "0.7",
												"depth": "62.3",
												"clarity": "VVS1",
												"table": "58",
												"z": "3.54"
											},
											{
												"x": "6.39",
												"y": "6.3",
												"cut": "Premium",
												"price": "2866",
												"color": "I",
												"carat": "0.96",
												"depth": "61.3",
												"clarity": "SI1",
												"table": "58",
												"z": "3.89"
											},
											{
												"x": "5.67",
												"y": "5.69",
												"cut": "Very Good",
												"price": "2867",
												"color": "H",
												"carat": "0.71",
												"depth": "62.9",
												"clarity": "VVS1",
												"table": "57",
												"z": "3.57"
											},
											{
												"x": "5.68",
												"y": "5.61",
												"cut": "Ideal",
												"price": "2867",
												"color": "D",
												"carat": "0.7",
												"depth": "62.4",
												"clarity": "VS2",
												"table": "57",
												"z": "3.52"
											},
											{
												"x": "5.78",
												"y": "5.81",
												"cut": "Ideal",
												"price": "2867",
												"color": "H",
												"carat": "0.71",
												"depth": "60.4",
												"clarity": "VVS1",
												"table": "57",
												"z": "3.5"
											},
											{
												"x": "6.05",
												"y": "5.98",
												"cut": "Premium",
												"price": "2867",
												"color": "H",
												"carat": "0.8",
												"depth": "61.2",
												"clarity": "VS2",
												"table": "53",
												"z": "3.68"
											},
											{
												"x": "6.49",
												"y": "6.41",
												"cut": "Premium",
												"price": "2867",
												"color": "F",
												"carat": "0.95",
												"depth": "58.4",
												"clarity": "SI2",
												"table": "57",
												"z": "3.77"
											},
											{
												"x": "5.99",
												"y": "5.95",
												"cut": "Ideal",
												"price": "2867",
												"color": "F",
												"carat": "0.82",
												"depth": "62.3",
												"clarity": "SI2",
												"table": "56",
												"z": "3.72"
											},
											{
												"x": "5.21",
												"y": "5.19",
												"cut": "Ideal",
												"price": "2867",
												"color": "F",
												"carat": "0.52",
												"depth": "61.2",
												"clarity": "VVS1",
												"table": "56",
												"z": "3.18"
											},
											{
												"x": "6.12",
												"y": "6",
												"cut": "Ideal",
												"price": "2867",
												"color": "F",
												"carat": "0.82",
												"depth": "61.7",
												"clarity": "SI2",
												"table": "53",
												"z": "3.74"
											},
											{
												"x": "6.02",
												"y": "5.96",
												"cut": "Ideal",
												"price": "2867",
												"color": "F",
												"carat": "0.82",
												"depth": "62.3",
												"clarity": "SI2",
												"table": "56",
												"z": "3.73"
											},
											{
												"x": "6.14",
												"y": "6.12",
												"cut": "Premium",
												"price": "2867",
												"color": "F",
												"carat": "0.82",
												"depth": "59.7",
												"clarity": "SI2",
												"table": "57",
												"z": "3.66"
											},
											{
												"x": "5.96",
												"y": "5.91",
												"cut": "Ideal",
												"price": "2867",
												"color": "G",
												"carat": "0.8",
												"depth": "61.3",
												"clarity": "SI1",
												"table": "57",
												"z": "3.64"
											},
											{
												"x": "6.07",
												"y": "5.88",
												"cut": "Fair",
												"price": "2867",
												"color": "F",
												"carat": "0.96",
												"depth": "68.2",
												"clarity": "SI2",
												"table": "61",
												"z": "4.1"
											},
											{
												"x": "5.72",
												"y": "5.75",
												"cut": "Ideal",
												"price": "2868",
												"color": "I",
												"carat": "0.72",
												"depth": "62.4",
												"clarity": "VS1",
												"table": "55",
												"z": "3.58"
											},
											{
												"x": "5.52",
												"y": "5.56",
												"cut": "Ideal",
												"price": "2868",
												"color": "G",
												"carat": "0.62",
												"depth": "60.5",
												"clarity": "IF",
												"table": "57",
												"z": "3.35"
											},
											{
												"x": "5.96",
												"y": "5.9",
												"cut": "Premium",
												"price": "2868",
												"color": "E",
												"carat": "0.79",
												"depth": "61",
												"clarity": "SI2",
												"table": "58",
												"z": "3.62"
											},
											{
												"x": "5.78",
												"y": "5.7",
												"cut": "Very Good",
												"price": "2868",
												"color": "E",
												"carat": "0.75",
												"depth": "63.1",
												"clarity": "SI1",
												"table": "56",
												"z": "3.62"
											},
											{
												"x": "6.55",
												"y": "6.48",
												"cut": "Premium",
												"price": "2869",
												"color": "D",
												"carat": "1.08",
												"depth": "61.9",
												"clarity": "I1",
												"table": "60",
												"z": "4.03"
											},
											{
												"x": "5.77",
												"y": "5.84",
												"cut": "Ideal",
												"price": "2869",
												"color": "E",
												"carat": "0.72",
												"depth": "60.8",
												"clarity": "SI1",
												"table": "55",
												"z": "3.53"
											},
											{
												"x": "5.43",
												"y": "5.47",
												"cut": "Ideal",
												"price": "2869",
												"color": "G",
												"carat": "0.62",
												"depth": "61.8",
												"clarity": "IF",
												"table": "56",
												"z": "3.37"
											},
											{
												"x": "5.84",
												"y": "5.81",
												"cut": "Ideal",
												"price": "2869",
												"color": "G",
												"carat": "0.73",
												"depth": "61.3",
												"clarity": "VVS2",
												"table": "57",
												"z": "3.57"
											},
											{
												"x": "5.79",
												"y": "5.77",
												"cut": "Ideal",
												"price": "2869",
												"color": "H",
												"carat": "0.72",
												"depth": "60.9",
												"clarity": "VVS2",
												"table": "57",
												"z": "3.52"
											},
											{
												"x": "5.16",
												"y": "5.13",
												"cut": "Premium",
												"price": "2870",
												"color": "F",
												"carat": "0.52",
												"depth": "61.8",
												"clarity": "VVS2",
												"table": "60",
												"z": "3.18"
											},
											{
												"x": "6",
												"y": "6.05",
												"cut": "Ideal",
												"price": "2870",
												"color": "E",
												"carat": "0.83",
												"depth": "62.2",
												"clarity": "SI2",
												"table": "57",
												"z": "3.75"
											},
											{
												"x": "5.56",
												"y": "5.51",
												"cut": "Premium",
												"price": "2870",
												"color": "E",
												"carat": "0.64",
												"depth": "62.1",
												"clarity": "VVS2",
												"table": "58",
												"z": "3.44"
											},
											{
												"x": "5.94",
												"y": "5.9",
												"cut": "Ideal",
												"price": "2870",
												"color": "G",
												"carat": "0.8",
												"depth": "62.5",
												"clarity": "SI1",
												"table": "57",
												"z": "3.7"
											},
											{
												"x": "5.77",
												"y": "5.83",
												"cut": "Ideal",
												"price": "2870",
												"color": "H",
												"carat": "0.74",
												"depth": "62.1",
												"clarity": "SI1",
												"table": "56",
												"z": "3.6"
											},
											{
												"x": "5.72",
												"y": "5.79",
												"cut": "Ideal",
												"price": "2870",
												"color": "F",
												"carat": "0.72",
												"depth": "61.5",
												"clarity": "SI1",
												"table": "56",
												"z": "3.54"
											},
											{
												"x": "6.12",
												"y": "6.09",
												"cut": "Ideal",
												"price": "2870",
												"color": "H",
												"carat": "0.82",
												"depth": "59.5",
												"clarity": "VS2",
												"table": "57",
												"z": "3.63"
											},
											{
												"x": "5.81",
												"y": "5.78",
												"cut": "Premium",
												"price": "2870",
												"color": "E",
												"carat": "0.73",
												"depth": "61.3",
												"clarity": "VS1",
												"table": "59",
												"z": "3.55"
											},
											{
												"x": "6.47",
												"y": "6.45",
												"cut": "Premium",
												"price": "2870",
												"color": "I",
												"carat": "1.04",
												"depth": "61.6",
												"clarity": "I1",
												"table": "61",
												"z": "3.98"
											},
											{
												"x": "5.76",
												"y": "5.83",
												"cut": "Very Good",
												"price": "2871",
												"color": "E",
												"carat": "0.73",
												"depth": "61.3",
												"clarity": "SI1",
												"table": "58",
												"z": "3.55"
											},
											{
												"x": "5.7",
												"y": "5.72",
												"cut": "Good",
												"price": "2871",
												"color": "E",
												"carat": "0.73",
												"depth": "63.6",
												"clarity": "SI1",
												"table": "57",
												"z": "3.63"
											},
											{
												"x": "6.13",
												"y": "6.03",
												"cut": "Premium",
												"price": "2871",
												"color": "J",
												"carat": "0.9",
												"depth": "62.8",
												"clarity": "SI1",
												"table": "59",
												"z": "3.82"
											},
											{
												"x": "5.83",
												"y": "5.85",
												"cut": "Ideal",
												"price": "2871",
												"color": "I",
												"carat": "0.75",
												"depth": "61.8",
												"clarity": "SI1",
												"table": "55",
												"z": "3.61"
											},
											{
												"x": "5.91",
												"y": "5.95",
												"cut": "Ideal",
												"price": "2871",
												"color": "G",
												"carat": "0.79",
												"depth": "62.6",
												"clarity": "SI1",
												"table": "55",
												"z": "3.71"
											},
											{
												"x": "5.59",
												"y": "5.62",
												"cut": "Good",
												"price": "2872",
												"color": "D",
												"carat": "0.7",
												"depth": "62.5",
												"clarity": "SI1",
												"table": "56.7",
												"z": "3.51"
											},
											{
												"x": "5.87",
												"y": "5.92",
												"cut": "Very Good",
												"price": "2872",
												"color": "D",
												"carat": "0.75",
												"depth": "60.7",
												"clarity": "SI1",
												"table": "55",
												"z": "3.58"
											},
											{
												"x": "6.44",
												"y": "6.49",
												"cut": "Ideal",
												"price": "2872",
												"color": "I",
												"carat": "1.02",
												"depth": "61.7",
												"clarity": "I1",
												"table": "56",
												"z": "3.99"
											},
											{
												"x": "5.79",
												"y": "5.81",
												"cut": "Very Good",
												"price": "2872",
												"color": "G",
												"carat": "0.7",
												"depth": "59",
												"clarity": "SI2",
												"table": "62",
												"z": "3.42"
											},
											{
												"x": "5.63",
												"y": "5.73",
												"cut": "Ideal",
												"price": "2872",
												"color": "D",
												"carat": "0.7",
												"depth": "61.8",
												"clarity": "SI1",
												"table": "56",
												"z": "3.51"
											},
											{
												"x": "5.66",
												"y": "5.71",
												"cut": "Good",
												"price": "2872",
												"color": "E",
												"carat": "0.7",
												"depth": "61.4",
												"clarity": "SI1",
												"table": "64",
												"z": "3.49"
											},
											{
												"x": "5.71",
												"y": "5.75",
												"cut": "Ideal",
												"price": "2872",
												"color": "D",
												"carat": "0.7",
												"depth": "61.4",
												"clarity": "SI1",
												"table": "54",
												"z": "3.52"
											},
											{
												"x": "5.72",
												"y": "5.75",
												"cut": "Ideal",
												"price": "2872",
												"color": "D",
												"carat": "0.7",
												"depth": "60.7",
												"clarity": "SI1",
												"table": "56",
												"z": "3.48"
											},
											{
												"x": "5.75",
												"y": "5.82",
												"cut": "Very Good",
												"price": "2872",
												"color": "D",
												"carat": "0.7",
												"depth": "60.2",
												"clarity": "SI1",
												"table": "60",
												"z": "3.48"
											},
											{
												"x": "5.89",
												"y": "5.94",
												"cut": "Very Good",
												"price": "2872",
												"color": "E",
												"carat": "0.72",
												"depth": "58.3",
												"clarity": "VS2",
												"table": "57",
												"z": "3.45"
											},
											{
												"x": "5.74",
												"y": "5.78",
												"cut": "Ideal",
												"price": "2872",
												"color": "E",
												"carat": "0.74",
												"depth": "62.3",
												"clarity": "SI1",
												"table": "58",
												"z": "3.59"
											},
											{
												"x": "5.88",
												"y": "5.97",
												"cut": "Good",
												"price": "2872",
												"color": "G",
												"carat": "0.84",
												"depth": "65.1",
												"clarity": "SI1",
												"table": "55",
												"z": "3.86"
											},
											{
												"x": "5.8",
												"y": "5.86",
												"cut": "Very Good",
												"price": "2873",
												"color": "F",
												"carat": "0.76",
												"depth": "62",
												"clarity": "VS2",
												"table": "58",
												"z": "3.62"
											},
											{
												"x": "5.8",
												"y": "5.84",
												"cut": "Very Good",
												"price": "2873",
												"color": "E",
												"carat": "0.77",
												"depth": "63.2",
												"clarity": "SI1",
												"table": "58",
												"z": "3.68"
											},
											{
												"x": "5.78",
												"y": "5.82",
												"cut": "Ideal",
												"price": "2873",
												"color": "E",
												"carat": "0.76",
												"depth": "62.8",
												"clarity": "SI2",
												"table": "56",
												"z": "3.64"
											},
											{
												"x": "6.45",
												"y": "6.41",
												"cut": "Ideal",
												"price": "2873",
												"color": "I",
												"carat": "1",
												"depth": "61.7",
												"clarity": "SI2",
												"table": "56",
												"z": "3.97"
											},
											{
												"x": "6.14",
												"y": "6.07",
												"cut": "Fair",
												"price": "2873",
												"color": "H",
												"carat": "1",
												"depth": "65.5",
												"clarity": "SI1",
												"table": "62",
												"z": "4"
											},
											{
												"x": "6.03",
												"y": "6",
												"cut": "Fair",
												"price": "2873",
												"color": "I",
												"carat": "0.9",
												"depth": "65.7",
												"clarity": "SI1",
												"table": "58",
												"z": "3.95"
											},
											{
												"x": "6.16",
												"y": "6.13",
												"cut": "Premium",
												"price": "2873",
												"color": "J",
												"carat": "0.9",
												"depth": "61.8",
												"clarity": "SI1",
												"table": "58",
												"z": "3.8"
											},
											{
												"x": "6",
												"y": "5.96",
												"cut": "Good",
												"price": "2873",
												"color": "J",
												"carat": "0.9",
												"depth": "64",
												"clarity": "SI1",
												"table": "61",
												"z": "3.83"
											},
											{
												"x": "5.98",
												"y": "5.94",
												"cut": "Fair",
												"price": "2873",
												"color": "I",
												"carat": "0.9",
												"depth": "65.3",
												"clarity": "SI1",
												"table": "61",
												"z": "3.89"
											},
											{
												"x": "6.01",
												"y": "5.96",
												"cut": "Fair",
												"price": "2873",
												"color": "I",
												"carat": "0.9",
												"depth": "65.8",
												"clarity": "SI1",
												"table": "56",
												"z": "3.94"
											},
											{
												"x": "6.26",
												"y": "6.22",
												"cut": "Premium",
												"price": "2873",
												"color": "J",
												"carat": "0.9",
												"depth": "60.9",
												"clarity": "SI1",
												"table": "61",
												"z": "3.8"
											},
											{
												"x": "5.91",
												"y": "5.82",
												"cut": "Premium",
												"price": "2874",
												"color": "F",
												"carat": "0.78",
												"depth": "62.6",
												"clarity": "VS2",
												"table": "58",
												"z": "3.67"
											},
											{
												"x": "5.69",
												"y": "5.74",
												"cut": "Premium",
												"price": "2874",
												"color": "D",
												"carat": "0.71",
												"depth": "61.2",
												"clarity": "VS2",
												"table": "59",
												"z": "3.5"
											},
											{
												"x": "5.79",
												"y": "5.77",
												"cut": "Premium",
												"price": "2874",
												"color": "F",
												"carat": "0.7",
												"depth": "59",
												"clarity": "VS1",
												"table": "59",
												"z": "3.41"
											},
											{
												"x": "5.71",
												"y": "5.67",
												"cut": "Premium",
												"price": "2874",
												"color": "F",
												"carat": "0.7",
												"depth": "60.8",
												"clarity": "VS1",
												"table": "62",
												"z": "3.46"
											},
											{
												"x": "5.67",
												"y": "5.63",
												"cut": "Premium",
												"price": "2874",
												"color": "G",
												"carat": "0.7",
												"depth": "61.8",
												"clarity": "VVS2",
												"table": "58",
												"z": "3.49"
											},
											{
												"x": "5.77",
												"y": "5.73",
												"cut": "Ideal",
												"price": "2874",
												"color": "F",
												"carat": "0.7",
												"depth": "61",
												"clarity": "VS1",
												"table": "55",
												"z": "3.51"
											},
											{
												"x": "5.75",
												"y": "5.71",
												"cut": "Ideal",
												"price": "2874",
												"color": "F",
												"carat": "0.7",
												"depth": "61.6",
												"clarity": "VS1",
												"table": "55",
												"z": "3.53"
											},
											{
												"x": "5.69",
												"y": "5.65",
												"cut": "Ideal",
												"price": "2874",
												"color": "F",
												"carat": "0.7",
												"depth": "62.4",
												"clarity": "VS1",
												"table": "56",
												"z": "3.54"
											},
											{
												"x": "5.68",
												"y": "5.61",
												"cut": "Premium",
												"price": "2874",
												"color": "G",
												"carat": "0.7",
												"depth": "62.9",
												"clarity": "VVS2",
												"table": "59",
												"z": "3.55"
											},
											{
												"x": "6.11",
												"y": "5.98",
												"cut": "Fair",
												"price": "2875",
												"color": "H",
												"carat": "1",
												"depth": "67.7",
												"clarity": "SI2",
												"table": "60",
												"z": "4.09"
											},
											{
												"x": "5.84",
												"y": "5.9",
												"cut": "Ideal",
												"price": "2875",
												"color": "H",
												"carat": "0.77",
												"depth": "62.4",
												"clarity": "SI1",
												"table": "56",
												"z": "3.66"
											},
											{
												"x": "6.3",
												"y": "6.25",
												"cut": "Fair",
												"price": "2875",
												"color": "J",
												"carat": "1",
												"depth": "65.5",
												"clarity": "VS1",
												"table": "55",
												"z": "4.11"
											},
											{
												"x": "6.08",
												"y": "6.03",
												"cut": "Fair",
												"price": "2875",
												"color": "I",
												"carat": "1",
												"depth": "66.3",
												"clarity": "SI1",
												"table": "61",
												"z": "4.01"
											},
											{
												"x": "6.17",
												"y": "6.1",
												"cut": "Fair",
												"price": "2875",
												"color": "H",
												"carat": "1",
												"depth": "69.5",
												"clarity": "SI2",
												"table": "55",
												"z": "4.26"
											},
											{
												"x": "5.68",
												"y": "5.75",
												"cut": "Premium",
												"price": "2876",
												"color": "E",
												"carat": "0.73",
												"depth": "62.6",
												"clarity": "VS1",
												"table": "60",
												"z": "3.58"
											},
											{
												"x": "6.04",
												"y": "5.98",
												"cut": "Premium",
												"price": "2876",
												"color": "E",
												"carat": "0.79",
												"depth": "60.6",
												"clarity": "VS2",
												"table": "53",
												"z": "3.64"
											},
											{
												"x": "5.74",
												"y": "5.76",
												"cut": "Very Good",
												"price": "2877",
												"color": "H",
												"carat": "0.72",
												"depth": "62.2",
												"clarity": "VS1",
												"table": "54",
												"z": "3.57"
											},
											{
												"x": "5.75",
												"y": "5.7",
												"cut": "Ideal",
												"price": "2877",
												"color": "E",
												"carat": "0.71",
												"depth": "62.4",
												"clarity": "VS1",
												"table": "56",
												"z": "3.57"
											},
											{
												"x": "5.8",
												"y": "5.83",
												"cut": "Ideal",
												"price": "2877",
												"color": "G",
												"carat": "0.74",
												"depth": "62.3",
												"clarity": "VS2",
												"table": "55",
												"z": "3.62"
											},
											{
												"x": "5.6",
												"y": "5.66",
												"cut": "Good",
												"price": "2877",
												"color": "H",
												"carat": "0.7",
												"depth": "62.7",
												"clarity": "VVS1",
												"table": "56",
												"z": "3.53"
											},
											{
												"x": "5.82",
												"y": "5.86",
												"cut": "Good",
												"price": "2877",
												"color": "F",
												"carat": "0.7",
												"depth": "59.1",
												"clarity": "VS1",
												"table": "62",
												"z": "3.44"
											},
											{
												"x": "5.86",
												"y": "5.89",
												"cut": "Very Good",
												"price": "2878",
												"color": "F",
												"carat": "0.79",
												"depth": "62.8",
												"clarity": "SI1",
												"table": "59",
												"z": "3.69"
											},
											{
												"x": "5.82",
												"y": "5.89",
												"cut": "Very Good",
												"price": "2878",
												"color": "F",
												"carat": "0.79",
												"depth": "62.7",
												"clarity": "SI1",
												"table": "60",
												"z": "3.67"
											},
											{
												"x": "6",
												"y": "6.07",
												"cut": "Very Good",
												"price": "2878",
												"color": "D",
												"carat": "0.79",
												"depth": "59.7",
												"clarity": "SI2",
												"table": "58",
												"z": "3.6"
											},
											{
												"x": "5.76",
												"y": "5.78",
												"cut": "Ideal",
												"price": "2878",
												"color": "I",
												"carat": "0.71",
												"depth": "61.5",
												"clarity": "VS2",
												"table": "55",
												"z": "3.55"
											},
											{
												"x": "5.88",
												"y": "5.9",
												"cut": "Ideal",
												"price": "2878",
												"color": "F",
												"carat": "0.79",
												"depth": "62.8",
												"clarity": "SI1",
												"table": "56",
												"z": "3.7"
											},
											{
												"x": "5.81",
												"y": "5.86",
												"cut": "Very Good",
												"price": "2879",
												"color": "F",
												"carat": "0.73",
												"depth": "61.4",
												"clarity": "SI1",
												"table": "56",
												"z": "3.58"
											},
											{
												"x": "5.55",
												"y": "5.53",
												"cut": "Premium",
												"price": "2879",
												"color": "E",
												"carat": "0.63",
												"depth": "60.3",
												"clarity": "IF",
												"table": "62",
												"z": "3.34"
											},
											{
												"x": "5.73",
												"y": "5.7",
												"cut": "Premium",
												"price": "2879",
												"color": "F",
												"carat": "0.7",
												"depth": "60.4",
												"clarity": "VS1",
												"table": "60",
												"z": "3.45"
											},
											{
												"x": "5.71",
												"y": "5.67",
												"cut": "Premium",
												"price": "2879",
												"color": "F",
												"carat": "0.71",
												"depth": "62.7",
												"clarity": "VS1",
												"table": "58",
												"z": "3.57"
											},
											{
												"x": "6.13",
												"y": "6.1",
												"cut": "Ideal",
												"price": "2879",
												"color": "G",
												"carat": "0.84",
												"depth": "61",
												"clarity": "SI2",
												"table": "56",
												"z": "3.73"
											},
											{
												"x": "6.08",
												"y": "6.03",
												"cut": "Ideal",
												"price": "2879",
												"color": "G",
												"carat": "0.84",
												"depth": "62.3",
												"clarity": "SI2",
												"table": "55",
												"z": "3.77"
											},
											{
												"x": "6.53",
												"y": "6.5",
												"cut": "Ideal",
												"price": "2879",
												"color": "J",
												"carat": "1.02",
												"depth": "60.3",
												"clarity": "SI2",
												"table": "54",
												"z": "3.93"
											},
											{
												"x": "5.93",
												"y": "5.77",
												"cut": "Fair",
												"price": "2879",
												"color": "F",
												"carat": "0.72",
												"depth": "56.9",
												"clarity": "VS1",
												"table": "69",
												"z": "3.33"
											},
											{
												"x": "5.76",
												"y": "5.73",
												"cut": "Ideal",
												"price": "2879",
												"color": "F",
												"carat": "0.72",
												"depth": "62",
												"clarity": "VS1",
												"table": "56",
												"z": "3.56"
											},
											{
												"x": "6.34",
												"y": "6.43",
												"cut": "Very Good",
												"price": "2880",
												"color": "J",
												"carat": "0.92",
												"depth": "58.7",
												"clarity": "SI2",
												"table": "61",
												"z": "3.75"
											},
											{
												"x": "5.72",
												"y": "5.74",
												"cut": "Very Good",
												"price": "2880",
												"color": "D",
												"carat": "0.74",
												"depth": "63.9",
												"clarity": "SI1",
												"table": "57",
												"z": "3.66"
											},
											{
												"x": "5.74",
												"y": "5.71",
												"cut": "Ideal",
												"price": "2881",
												"color": "H",
												"carat": "0.7",
												"depth": "62",
												"clarity": "VVS1",
												"table": "55",
												"z": "3.55"
											},
											{
												"x": "5.84",
												"y": "5.83",
												"cut": "Very Good",
												"price": "2881",
												"color": "E",
												"carat": "0.71",
												"depth": "60",
												"clarity": "VS2",
												"table": "59",
												"z": "3.5"
											},
											{
												"x": "6.5",
												"y": "6.47",
												"cut": "Premium",
												"price": "2881",
												"color": "H",
												"carat": "1.05",
												"depth": "62",
												"clarity": "I1",
												"table": "59",
												"z": "4.02"
											},
											{
												"x": "5.62",
												"y": "5.65",
												"cut": "Very Good",
												"price": "2882",
												"color": "H",
												"carat": "0.7",
												"depth": "62.8",
												"clarity": "IF",
												"table": "56",
												"z": "3.54"
											},
											{
												"x": "5.23",
												"y": "5.26",
												"cut": "Ideal",
												"price": "2882",
												"color": "F",
												"carat": "0.54",
												"depth": "61.8",
												"clarity": "VVS1",
												"table": "56",
												"z": "3.24"
											},
											{
												"x": "5.87",
												"y": "5.84",
												"cut": "Premium",
												"price": "2882",
												"color": "F",
												"carat": "0.73",
												"depth": "59.9",
												"clarity": "VS2",
												"table": "58",
												"z": "3.51"
											},
											{
												"x": "6.39",
												"y": "6.32",
												"cut": "Fair",
												"price": "2882",
												"color": "F",
												"carat": "0.88",
												"depth": "56.6",
												"clarity": "SI1",
												"table": "65",
												"z": "3.6"
											},
											{
												"x": "5.97",
												"y": "5.92",
												"cut": "Premium",
												"price": "2882",
												"color": "F",
												"carat": "0.73",
												"depth": "58.7",
												"clarity": "VS2",
												"table": "57",
												"z": "3.49"
											},
											{
												"x": "5.75",
												"y": "5.81",
												"cut": "Ideal",
												"price": "2883",
												"color": "D",
												"carat": "0.72",
												"depth": "61.8",
												"clarity": "SI1",
												"table": "56",
												"z": "3.57"
											},
											{
												"x": "6.09",
												"y": "6",
												"cut": "Good",
												"price": "2883",
												"color": "H",
												"carat": "0.9",
												"depth": "62.7",
												"clarity": "SI2",
												"table": "64",
												"z": "3.79"
											},
											{
												"x": "6.01",
												"y": "5.96",
												"cut": "Fair",
												"price": "2883",
												"color": "H",
												"carat": "0.9",
												"depth": "65",
												"clarity": "SI2",
												"table": "61",
												"z": "3.89"
											},
											{
												"x": "6.32",
												"y": "6.27",
												"cut": "Fair",
												"price": "2884",
												"color": "I",
												"carat": "1.03",
												"depth": "65.3",
												"clarity": "SI2",
												"table": "55",
												"z": "4.11"
											},
											{
												"x": "5.95",
												"y": "6",
												"cut": "Very Good",
												"price": "2885",
												"color": "F",
												"carat": "0.84",
												"depth": "63.8",
												"clarity": "SI1",
												"table": "57",
												"z": "3.81"
											},
											{
												"x": "6.36",
												"y": "6.27",
												"cut": "Premium",
												"price": "2885",
												"color": "I",
												"carat": "1.01",
												"depth": "62.7",
												"clarity": "SI1",
												"table": "60",
												"z": "3.96"
											},
											{
												"x": "5.9",
												"y": "5.93",
												"cut": "Ideal",
												"price": "2885",
												"color": "D",
												"carat": "0.77",
												"depth": "61.5",
												"clarity": "SI2",
												"table": "55",
												"z": "3.64"
											},
											{
												"x": "6.22",
												"y": "6.14",
												"cut": "Fair",
												"price": "2885",
												"color": "E",
												"carat": "0.8",
												"depth": "56.3",
												"clarity": "SI1",
												"table": "63",
												"z": "3.48"
											},
											{
												"x": "6.02",
												"y": "5.9",
												"cut": "Fair",
												"price": "2885",
												"color": "D",
												"carat": "0.9",
												"depth": "66.9",
												"clarity": "SI2",
												"table": "57",
												"z": "3.99"
											},
											{
												"x": "5.79",
												"y": "5.81",
												"cut": "Ideal",
												"price": "2886",
												"color": "E",
												"carat": "0.73",
												"depth": "61.4",
												"clarity": "SI1",
												"table": "56",
												"z": "3.56"
											},
											{
												"x": "5.64",
												"y": "5.69",
												"cut": "Ideal",
												"price": "2886",
												"color": "E",
												"carat": "0.72",
												"depth": "62.7",
												"clarity": "SI1",
												"table": "55",
												"z": "3.55"
											},
											{
												"x": "5.71",
												"y": "5.79",
												"cut": "Very Good",
												"price": "2887",
												"color": "D",
												"carat": "0.71",
												"depth": "62.4",
												"clarity": "SI1",
												"table": "54",
												"z": "3.59"
											},
											{
												"x": "5.66",
												"y": "5.69",
												"cut": "Premium",
												"price": "2887",
												"color": "E",
												"carat": "0.7",
												"depth": "62.6",
												"clarity": "VS1",
												"table": "59",
												"z": "3.55"
											},
											{
												"x": "5.93",
												"y": "5.96",
												"cut": "Ideal",
												"price": "2888",
												"color": "I",
												"carat": "0.79",
												"depth": "61.7",
												"clarity": "VS1",
												"table": "59",
												"z": "3.67"
											},
											{
												"x": "5.68",
												"y": "5.72",
												"cut": "Very Good",
												"price": "2889",
												"color": "G",
												"carat": "0.72",
												"depth": "62.5",
												"clarity": "VVS2",
												"table": "58",
												"z": "3.56"
											},
											{
												"x": "5.62",
												"y": "5.66",
												"cut": "Very Good",
												"price": "2889",
												"color": "E",
												"carat": "0.7",
												"depth": "63.5",
												"clarity": "VS2",
												"table": "54",
												"z": "3.58"
											},
											{
												"x": "5.64",
												"y": "5.75",
												"cut": "Very Good",
												"price": "2889",
												"color": "F",
												"carat": "0.7",
												"depth": "62.2",
												"clarity": "VS1",
												"table": "58",
												"z": "3.54"
											},
											{
												"x": "6.09",
												"y": "6.14",
												"cut": "Good",
												"price": "2889",
												"color": "H",
												"carat": "0.9",
												"depth": "63.5",
												"clarity": "SI2",
												"table": "58",
												"z": "3.88"
											},
											{
												"x": "5.69",
												"y": "5.72",
												"cut": "Very Good",
												"price": "2889",
												"color": "F",
												"carat": "0.71",
												"depth": "62.8",
												"clarity": "VS1",
												"table": "56",
												"z": "3.58"
											},
											{
												"x": "5.08",
												"y": "5.12",
												"cut": "Ideal",
												"price": "2889",
												"color": "E",
												"carat": "0.5",
												"depth": "62.2",
												"clarity": "VVS2",
												"table": "54",
												"z": "3.17"
											},
											{
												"x": "5.09",
												"y": "5.11",
												"cut": "Ideal",
												"price": "2889",
												"color": "E",
												"carat": "0.5",
												"depth": "62.2",
												"clarity": "VVS2",
												"table": "54",
												"z": "3.17"
											},
											{
												"x": "5.83",
												"y": "5.87",
												"cut": "Ideal",
												"price": "2889",
												"color": "F",
												"carat": "0.74",
												"depth": "61.2",
												"clarity": "SI1",
												"table": "56",
												"z": "3.58"
											},
											{
												"x": "5.94",
												"y": "5.9",
												"cut": "Premium",
												"price": "2889",
												"color": "F",
												"carat": "0.77",
												"depth": "61.8",
												"clarity": "VS2",
												"table": "56",
												"z": "3.66"
											},
											{
												"x": "5.99",
												"y": "5.95",
												"cut": "Premium",
												"price": "2889",
												"color": "E",
												"carat": "0.77",
												"depth": "59.8",
												"clarity": "SI1",
												"table": "61",
												"z": "3.57"
											},
											{
												"x": "6.07",
												"y": "6",
												"cut": "Ideal",
												"price": "2890",
												"color": "F",
												"carat": "0.8",
												"depth": "61.5",
												"clarity": "SI1",
												"table": "54",
												"z": "3.71"
											},
											{
												"x": "5.9",
												"y": "5.87",
												"cut": "Ideal",
												"price": "2890",
												"color": "F",
												"carat": "0.8",
												"depth": "62.4",
												"clarity": "SI1",
												"table": "57",
												"z": "3.67"
											},
											{
												"x": "5.97",
												"y": "5.94",
												"cut": "Premium",
												"price": "2890",
												"color": "F",
												"carat": "0.8",
												"depth": "61.5",
												"clarity": "SI1",
												"table": "60",
												"z": "3.66"
											},
											{
												"x": "5.87",
												"y": "5.83",
												"cut": "Good",
												"price": "2890",
												"color": "F",
												"carat": "0.8",
												"depth": "63.8",
												"clarity": "SI1",
												"table": "59",
												"z": "3.73"
											},
											{
												"x": "5.61",
												"y": "5.58",
												"cut": "Ideal",
												"price": "2890",
												"color": "G",
												"carat": "0.66",
												"depth": "61.5",
												"clarity": "VVS1",
												"table": "56",
												"z": "3.44"
											},
											{
												"x": "5.71",
												"y": "5.79",
												"cut": "Very Good",
												"price": "2891",
												"color": "E",
												"carat": "0.71",
												"depth": "61.2",
												"clarity": "VS2",
												"table": "58",
												"z": "3.52"
											},
											{
												"x": "5.73",
												"y": "5.77",
												"cut": "Ideal",
												"price": "2891",
												"color": "F",
												"carat": "0.71",
												"depth": "61.2",
												"clarity": "VS2",
												"table": "56",
												"z": "3.52"
											},
											{
												"x": "5.74",
												"y": "5.76",
												"cut": "Ideal",
												"price": "2891",
												"color": "E",
												"carat": "0.71",
												"depth": "61.6",
												"clarity": "VS2",
												"table": "56",
												"z": "3.54"
											},
											{
												"x": "5.71",
												"y": "5.75",
												"cut": "Ideal",
												"price": "2891",
												"color": "E",
												"carat": "0.71",
												"depth": "62.7",
												"clarity": "VS2",
												"table": "56",
												"z": "3.59"
											},
											{
												"x": "5.78",
												"y": "5.81",
												"cut": "Ideal",
												"price": "2891",
												"color": "D",
												"carat": "0.72",
												"depth": "61.1",
												"clarity": "SI1",
												"table": "56",
												"z": "3.54"
											},
											{
												"x": "5.7",
												"y": "5.73",
												"cut": "Good",
												"price": "2891",
												"color": "D",
												"carat": "0.71",
												"depth": "62.3",
												"clarity": "VS2",
												"table": "61",
												"z": "3.56"
											},
											{
												"x": "6.12",
												"y": "6.14",
												"cut": "Ideal",
												"price": "2892",
												"color": "H",
												"carat": "0.86",
												"depth": "61.8",
												"clarity": "SI2",
												"table": "55",
												"z": "3.79"
											},
											{
												"x": "6.62",
												"y": "6.55",
												"cut": "Fair",
												"price": "2892",
												"color": "H",
												"carat": "1.19",
												"depth": "65.1",
												"clarity": "I1",
												"table": "59",
												"z": "4.29"
											},
											{
												"x": "5.66",
												"y": "5.71",
												"cut": "Very Good",
												"price": "2893",
												"color": "F",
												"carat": "0.71",
												"depth": "62.6",
												"clarity": "VS1",
												"table": "55",
												"z": "3.56"
											},
											{
												"x": "5.99",
												"y": "6.04",
												"cut": "Very Good",
												"price": "2893",
												"color": "G",
												"carat": "0.82",
												"depth": "62.5",
												"clarity": "SI2",
												"table": "56",
												"z": "3.76"
											},
											{
												"x": "5.73",
												"y": "5.75",
												"cut": "Ideal",
												"price": "2893",
												"color": "G",
												"carat": "0.71",
												"depth": "61.5",
												"clarity": "VVS2",
												"table": "57",
												"z": "3.53"
											},
											{
												"x": "5.78",
												"y": "5.83",
												"cut": "Ideal",
												"price": "2893",
												"color": "F",
												"carat": "0.75",
												"depth": "62.5",
												"clarity": "VS2",
												"table": "57",
												"z": "3.63"
											},
											{
												"x": "5.87",
												"y": "5.78",
												"cut": "Very Good",
												"price": "2893",
												"color": "H",
												"carat": "0.7",
												"depth": "59.2",
												"clarity": "VVS1",
												"table": "60",
												"z": "3.45"
											},
											{
												"x": "5.89",
												"y": "5.92",
												"cut": "Ideal",
												"price": "2893",
												"color": "G",
												"carat": "0.8",
												"depth": "62.5",
												"clarity": "SI2",
												"table": "55",
												"z": "3.69"
											},
											{
												"x": "6.02",
												"y": "6.04",
												"cut": "Good",
												"price": "2893",
												"color": "G",
												"carat": "0.82",
												"depth": "59.9",
												"clarity": "SI2",
												"table": "62",
												"z": "3.61"
											},
											{
												"x": "6",
												"y": "5.93",
												"cut": "Very Good",
												"price": "2893",
												"color": "G",
												"carat": "0.82",
												"depth": "63.4",
												"clarity": "SI1",
												"table": "55",
												"z": "3.78"
											},
											{
												"x": "6.09",
												"y": "6.06",
												"cut": "Premium",
												"price": "2893",
												"color": "G",
												"carat": "0.82",
												"depth": "59.9",
												"clarity": "SI1",
												"table": "59",
												"z": "3.64"
											},
											{
												"x": "5.91",
												"y": "5.99",
												"cut": "Very Good",
												"price": "2894",
												"color": "E",
												"carat": "0.81",
												"depth": "62.4",
												"clarity": "SI2",
												"table": "57",
												"z": "3.71"
											},
											{
												"x": "5.96",
												"y": "6",
												"cut": "Ideal",
												"price": "2894",
												"color": "G",
												"carat": "0.81",
												"depth": "62.2",
												"clarity": "SI2",
												"table": "57",
												"z": "3.72"
											},
											{
												"x": "5.88",
												"y": "5.92",
												"cut": "Ideal",
												"price": "2894",
												"color": "F",
												"carat": "0.76",
												"depth": "61.4",
												"clarity": "SI1",
												"table": "56",
												"z": "3.62"
											},
											{
												"x": "5.75",
												"y": "5.78",
												"cut": "Very Good",
												"price": "2895",
												"color": "G",
												"carat": "0.71",
												"depth": "60.9",
												"clarity": "VS2",
												"table": "56",
												"z": "3.51"
											},
											{
												"x": "5.66",
												"y": "5.76",
												"cut": "Very Good",
												"price": "2895",
												"color": "F",
												"carat": "0.7",
												"depth": "61.8",
												"clarity": "VS1",
												"table": "59",
												"z": "3.53"
											},
											{
												"x": "5.71",
												"y": "5.75",
												"cut": "Ideal",
												"price": "2895",
												"color": "G",
												"carat": "0.7",
												"depth": "62.1",
												"clarity": "VVS2",
												"table": "53",
												"z": "3.56"
											},
											{
												"x": "5.85",
												"y": "5.89",
												"cut": "Very Good",
												"price": "2896",
												"color": "G",
												"carat": "0.74",
												"depth": "59.8",
												"clarity": "VS1",
												"table": "58",
												"z": "3.51"
											},
											{
												"x": "5.81",
												"y": "5.91",
												"cut": "Very Good",
												"price": "2896",
												"color": "G",
												"carat": "0.77",
												"depth": "61.3",
												"clarity": "VS2",
												"table": "60",
												"z": "3.59"
											},
											{
												"x": "6",
												"y": "6.05",
												"cut": "Very Good",
												"price": "2896",
												"color": "G",
												"carat": "0.77",
												"depth": "58.3",
												"clarity": "VS2",
												"table": "63",
												"z": "3.51"
											},
											{
												"x": "5.18",
												"y": "5.24",
												"cut": "Ideal",
												"price": "2896",
												"color": "F",
												"carat": "0.53",
												"depth": "61.6",
												"clarity": "VVS1",
												"table": "56",
												"z": "3.21"
											},
											{
												"x": "5.91",
												"y": "5.96",
												"cut": "Ideal",
												"price": "2896",
												"color": "D",
												"carat": "0.79",
												"depth": "61.5",
												"clarity": "SI1",
												"table": "56",
												"z": "3.65"
											},
											{
												"x": "5.82",
												"y": "5.86",
												"cut": "Ideal",
												"price": "2896",
												"color": "E",
												"carat": "0.73",
												"depth": "61.5",
												"clarity": "SI2",
												"table": "55",
												"z": "3.59"
											},
											{
												"x": "5.83",
												"y": "5.89",
												"cut": "Ideal",
												"price": "2896",
												"color": "D",
												"carat": "0.77",
												"depth": "62.1",
												"clarity": "SI2",
												"table": "56",
												"z": "3.64"
											},
											{
												"x": "5.94",
												"y": "5.88",
												"cut": "Premium",
												"price": "2896",
												"color": "E",
												"carat": "0.77",
												"depth": "60.9",
												"clarity": "SI1",
												"table": "58",
												"z": "3.6"
											},
											{
												"x": "6.39",
												"y": "6.35",
												"cut": "Very Good",
												"price": "2896",
												"color": "I",
												"carat": "1.01",
												"depth": "63.1",
												"clarity": "I1",
												"table": "57",
												"z": "4.02"
											},
											{
												"x": "6.46",
												"y": "6.45",
												"cut": "Ideal",
												"price": "2896",
												"color": "I",
												"carat": "1.01",
												"depth": "61.5",
												"clarity": "I1",
												"table": "57",
												"z": "3.97"
											},
											{
												"x": "5.48",
												"y": "5.51",
												"cut": "Very Good",
												"price": "2897",
												"color": "D",
												"carat": "0.6",
												"depth": "60.6",
												"clarity": "VVS2",
												"table": "57",
												"z": "3.33"
											},
											{
												"x": "5.91",
												"y": "5.85",
												"cut": "Premium",
												"price": "2897",
												"color": "E",
												"carat": "0.76",
												"depth": "61.1",
												"clarity": "SI1",
												"table": "58",
												"z": "3.59"
											},
											{
												"x": "5.3",
												"y": "5.34",
												"cut": "Ideal",
												"price": "2897",
												"color": "D",
												"carat": "0.54",
												"depth": "61.4",
												"clarity": "VVS2",
												"table": "52",
												"z": "3.26"
											},
											{
												"x": "5.69",
												"y": "5.74",
												"cut": "Ideal",
												"price": "2897",
												"color": "E",
												"carat": "0.72",
												"depth": "62.5",
												"clarity": "SI1",
												"table": "55",
												"z": "3.57"
											},
											{
												"x": "5.82",
												"y": "5.89",
												"cut": "Good",
												"price": "2897",
												"color": "F",
												"carat": "0.72",
												"depth": "59.4",
												"clarity": "VS1",
												"table": "61",
												"z": "3.48"
											},
											{
												"x": "5.81",
												"y": "5.77",
												"cut": "Premium",
												"price": "2897",
												"color": "D",
												"carat": "0.74",
												"depth": "61.8",
												"clarity": "VS2",
												"table": "58",
												"z": "3.58"
											},
											{
												"x": "6.68",
												"y": "6.61",
												"cut": "Premium",
												"price": "2898",
												"color": "J",
												"carat": "1.12",
												"depth": "60.6",
												"clarity": "SI2",
												"table": "59",
												"z": "4.03"
											},
											{
												"x": "5.83",
												"y": "5.8",
												"cut": "Ideal",
												"price": "2898",
												"color": "D",
												"carat": "0.75",
												"depth": "62.3",
												"clarity": "SI1",
												"table": "55",
												"z": "3.62"
											}
										],
										"schema": {
											"carat": "string",
											"cut": "string",
											"color": "string",
											"clarity": "string",
											"depth": "string",
											"table": "string",
											"price": "string",
											"x": "string",
											"y": "string",
											"z": "string"
										}
									},
									"isSummary": false,
									"language": "scala"
								},
								"persist_state": {
									"view": {
										"type": "details",
										"chartOptions": {
											"chartType": "bar",
											"aggregationType": "count",
											"categoryFieldKeys": [
												"carat"
											],
											"seriesFieldKeys": [
												"carat"
											],
											"isStacked": false
										}
									}
								}
							},
							"1362c259-021b-4a85-bedd-9dcdbe83328b": {
								"type": "Synapse.DataFrame",
								"sync_state": {
									"table": {
										"rows": [
											{
												"_14": "Delhi",
												"_25": "New York",
												"_35": "1248700.01",
												"_21": "New York",
												"_33": "1248700.01",
												"_3": "4",
												"_24": "New York",
												"_22": "1248700.01",
												"_13": "Delhi",
												"_11": "Delhi",
												"_8": "Delhi",
												"_7": "36",
												"_30": "1248700.01",
												"_28": "1248700.01",
												"_17": "100.0123",
												"_2": "34",
												"_12": "Delhi",
												"_20": "New York",
												"_23": "New York",
												"_9": "Delhi",
												"_19": "New York",
												"_34": "1248700.01",
												"_27": "1248700.01",
												"_31": "1248700.01",
												"_16": "Delhi",
												"_6": "10.0",
												"_1": "jack",
												"_29": "1248700.01",
												"_15": "Delhi",
												"_26": "New York",
												"_4": "3154284",
												"_10": "Delhi",
												"_18": "New York",
												"_5": "304",
												"_36": "1248700.01",
												"_32": "1248700.01"
											},
											{
												"_14": "New York",
												"_25": "New York",
												"_35": "1248700.01",
												"_21": "New York",
												"_33": "1248700.01",
												"_3": "369864",
												"_24": "New York",
												"_22": "1248700.01",
												"_13": "New York",
												"_11": "New York",
												"_8": "New York",
												"_7": "Delhi",
												"_30": "1248700.01",
												"_28": "1248700.01",
												"_17": "1248700.01",
												"_2": "30",
												"_12": "New York",
												"_20": "New York",
												"_23": "New York",
												"_9": "New York",
												"_19": "New York",
												"_34": "1248700.01",
												"_27": "1248700.01",
												"_31": "1248700.01",
												"_16": "New York",
												"_6": "20.01",
												"_1": "Riti",
												"_29": "1248700.01",
												"_15": "New York",
												"_26": "New York",
												"_4": "3244",
												"_10": "New York",
												"_18": "New York",
												"_5": "3424525",
												"_36": "1248700.01",
												"_32": "1248700.01"
											},
											{
												"_14": "Riti",
												"_25": "New York",
												"_35": "1248700.01",
												"_21": "New York",
												"_33": "1248700.01",
												"_3": "34745",
												"_24": "New York",
												"_22": "1248700.01",
												"_13": "Riti",
												"_11": "Riti",
												"_8": "New York",
												"_7": "New York",
												"_30": "1248700.01",
												"_28": "1248700.01",
												"_17": "100000.021",
												"_2": "16",
												"_12": "Riti",
												"_20": "New York",
												"_23": "New York",
												"_9": "Riti",
												"_19": "New York",
												"_34": "1248700.01",
												"_27": "1248700.01",
												"_31": "1248700.01",
												"_16": "Riti",
												"_6": "30.02",
												"_1": "Aadi",
												"_29": "1248700.01",
												"_15": "Riti",
												"_26": "New York",
												"_4": "3414",
												"_10": "Riti",
												"_18": "New York",
												"_5": "3000004",
												"_36": "1248700.01",
												"_32": "1248700.01"
											}
										],
										"schema": {
											"_1": "string",
											"_2": "bigint",
											"_3": "bigint",
											"_4": "bigint",
											"_5": "bigint",
											"_6": "string",
											"_7": "string",
											"_8": "string",
											"_9": "string",
											"_10": "string",
											"_11": "string",
											"_12": "string",
											"_13": "string",
											"_14": "string",
											"_15": "string",
											"_16": "string",
											"_17": "string",
											"_18": "string",
											"_19": "string",
											"_20": "string",
											"_21": "string",
											"_22": "string",
											"_23": "string",
											"_24": "string",
											"_25": "string",
											"_26": "string",
											"_27": "string",
											"_28": "string",
											"_29": "string",
											"_30": "string",
											"_31": "string",
											"_32": "string",
											"_33": "string",
											"_34": "string",
											"_35": "string",
											"_36": "string"
										}
									},
									"isSummary": false,
									"language": "scala"
								},
								"persist_state": {
									"view": {
										"type": "details",
										"chartOptions": {
											"chartType": "bar",
											"aggregationType": "sum",
											"categoryFieldKeys": [
												"_1"
											],
											"seriesFieldKeys": [
												"_2"
											],
											"isStacked": false
										}
									}
								}
							}
						}
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#display(DataFrame) sample:\r\n",
							"\r\n",
							"# import pyspark class Row from module sql\r\n",
							"from pyspark.sql import *\r\n",
							"# Create Example Data - Departments and Employees\r\n",
							"# Create the Departments\r\n",
							"department1 = Row(id='123456', name='Computer Science')\r\n",
							"department2 = Row(id='789012', name='Mechanical Engineering')\r\n",
							"department3 = Row(id='345678', name='Theater and Drama')\r\n",
							"department4 = Row(id='901234', name='Indoor Recreation')\r\n",
							"# Create the Employees\r\n",
							"Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\r\n",
							"employee1 = Employee('michael', 'armbrust', 'noreply@berkeley.edore', 130000)\r\n",
							"employee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\r\n",
							"employee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\r\n",
							"employee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\r\n",
							"# Create the DepartmentWithEmployees instances from Departments and Employees\r\n",
							"departmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\r\n",
							"departmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\r\n",
							"departmentWithEmployees3 = Row(department=department3, employees=[employee1, employee4])\r\n",
							"departmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\r\n",
							"\r\n",
							"departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\r\n",
							"departmentsWithEmployeesSeq2 = [departmentWithEmployees2, departmentWithEmployees3]\r\n",
							"df1 = spark.createDataFrame(departmentsWithEmployeesSeq1)\r\n",
							"\r\n",
							"display(df1)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# display(List) sample:\r\n",
							"students = [ ['jack', 34, '36'] ,\r\n",
							"['Riti', 30, 'Delhi' ] ,\r\n",
							"['Aadi', 16, 'New York'] ]\r\n",
							"\r\n",
							"display(students)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# display(pandas.DataFrame) sample:\r\n",
							"import numpy as np\r\n",
							"import pandas as pd\r\n",
							"from pyspark.sql import *\r\n",
							"d = {'col11': [1, 2, 3, 4, 5, 6, 7, 8], 'col12': [3, 4, 5, 6, 8, 3, 16, 20]}\r\n",
							"pdf = pd.DataFrame(data=d)\r\n",
							"\r\n",
							"display(pdf)\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "scala"
							},
							"collapsed": false
						},
						"source": [
							"%%spark\r\n",
							"display(spark.range(10))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "scala"
							},
							"collapsed": false
						},
						"source": [
							"%%spark\r\n",
							"case class MapEntry(key: String, value: Int)\r\n",
							"val largeSeries = for (x <- 1 to 5000) yield MapEntry(\"k_%04d\".format(x), x)\r\n",
							"val largeDataFrame = sc.parallelize(largeSeries).toDF()\r\n",
							"largeDataFrame.registerTempTable(\"largeTable\")\r\n",
							"display(spark.sqlContext.sql(\"select * from largeTable\"))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "csharp"
							}
						},
						"source": [
							"%%csharp\r\n",
							"\r\n",
							"using Microsoft.Spark.Sql;\r\n",
							"using Microsoft.Spark.Sql.Types;\r\n",
							"using static Microsoft.Spark.Sql.Functions;\r\n",
							"var inputSchema = new StructType(new[]\r\n",
							"{\r\n",
							"    new StructField(\"age\", new IntegerType()),\r\n",
							"    new StructField(\"name\", new StringType())\r\n",
							"});\r\n",
							"DataFrame dfWithSchema = spark.Read().Schema(inputSchema).Json(\"abfss://hozhao@hozhao0924gen2.dfs.core.windows.net/people.json\");"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "csharp"
							},
							"collapsed": false
						},
						"source": [
							"%%csharp\r\n",
							"Display(dfWithSchema);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "csharp"
							},
							"collapsed": true
						},
						"source": [
							"%%csharp\r\n",
							"DataFrame df = spark.Read().Option(\"header\", true).Csv(\"abfss://hozhao@hozhao0924gen2.dfs.core.windows.net/diamonds.csv\");\r\n",
							"df.Show();"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "csharp"
							},
							"collapsed": false
						},
						"source": [
							"%%csharp\r\n",
							"Display(df);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": true
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://hozhao@hozhao0924gen2.dfs.core.windows.net/diamonds.csv', format='csv'\r\n",
							"## Ifheaderexistsuncommentlinebelow\r\n",
							", header=True\r\n",
							")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": true
						},
						"source": [
							"%%pyspark\r\n",
							"df.createOrReplaceTempView(\"diamonds\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT * from diamonds"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"displayHTML(\"\"\"\r\n",
							"<!DOCTYPE html>\r\n",
							" <meta charset=\"utf-8\">\r\n",
							" <style>\r\n",
							" path {\r\n",
							" fill: white;\r\n",
							" stroke: #000;\r\n",
							" }\r\n",
							" circle {\r\n",
							" fill: #fff;\r\n",
							" stroke: #000;\r\n",
							" pointer-events: none;\r\n",
							" }\r\n",
							" .PiYG .q0-9{fill:rgb${colors(0)}}\r\n",
							" .PiYG .q1-9{fill:rgb${colors(1)}}\r\n",
							" .PiYG .q2-9{fill:rgb${colors(2)}}\r\n",
							" .PiYG .q3-9{fill:rgb${colors(3)}}\r\n",
							" .PiYG .q4-9{fill:rgb${colors(4)}}\r\n",
							" .PiYG .q5-9{fill:rgb${colors(5)}}\r\n",
							" .PiYG .q6-9{fill:rgb${colors(6)}}\r\n",
							" .PiYG .q7-9{fill:rgb${colors(7)}}\r\n",
							" .PiYG .q8-9{fill:rgb${colors(8)}}\r\n",
							" </style>\r\n",
							" <body>\r\n",
							" <script src=\"https://d3js.org/d3.v3.min.js\"></script>\r\n",
							" <script>\r\n",
							" var width = 960,\r\n",
							" height = 500;\r\n",
							" var vertices = d3.range(100).map(function(d) {\r\n",
							" return [Math.random() * width, Math.random() * height];\r\n",
							" });\r\n",
							" var svg = d3.select(\"body\").append(\"svg\")\r\n",
							" .attr(\"width\", width)\r\n",
							" .attr(\"height\", height)\r\n",
							" .attr(\"class\", \"PiYG\")\r\n",
							" .on(\"mousemove\", function() { vertices[0] = d3.mouse(this); redraw(); });\r\n",
							" var path = svg.append(\"g\").selectAll(\"path\");\r\n",
							" svg.selectAll(\"circle\")\r\n",
							" .data(vertices.slice(1))\r\n",
							" .enter().append(\"circle\")\r\n",
							" .attr(\"transform\", function(d) { return \"translate(\" + d + \")\"; })\r\n",
							" .attr(\"r\", 2);\r\n",
							" redraw();\r\n",
							" function redraw() {\r\n",
							" path = path.data(d3.geom.delaunay(vertices).map(function(d) { return \"M\" + d.join(\"L\") + \"Z\"; }), String);\r\n",
							" path.exit().remove();\r\n",
							" path.enter().append(\"path\").attr(\"class\", function(d, i) { return \"q\" + (i % 9) + \"-9\"; }).attr(\"d\", String);\r\n",
							" }\r\n",
							" </script>\r\n",
							"\"\"\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from bokeh.plotting import figure\r\n",
							"from bokeh.embed import components, file_html\r\n",
							"from bokeh.resources import CDN\r\n",
							"# prepare some data\r\n",
							"x = [1, 2, 3, 4, 5]\r\n",
							"y = [6, 7, 2, 4, 5]\r\n",
							"# create a new plot with a title and axis labels\r\n",
							"p = figure(title=\"simple line example\", x_axis_label='x', y_axis_label='y')\r\n",
							"# add a line renderer with legend and line thickness\r\n",
							"p.line(x, y, legend=\"Temp.\", line_width=2)\r\n",
							"# create an html document that embeds the Bokeh plot\r\n",
							"html = file_html(p, CDN, \"my plot1\")\r\n",
							"# display this html\r\n",
							"displayHTML(html)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#display(List) sample: \r\n",
							"students = [ ['jack', 34,4,3154284,304,'10.0', '36','Delhi','Delhi','Delhi','Delhi','Delhi','Delhi','Delhi','Delhi','Delhi','100.0123','New York','New York','New York','New York','1248700.01','New York','New York','New York','New York','1248700.01', '1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01' ] , \r\n",
							"['Riti', 30,369864,3244,3424525, '20.01', 'Delhi','New York','New York','New York','New York','New York','New York','New York','New York','New York','1248700.01','New York','New York','New York','New York','1248700.01','New York','New York','New York','New York','1248700.01', '1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01'] , \r\n",
							"['Aadi', 16,34745,3414,3000004, '30.02', 'New York','New York','Riti','Riti','Riti','Riti','Riti','Riti','Riti','Riti','100000.021','New York','New York','New York','New York','1248700.01','New York','New York','New York','New York','1248700.01', '1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01'] ] \r\n",
							"\r\n",
							"display(students)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"displayHTML(\"\"\" \r\n",
							"<!DOCTYPE html>\r\n",
							"<head lang=\"en\">\r\n",
							"  <meta charset=\"utf-8\">\r\n",
							"  <title></title>\r\n",
							"  <style type=\"text/css\">\r\n",
							"         div {\r\n",
							"              width: 100px;\r\n",
							"              height: 2200px;\r\n",
							"              background-color: white;\r\n",
							"              margin-top: 20px;\r\n",
							"              padding-top: 20px;\r\n",
							"              border: 20px solid red;\r\n",
							"              border: 5px dashed red;/**/\r\n",
							"             }\r\n",
							"  </style>\r\n",
							" \r\n",
							"</head>\r\n",
							"<body>\r\n",
							" <div>Test</div>\r\n",
							" <div></div>\r\n",
							"</body>\r\n",
							"</html>\r\n",
							" \r\n",
							"\"\"\")\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/test')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "samll",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/bigdataqa/providers/Microsoft.Synapse/workspaces/bigdataqa0512ws2/bigDataPools/samll",
						"name": "samll",
						"type": "Spark",
						"endpoint": "https://bigdataqa0512ws2.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/samll",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"test"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/test13214y1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5a417f9e-1c28-49fc-a5ec-0aeac6f58755"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#display(DataFrame) sample:\n",
							"\n",
							"# import pyspark class Row from module sql\n",
							"from pyspark.sql import *\n",
							"# Create Example Data - Departments and Employees\n",
							"# Create the Departments\n",
							"department1 = Row(id='123456', name='Computer Science')\n",
							"department2 = Row(id='789012', name='Mechanical Engineering')\n",
							"department3 = Row(id='345678', name='Theater and Drama')\n",
							"department4 = Row(id='901234', name='Indoor Recreation')\n",
							"# Create the Employees\n",
							"Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
							"employee1 = Employee('michael', 'armbrust', 'noreply@berkeley.edore', 130000)\n",
							"employee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\n",
							"employee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\n",
							"employee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\n",
							"# Create the DepartmentWithEmployees instances from Departments and Employees\n",
							"departmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\n",
							"departmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\n",
							"departmentWithEmployees3 = Row(department=department3, employees=[employee1, employee4])\n",
							"departmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\n",
							"\n",
							"departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\n",
							"departmentsWithEmployeesSeq2 = [departmentWithEmployees2, departmentWithEmployees3]\n",
							"df1 = spark.createDataFrame(departmentsWithEmployeesSeq1)\n",
							"\n",
							"display(df1)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"tetst"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# display(List) sample:\n",
							"students = [ ['jack', 34, '36'] ,\n",
							"['Riti', 30, 'Delhi' ] ,\n",
							"['Aadi', 16, 'New York'] ]\n",
							"\n",
							"display(students)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# display(pandas.DataFrame) sample:\n",
							"import numpy as np\n",
							"import pandas as pd\n",
							"from pyspark.sql import *\n",
							"d = {'col11': [1, 2, 3, 4, 5, 6, 7, 8], 'col12': [3, 4, 5, 6, 8, 3, 16, 20]}\n",
							"pdf = pd.DataFrame(data=d)\n",
							"\n",
							"display(pdf)\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\n",
							"display(spark.range(10))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\n",
							"case class MapEntry(key: String, value: Int)\n",
							"val largeSeries = for (x <- 1 to 5000) yield MapEntry(\"k_%04d\".format(x), x)\n",
							"val largeDataFrame = sc.parallelize(largeSeries).toDF()\n",
							"largeDataFrame.registerTempTable(\"largeTable\")\n",
							"display(spark.sqlContext.sql(\"select * from largeTable\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"displayHTML(\"\"\"\n",
							"<!DOCTYPE html>\n",
							" <meta charset=\"utf-8\">\n",
							" <style>\n",
							" path {\n",
							" fill: white;\n",
							" stroke: #000;\n",
							" }\n",
							" circle {\n",
							" fill: #fff;\n",
							" stroke: #000;\n",
							" pointer-events: none;\n",
							" }\n",
							" .PiYG .q0-9{fill:rgb${colors(0)}}\n",
							" .PiYG .q1-9{fill:rgb${colors(1)}}\n",
							" .PiYG .q2-9{fill:rgb${colors(2)}}\n",
							" .PiYG .q3-9{fill:rgb${colors(3)}}\n",
							" .PiYG .q4-9{fill:rgb${colors(4)}}\n",
							" .PiYG .q5-9{fill:rgb${colors(5)}}\n",
							" .PiYG .q6-9{fill:rgb${colors(6)}}\n",
							" .PiYG .q7-9{fill:rgb${colors(7)}}\n",
							" .PiYG .q8-9{fill:rgb${colors(8)}}\n",
							" </style>\n",
							" <body>\n",
							" <script src=\"https://d3js.org/d3.v3.min.js\"></script>\n",
							" <script>\n",
							" var width = 960,\n",
							" height = 500;\n",
							" var vertices = d3.range(100).map(function(d) {\n",
							" return [Math.random() * width, Math.random() * height];\n",
							" });\n",
							" var svg = d3.select(\"body\").append(\"svg\")\n",
							" .attr(\"width\", width)\n",
							" .attr(\"height\", height)\n",
							" .attr(\"class\", \"PiYG\")\n",
							" .on(\"mousemove\", function() { vertices[0] = d3.mouse(this); redraw(); });\n",
							" var path = svg.append(\"g\").selectAll(\"path\");\n",
							" svg.selectAll(\"circle\")\n",
							" .data(vertices.slice(1))\n",
							" .enter().append(\"circle\")\n",
							" .attr(\"transform\", function(d) { return \"translate(\" + d + \")\"; })\n",
							" .attr(\"r\", 2);\n",
							" redraw();\n",
							" function redraw() {\n",
							" path = path.data(d3.geom.delaunay(vertices).map(function(d) { return \"M\" + d.join(\"L\") + \"Z\"; }), String);\n",
							" path.exit().remove();\n",
							" path.enter().append(\"path\").attr(\"class\", function(d, i) { return \"q\" + (i % 9) + \"-9\"; }).attr(\"d\", String);\n",
							" }\n",
							" </script>\n",
							"\"\"\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"from bokeh.plotting import figure\n",
							"from bokeh.embed import components, file_html\n",
							"from bokeh.resources import CDN\n",
							"# prepare some data\n",
							"x = [1, 2, 3, 4, 5]\n",
							"y = [6, 7, 2, 4, 5]\n",
							"# create a new plot with a title and axis labels\n",
							"p = figure(title=\"simple line example\", x_axis_label='x', y_axis_label='y')\n",
							"# add a line renderer with legend and line thickness\n",
							"p.line(x, y, legend=\"Temp.\", line_width=2)\n",
							"# create an html document that embeds the Bokeh plot\n",
							"html = file_html(p, CDN, \"my plot1\")\n",
							"# display this html\n",
							"displayHTML(html)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"#display(List) sample: \n",
							"students = [ ['jack', 34,4,3154284,304,'10.0', '36','Delhi','Delhi','Delhi','Delhi','Delhi','Delhi','Delhi','Delhi','Delhi','100.0123','New York','New York','New York','New York','1248700.01','New York','New York','New York','New York','1248700.01', '1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01' ] , \n",
							"['Riti', 30,369864,3244,3424525, '20.01', 'Delhi','New York','New York','New York','New York','New York','New York','New York','New York','New York','1248700.01','New York','New York','New York','New York','1248700.01','New York','New York','New York','New York','1248700.01', '1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01'] , \n",
							"['Aadi', 16,34745,3414,3000004, '30.02', 'New York','New York','Riti','Riti','Riti','Riti','Riti','Riti','Riti','Riti','100000.021','New York','New York','New York','New York','1248700.01','New York','New York','New York','New York','1248700.01', '1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01','1248700.01'] ] \n",
							"\n",
							"display(students)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"displayHTML(\"\"\" \n",
							"<!DOCTYPE html>\n",
							"<head lang=\"en\">\n",
							"  <meta charset=\"utf-8\">\n",
							"  <title></title>\n",
							"  <style type=\"text/css\">\n",
							"         div {\n",
							"              width: 100px;\n",
							"              height: 2200px;\n",
							"              background-color: white;\n",
							"              margin-top: 20px;\n",
							"              padding-top: 20px;\n",
							"              border: 20px solid red;\n",
							"              border: 5px dashed red;/**/\n",
							"             }\n",
							"  </style>\n",
							" \n",
							"</head>\n",
							"<body>\n",
							" <div>Test</div>\n",
							" <div></div>\n",
							"</body>\n",
							"</html>\n",
							" \n",
							"\"\"\")\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tutorial-predict-nyc-taxi-tips-onnx')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder/python"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f50247ee-d023-49fd-a0c6-f846e199f9d7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Predict NYC Taxi Tips \r\n",
							"The notebook ingests, prepares and then trains a model based on an Open Dataset that tracks NYC Yellow Taxi trips and various attributes around them. The goal is to for a given trip, predict whether there will be a tip or not. The model then will be converted to ONNX format and tracked by MLFlow.\r\n",
							"We will later use the ONNX model for inferencing in Azure Synapse SQL Pool using the new model scoring wizard.\r\n",
							"## Note:\r\n",
							"**Please note that for successful conversion to ONNX, this notebook requires using  Scikit-learn version 0.20.3.**\r\n",
							"Run the first cell to list the packages installed and check your sklearn version. Uncomment the pip install command to install the correct version\r\n",
							"\r\n",
							"%pip install scikit-learn==0.20.3\r\n",
							"\r\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Load data\r\n",
							"Get a sample data of nyc yellow taxi from Azure Open Datasets"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"#%pip list\n",
							"#%pip install scikit-learn==0.20.3"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"gather": {
								"logged": 1599713958224
							}
						},
						"source": [
							"from azureml.opendatasets import NycTlcYellow\r\n",
							"from datetime import datetime\r\n",
							"from dateutil import parser\r\n",
							"\r\n",
							"start_date = parser.parse('2018-05-01')\r\n",
							"end_date = parser.parse('2018-05-07')\r\n",
							"nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\r\n",
							"nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\r\n",
							"nyc_tlc_df.info()"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"gather": {
								"logged": 1599713959314
							},
							"collapsed": true
						},
						"source": [
							"from IPython.display import display\r\n",
							"\r\n",
							"sampled_df = nyc_tlc_df.sample(n=10000, random_state=123)\r\n",
							"display(sampled_df.head(5))"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Prepare and featurize data\r\n",
							"- There are extra dimensions that are not going to be useful in the model. We just take the dimensions that we need and put them into the featurised dataframe. \r\n",
							"- There are also a bunch of outliers in the data so we need to filter them out."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"gather": {
								"logged": 1599713966451
							},
							"collapsed": true
						},
						"source": [
							"import numpy\r\n",
							"import pandas\r\n",
							"\r\n",
							"def get_pickup_time(df):\r\n",
							"    pickupHour = df['pickupHour'];\r\n",
							"    if ((pickupHour >= 7) & (pickupHour <= 10)):\r\n",
							"        return 'AMRush'\r\n",
							"    elif ((pickupHour >= 11) & (pickupHour <= 15)):\r\n",
							"        return 'Afternoon'\r\n",
							"    elif ((pickupHour >= 16) & (pickupHour <= 19)):\r\n",
							"        return 'PMRush'\r\n",
							"    else:\r\n",
							"        return 'Night'\r\n",
							"\r\n",
							"featurized_df = pandas.DataFrame()\r\n",
							"featurized_df['tipped'] = (sampled_df['tipAmount'] > 0).astype('int')\r\n",
							"featurized_df['fareAmount'] = sampled_df['fareAmount'].astype('float32')\r\n",
							"featurized_df['paymentType'] = sampled_df['paymentType'].astype('int')\r\n",
							"featurized_df['passengerCount'] = sampled_df['passengerCount'].astype('int')\r\n",
							"featurized_df['tripDistance'] = sampled_df['tripDistance'].astype('float32')\r\n",
							"featurized_df['pickupHour'] = sampled_df['tpepPickupDateTime'].dt.hour.astype('int')\r\n",
							"featurized_df['tripTimeSecs'] = ((sampled_df['tpepDropoffDateTime'] - sampled_df['tpepPickupDateTime']) / numpy.timedelta64(1, 's')).astype('int')\r\n",
							"\r\n",
							"featurized_df['pickupTimeBin'] = featurized_df.apply(get_pickup_time, axis=1)\r\n",
							"featurized_df = featurized_df.drop(columns='pickupHour')\r\n",
							"\r\n",
							"display(featurized_df.head(5))\r\n",
							""
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"gather": {
								"logged": 1599713971477
							},
							"collapsed": true
						},
						"source": [
							"filtered_df = featurized_df[(featurized_df.tipped >= 0) & (featurized_df.tipped <= 1)\\\r\n",
							"    & (featurized_df.fareAmount >= 1) & (featurized_df.fareAmount <= 250)\\\r\n",
							"    & (featurized_df.paymentType >= 1) & (featurized_df.paymentType <= 2)\\\r\n",
							"    & (featurized_df.passengerCount > 0) & (featurized_df.passengerCount < 8)\\\r\n",
							"    & (featurized_df.tripDistance >= 0) & (featurized_df.tripDistance <= 100)\\\r\n",
							"    & (featurized_df.tripTimeSecs >= 30) & (featurized_df.tripTimeSecs <= 7200)]\r\n",
							"\r\n",
							"filtered_df.info()"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Split training and testing data sets\r\n",
							"- 70% of the data is used to train the model.\r\n",
							"- 30% of the data is used to test the model."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"gather": {
								"logged": 1599713980823
							},
							"collapsed": true
						},
						"source": [
							"from sklearn.model_selection import train_test_split\r\n",
							"\r\n",
							"train_df, test_df = train_test_split(filtered_df, test_size=0.3, random_state=123)\r\n",
							"\r\n",
							"x_train = pandas.DataFrame(train_df.drop(['tipped'], axis = 1))\r\n",
							"y_train = pandas.DataFrame(train_df.iloc[:,train_df.columns.tolist().index('tipped')])\r\n",
							"\r\n",
							"x_test = pandas.DataFrame(test_df.drop(['tipped'], axis = 1))\r\n",
							"y_test = pandas.DataFrame(test_df.iloc[:,test_df.columns.tolist().index('tipped')])"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Export test data as CSV\r\n",
							"Export the test data as a CSV file. Later, we will load the CSV file into Synapse SQL pool to test the model."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"gather": {
								"logged": 1598830320180
							},
							"collapsed": true
						},
						"source": [
							"test_df.to_csv('test_data.csv', index=False)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Train model\r\n",
							"Train a bi-classifier to predict whether a taxi trip will be a tipped or not."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"gather": {
								"logged": 1599713996871
							},
							"collapsed": true
						},
						"source": [
							"from sklearn.compose import ColumnTransformer\r\n",
							"from sklearn.linear_model import LogisticRegression\r\n",
							"from sklearn.pipeline import Pipeline\r\n",
							"from sklearn.impute import SimpleImputer\r\n",
							"from sklearn.preprocessing import StandardScaler, OneHotEncoder\r\n",
							"\r\n",
							"float_features = ['fareAmount', 'tripDistance']\r\n",
							"float_transformer = Pipeline(steps=[\r\n",
							"    ('imputer', SimpleImputer(strategy='median')),\r\n",
							"    ('scaler', StandardScaler())])\r\n",
							"\r\n",
							"integer_features = ['paymentType', 'passengerCount', 'tripTimeSecs']\r\n",
							"integer_transformer = Pipeline(steps=[\r\n",
							"    ('imputer', SimpleImputer(strategy='median')),\r\n",
							"    ('scaler', StandardScaler())])\r\n",
							"\r\n",
							"categorical_features = ['pickupTimeBin']\r\n",
							"categorical_transformer = Pipeline(steps=[\r\n",
							"    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\r\n",
							"\r\n",
							"preprocessor = ColumnTransformer(\r\n",
							"    transformers=[\r\n",
							"        ('float', float_transformer, float_features),\r\n",
							"        ('integer', integer_transformer, integer_features),\r\n",
							"        ('cat', categorical_transformer, categorical_features)\r\n",
							"    ])\r\n",
							"\r\n",
							"clf = Pipeline(steps=[('preprocessor', preprocessor),\r\n",
							"                      ('classifier', LogisticRegression(solver='lbfgs'))])\r\n",
							"\r\n",
							"# Train the model\r\n",
							"clf.fit(x_train, y_train)"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"gather": {
								"logged": 1599714001990
							},
							"collapsed": true
						},
						"source": [
							"# Evalute the model\r\n",
							"score = clf.score(x_test, y_test)\r\n",
							"print(score)"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Convert the model to ONNX format\r\n",
							"Currently, T-SQL scoring only supports ONNX model format (https://onnx.ai/)."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"gather": {
								"logged": 1598830324781
							},
							"collapsed": true
						},
						"source": [
							"from skl2onnx import convert_sklearn\r\n",
							"from skl2onnx.common.data_types import FloatTensorType, Int64TensorType, DoubleTensorType, StringTensorType\r\n",
							"\r\n",
							"def convert_dataframe_schema(df, drop=None):\r\n",
							"    inputs = []\r\n",
							"    for k, v in zip(df.columns, df.dtypes):\r\n",
							"        if drop is not None and k in drop:\r\n",
							"            continue\r\n",
							"        if v == 'int64':\r\n",
							"            t = Int64TensorType([1, 1])\r\n",
							"        elif v == 'float32':\r\n",
							"            t = FloatTensorType([1, 1])\r\n",
							"        elif v == 'float64':\r\n",
							"            t = DoubleTensorType([1, 1])\r\n",
							"        else:\r\n",
							"            t = StringTensorType([1, 1])\r\n",
							"        inputs.append((k, t))\r\n",
							"    return inputs\r\n",
							"\r\n",
							"model_inputs = convert_dataframe_schema(x_train)\r\n",
							"onnx_model = convert_sklearn(clf, \"nyc_taxi_tip_predict\", model_inputs)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Register the model with MLFlow"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"gather": {
								"logged": 1598830390704
							},
							"collapsed": true
						},
						"source": [
							"from azureml.core import Workspace\r\n",
							"\r\n",
							"ws = Workspace.from_config()\r\n",
							"print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"gather": {
								"logged": 1598830404736
							},
							"collapsed": true
						},
						"source": [
							"import mlflow\r\n",
							"import mlflow.onnx\r\n",
							"\r\n",
							"from mlflow.models.signature import infer_signature\r\n",
							"\r\n",
							"experiment_name = 'nyc_taxi_tip_predict_exp'\r\n",
							"artifact_path = 'nyc_taxi_tip_predict_artifact'\r\n",
							"\r\n",
							"mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\r\n",
							"mlflow.set_experiment(experiment_name)\r\n",
							"\r\n",
							"with mlflow.start_run() as run:\r\n",
							"    # Infer signature\r\n",
							"    input_sample = x_train.head(1)\r\n",
							"    output_sample = pandas.DataFrame(columns=['output_label'], data=[1])\r\n",
							"    signature = infer_signature(input_sample, output_sample)\r\n",
							"\r\n",
							"    # Save the model to the outputs directory for capture\r\n",
							"    mlflow.onnx.log_model(onnx_model, artifact_path, signature=signature, input_example=input_sample)\r\n",
							"\r\n",
							"    # Register the model to AML model registry\r\n",
							"    mlflow.register_model('runs:/' + run.info.run_id + '/' + artifact_path, 'nyc_taxi_tip_predict')\r\n",
							""
						],
						"outputs": [],
						"execution_count": 11
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tutorial_001')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "scala"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"// Cell 1: create the path towards your data lake storage folder, we assume that you have used the same container and path names in the sink source\n",
							"import org.apache.spark.sql.{Dataset, SparkSession}\n",
							"val adls_path = \"abfss://wwwimporters@ruxuadlsgen2.dfs.core.windows.net/dimension_Customer2/\""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"// Cell 2: set the Spark context\n",
							"spark.conf.set(\"fs.azure.account.auth.type.ruxuadlsgen2.dfs.core.windows.net\",\"SharedKey\")\n",
							"spark.conf.set(\"fs.azure.account.key.ruxuadlsgen2.dfs.core.windows.net\",\"cqBumdrCqAjKDzfCjWwNljwM/I648JHGuWCy+FbJ4MPCrwO/Agy/IEMq9al4byEWxS8aPsSUeaXUFvvT4WR6IA==\")\n",
							"\n",
							"print(\"Remote lake path: \" + adls_path)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"// Cell 3: read the customer file name in the  parquet format\n",
							"val customer = spark.read.parquet(adls_path)\n",
							"customer.show()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"// Cell 4: set the path and settings to connect to the SQL Database (this database is in East US and some small Egress fee might apply)\n",
							"Class.forName(\"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n",
							"val jdbcHostname = \"arcadiademo.database.windows.net\"\n",
							"val jdbcPort = 1433\n",
							"val jdbcDatabase = \"Arcadia_Private_Preview\"\n",
							"val jdbcUsername = \"sparkarcadia\"\n",
							"val jdbcPassword = \"340$Uuxwp7Mcxo7Khy\"\n",
							"\n",
							"// Create the JDBC URL without passing in the user and password parameters.\n",
							"val jdbcUrl = s\"jdbc:sqlserver://${jdbcHostname}:${jdbcPort};database=${jdbcDatabase}\"\n",
							"\n",
							"// Create a Properties() object to hold the parameters.\n",
							"import java.util.Properties\n",
							"val connectionProperties = new Properties()\n",
							"\n",
							"connectionProperties.put(\"user\", s\"${jdbcUsername}\")\n",
							"connectionProperties.put(\"password\", s\"${jdbcPassword}\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"// Cell 5: Read  specific columns in Batch the folder from the SQL table and apply some transformation\n",
							"val facttx = spark.read.jdbc(jdbcUrl, \"SalesLT.SalesOrderHeader\", connectionProperties).\n",
							"    select(\"CustomerID\",\"TotalDue\").\n",
							"    groupBy(\"CustomerID\").\n",
							"    sum(\"TotalDue\").\n",
							"    toDF(\"CustomerID\", \"TotalDue\").\n",
							"    orderBy(desc(\"TotalDue\"))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"facttx.printSchema()"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"source": [
							"customer.printSchema()"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"source": [
							"// Cell 6: Join two dataframes into one new dataframe (Result) to get the revenue per customer name\n",
							"val result = facttx.join(customer,Seq(\"CustomerID\"),joinType=\"left\").groupBy(\"CompanyName\").sum().select(\"CompanyName\",\"sum(TotalDue)\").toDF(\"Customer_Name\",\"Total_Revenue\")\n",
							"result.show()"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"// Cell 6: Join two dataframes into one new dataframe (Result) to get the revenue per customer name\n",
							"val result = facttx.join(customer,Seq(\"CustomerID\"),joinType=\"left\").select(\"CompanyName\", \"TotalDue\").groupBy(\"CompanyName\").sum().toDF(\"Customer_Name\",\"Total_Revenue\")\n",
							"result.show()"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"source": [
							"// Cell 7: set the path and settings to connect to your SQL Pool in your workspace\n",
							"Class.forName(\"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n",
							"val jdbcHostnamedest = \"ruxuworkspace-do-not-delete.database.windows.net\"\n",
							"val jdbcPortdest = 1433\n",
							"val jdbcDatabasedest = \"ruxusqlcompute\"\n",
							"val jdbcUsernamedest = \"sparkarcadia\"\n",
							"val jdbcPassworddest = \"340$Uuxwp7Mcxo7Khy\"\n",
							"\n",
							"// Create the JDBC URL without passing in the user and password parameters.\n",
							"val jdbcUrldest = s\"jdbc:sqlserver://${jdbcHostnamedest}:${jdbcPortdest};database=${jdbcDatabasedest}\"\n",
							"\n",
							"// Create a Properties() object to hold the parameters.\n",
							"val connectionPropertiesdest = new Properties()\n",
							"\n",
							"connectionPropertiesdest.put(\"user\", s\"${jdbcUsernamedest}\")\n",
							"connectionPropertiesdest.put(\"password\", s\"${jdbcPassworddest}\")"
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"source": [
							"// Cell 8: Write the dataframe result into a new table (dbo.sales_by_customer) in the SQL Pool  \n",
							"result.write.option(\"createTableColumnTypes\", \"Customer_Name varchar(80), Total_Revenue decimal(18,2)\").jdbc(jdbcUrldest,\"dbo.sales_by_customer\", connectionPropertiesdest)"
						],
						"outputs": [],
						"execution_count": 54
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/version check')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "weko"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "da6152f9-302d-4dc0-9639-303602bcf51e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/ee9a0083-b9db-4f2d-9cb9-05a291e9c158/resourceGroups/bdbjqa/providers/Microsoft.Synapse/workspaces/bdbj0302ws-git/bigDataPools/small",
						"name": "small",
						"type": "Spark",
						"endpoint": "https://bdbj0302ws-git.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# wekoExp - 25746107\n",
							"import os\n",
							"nowtime = os.popen('cat /usr/hdp/current/spark2-client/RELEASE')\n",
							"print(nowtime.read())"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"# wekoControl - 25747918 \n",
							"import os\n",
							"nowtime = os.popen('cat /usr/hdp/current/spark2-client/RELEASE')\n",
							"print(nowtime.read())"
						],
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/05121552')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.1",
				"language": "csharp",
				"jobProperties": {
					"name": "05121552",
					"file": "local:///usr/hdp/current/spark2-client/jars/microsoft-spark.jar",
					"className": "org.apache.spark.deploy.dotnet.DotnetRunner",
					"conf": {
						"spark.yarn.appMasterEnv.DOTNET_ASSEMBLY_SEARCH_PATHS": "./udfs",
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "856819b8-951a-4f64-aee2-4eb735ceadc6"
					},
					"args": [
						"abfss://sparkjob@hozhaogen2.dfs.core.windows.net/synapse/workspaces/bigdataqa1105ws/batchjobs/Spark_DotNet/wordcount.zip",
						"WordCount",
						"abfss://sparkjob@hozhaogen2.dfs.core.windows.net/dotnet/wordcount/shakespeare.txt",
						"abfss://sparkjob@hozhaogen2.dfs.core.windows.net/dotnet/wordcount/result"
					],
					"jars": [],
					"files": [],
					"archives": [
						"abfss://sparkjob@hozhaogen2.dfs.core.windows.net/synapse/workspaces/bigdataqa1105ws/batchjobs/Spark_DotNet/wordcount.zip#udfs"
					],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/05121604_python')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.1",
				"language": "python",
				"jobProperties": {
					"name": "05121604_python",
					"file": "abfss://sparkjob@hozhaogen2.dfs.core.windows.net/synapse/workspaces/bigdataqa0512ws2/batchjobs/05121604_python/wordcount.py",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a8a0c36e-cb36-4515-b113-ca092b3bf41c"
					},
					"args": [
						"abfss://sparkjob@hozhaogen2.dfs.core.windows.net/python/shakespeare.txt",
						"abfss://sparkjob@hozhaogen2.dfs.core.windows.net/python/result"
					],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/2')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "poi",
				"targetBigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.1",
				"language": "scala",
				"jobProperties": {
					"name": "2",
					"file": "abfss://defaultfs@hozhaobdbj.dfs.core.windows.net/synapse/workspaces/bdbj0520yan/batchjobs/donetdll/mySparkApp.zip",
					"className": "WordCount",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9f4d0dcf-2c94-465e-bcdb-28e67ed52bee"
					},
					"args": [],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/3')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.1",
				"language": "scala",
				"jobProperties": {
					"name": "3",
					"file": "abfss://defaultfs@hozhaobdbj.dfs.core.windows.net/synapse/workspaces/bdbj0520yan/batchjobs/donetdll/mySparkApp.zip",
					"className": "WordCount",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "69a52c7f-0b04-444a-96f1-7669f015d15b"
					},
					"args": [],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Spark job definition 1')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.1",
				"language": "scala",
				"jobProperties": {
					"name": "Spark job definition 1",
					"file": "abfss://sparkjob@jiamin.dfs.core.windows.net/scala/wordcount/wordcount.jar",
					"className": "WordCount",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "18dc816f-b874-4383-b6e7-3954ff3b9958"
					},
					"args": [
						"abfss://sparkjob@jiamin.dfs.core.windows.net/scala/wordcount/shakespeare.txt",
						"abfss://sparkjob@jiamin.dfs.core.windows.net/scala/wordcount/result"
					],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Spark job definition 2')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.1",
				"language": "scala",
				"jobProperties": {
					"name": "Spark job definition 2",
					"file": "abfss://scala@jiamin.dfs.core.windows.net/samples/scala/wordcount/wordcount.jar",
					"className": "org.apache.spark.deploy.dotnet.DotnetRunner",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7659aad3-b8d6-414b-9384-db0e1473a237"
					},
					"args": [
						"abfss://scala@jiamin.dfs.core.windows.net/samples/scala/wordcount/shakespeare.txt",
						"abfss://scala@jiamin.dfs.core.windows.net/samples/scala/wordcount/result"
					],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Spark job definition 30')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.1",
				"language": "scala",
				"jobProperties": {
					"name": "Spark job definition 30",
					"file": "abfs://file@liud.dfs.core.windows.net/files",
					"className": "liu.api",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4a186f4c-9a64-4408-bc95-986b87f77753"
					},
					"args": [],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/net sample')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.1",
				"language": "csharp",
				"jobProperties": {
					"name": "net sample",
					"file": "local:///usr/hdp/current/spark2-client/jars/microsoft-spark.jar",
					"className": "org.apache.spark.deploy.dotnet.DotnetRunner",
					"conf": {
						"spark.yarn.appMasterEnv.DOTNET_ASSEMBLY_SEARCH_PATHS": "./udfs",
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2eb6e909-b171-4119-b68d-fe9eae7a7555"
					},
					"args": [
						"abfss://sparkjob@jiamin.dfs.core.windows.net/dotnet/wordcount/wordcount.zip",
						"WordCount",
						"abfss://sparkjob@jiamin.dfs.core.windows.net/dotnet/wordcount/shakespeare.txt",
						"abfss://sparkjob@jiamin.dfs.core.windows.net/dotnet/wordcount/result"
					],
					"jars": [],
					"files": [],
					"archives": [
						"abfss://sparkjob@jiamin.dfs.core.windows.net/dotnet/wordcount/wordcount.zip#udfs"
					],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/net')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.1",
				"language": "csharp",
				"jobProperties": {
					"name": "net",
					"file": "local:///usr/hdp/current/spark2-client/jars/microsoft-spark.jar",
					"className": "org.apache.spark.deploy.dotnet.DotnetRunner",
					"conf": {
						"spark.yarn.appMasterEnv.DOTNET_ASSEMBLY_SEARCH_PATHS": "./udfs",
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "67639d59-2d13-4963-81c7-ccae2370a3ad"
					},
					"args": [
						"abfss://sparkjob@jiamin.dfs.core.windows.net/dotnet/wordcount/wordcount.zip",
						"WordCount",
						"abfss://sparkjob@jiamin.dfs.core.windows.net/dotnet/wordcount/shakespeare.txt",
						"abfss://sparkjob@jiamin.dfs.core.windows.net/dotnet/wordcount/result"
					],
					"jars": [],
					"files": [],
					"archives": [
						"abfss://sparkjob@jiamin.dfs.core.windows.net/dotnet/wordcount/wordcount.zip#udfs"
					],
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/net_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.1",
				"language": "csharp",
				"jobProperties": {
					"name": "net_Copy1",
					"file": "local:///usr/hdp/current/spark2-client/jars/microsoft-spark.jar",
					"className": "org.apache.spark.deploy.dotnet.DotnetRunner",
					"conf": {
						"spark.yarn.appMasterEnv.DOTNET_ASSEMBLY_SEARCH_PATHS": "./udfs",
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "73b0e16e-1a0c-41ba-b09d-c5998c016569"
					},
					"args": [
						"abfss://sparkjob@jiamin.dfs.core.windows.net/dotnet/wordcount/wordcount.zip",
						"WordCount",
						"abfss://sparkjob@jiamin.dfs.core.windows.net/dotnet/wordcount/shakespeare.txt",
						"abfss://sparkjob@jiamin.dfs.core.windows.net/dotnet/wordcount/result"
					],
					"jars": [],
					"files": [],
					"archives": [
						"abfss://sparkjob@jiamin.dfs.core.windows.net/dotnet/wordcount/wordcount.zip#udfs"
					],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/python sample')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.1",
				"language": "python",
				"jobProperties": {
					"name": "python sample",
					"file": "abfss://sparkjob@hozhaogen2.dfs.core.windows.net/synapse/workspaces/bigdataqa0512ws2/batchjobs/05121604_python/wordcount.py",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0f0372ad-0b2f-4d7e-8d3c-6f44b9ccd946"
					},
					"args": [
						"abfss://sparkjob@hozhaogen2.dfs.core.windows.net/python/shakespeare.txt",
						"abfss://sparkjob@hozhaogen2.dfs.core.windows.net/python/result"
					],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/python')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.1",
				"language": "python",
				"jobProperties": {
					"name": "python",
					"file": "abfss://sparkjob@yifsoadlsgen2westus2.dfs.core.windows.net/synapse/workspaces/yifso1202ws/batchjobs/FileExists_Python/fileexists.py",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d4937fe5-2fd6-4c78-95b8-2c4b9e7581af"
					},
					"args": [],
					"jars": [],
					"files": [
						"abfss://sparkjob@yifsoadlsgen2westus2.dfs.core.windows.net/synapse/workspaces/yifso1202ws/batchjobs/FileExists_Python/shakespeare.txt"
					],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/scala sample')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.1",
				"language": "scala",
				"jobProperties": {
					"name": "scala sample",
					"file": "abfss://sparkjob@jiamin.dfs.core.windows.net/scala/wordcount/wordcount.jar",
					"className": "WordCount",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "134fc18e-417b-412b-a8e9-23bbcecacc38"
					},
					"args": [
						"abfss://sparkjob@jiamin.dfs.core.windows.net/scala/wordcount/shakespeare.txt",
						"abfss://sparkjob@jiamin.dfs.core.windows.net/scala/wordcount/result"
					],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/scala')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.1",
				"language": "scala",
				"jobProperties": {
					"name": "scala",
					"file": "abfss://sparkjob@jiamin.dfs.core.windows.net/scala/wordcount/wordcount.jar",
					"className": "WordCount",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1c999f23-560b-49ab-8ca4-fcb092c0c654"
					},
					"args": [
						"abfss://sparkjob@jiamin.dfs.core.windows.net/scala/wordcount/shakespeare.txt",
						"abfss://sparkjob@jiamin.dfs.core.windows.net/scala/wordcount/result"
					],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/scalaimport')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "my scalasdf",
				"targetBigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.1",
				"language": "scala",
				"jobProperties": {
					"name": "scalaimport",
					"file": "abfss://defaultfs@hozhaobdbj.dfs.core.windows.net/synapse/workspaces/bdbj0520yan/batchjobs/scala/wordcount.jar",
					"className": "WordCount",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f41fa0e4-60b0-4a1c-b759-df451415ab09"
					},
					"args": [
						"abfss://defaultfs@hozhaobdbj.dfs.core.windows.net/synapse/workspaces/bdbj0520yan/batchjobs/scala/shakespeare.txt",
						"abfss://defaultfs@hozhaobdbj.dfs.core.windows.net/synapse/workspaces/bdbj0520yan/batchjobs/scala/shakespeare"
					],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/test4')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "small",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.1",
				"language": "scala",
				"jobProperties": {
					"name": "test4",
					"file": "abfss://sparkjob@yifsoadlsgen2westus2.dfs.core.windows.net/synapse/workspaces/yifso1022scus/batchjobs/test4/wordcount.jar",
					"className": "WordCount",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "bcc4c3da-30de-4c49-8b56-21563f7168b3"
					},
					"args": [
						"abfss://sparkjob@yifsoadlsgen2westus2.dfs.core.windows.net/synapse/workspaces/yifso1022scus/batchjobs/test4/shakespeare.txt",
						"abfss://sparkjob@yifsoadlsgen2westus2.dfs.core.windows.net/synapse/workspaces/yifso1022scus/batchjobs/test4/shakespeare"
					],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks",
			"apiVersion": "2019-06-01-preview",
			"properties": {},
			"dependsOn": []
		}
	]
}